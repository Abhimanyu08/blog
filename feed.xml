<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://abhimanyu08.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://abhimanyu08.github.io/blog/" rel="alternate" type="text/html" /><updated>2020-07-20T11:31:44-05:00</updated><id>https://abhimanyu08.github.io/blog/feed.xml</id><title type="html">Abhimanyu</title><subtitle>A blog where I write about things I learn on my self-taught journey in the field of Artificial Intelligence</subtitle><entry><title type="html">Directional Derivatives, Hessians and Why Poor Condition Number Is A ?</title><link href="https://abhimanyu08.github.io/blog/deep-learning/2020/07/20/final.html" rel="alternate" type="text/html" title="Directional Derivatives, Hessians and Why Poor Condition Number Is A ?" /><published>2020-07-20T00:00:00-05:00</published><updated>2020-07-20T00:00:00-05:00</updated><id>https://abhimanyu08.github.io/blog/deep-learning/2020/07/20/final</id><content type="html" xml:base="https://abhimanyu08.github.io/blog/deep-learning/2020/07/20/final.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-20-final.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.-Directional-Derivative&quot;&gt;1. Directional Derivative&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-Directional-Derivative&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The dot product of two vectors ${a}$ and ${b}$ can be written as ${a.b}$ (where . represents dot product) or ${a^{T}b}$ or ${\lVert a\rVert\lVert b\rVert cos\theta}$ where ${\theta}$ is the angle between ${a}$ and ${b}$ and ${\lVert . \rVert}$ denotes the Frobenius norm of a quantity.&lt;/p&gt;
&lt;p&gt;Let's say we have a multivariable function with two variables ${f(x_{1},x_{2}) = x_{1}^{2} + x_{2}^{2}}$ where ${x_{1}}$ and ${x_{2}}$ represent the standard axes in a 2 dimensional space. We can say that f takes in a two-dimensional vector as input where the entries of vector are values of ${x_{1}}$ and ${x_{2}}$ respectively. We want to know how the function changes if we are at point ${(1,1)}$ and move in the direction of ${x_{1}}$ by a little amount. We can answer this question by calculating the partial derivative of ${f}$ w.r.t ${x_{1}}$ which is ${2x_{1}}$. Putting ${x_{1} = 1}$, we conclude that if we move a minisule amount in ${x_{1}}$ direction the function ${f}$ will change by twice that amount. The same goes for the case in which we want to take a tiny step in direction of ${x_{2}}$. But what if we want to move in the direction ${(1,2)}$ ? For calcualting the effect of this movement on ${f}$ we need to calculate the derivative of ${f}$ in direction ${[\frac {1}{\sqrt{5}},\frac{2}{\sqrt{5}}]^{T}}$.&lt;/p&gt;
&lt;p&gt;More generally, to find out how does nudging the input of a multivariable function in a certain direction affects the value of that function we need to calculate the derivative of that function in that direction. This derivative is called the directional derivative of that function for that particular direction. Fortunately, it's trivially easy to calculate. The directional derivative of a function g in direction ${u}$ is just ${u^{T}\nabla \scriptsize x\normalsize g(x)}$ (matrix mutltiplication) or ${u . \nabla \scriptsize x\normalsize g(x)}$ (dot product) where ${\nabla \scriptsize x\normalsize g(x)}$ is called gradient of function g. Gradient of g is a vector containing all the partial derivatives of ${g}$ with respect to vector ${\textbf{x}}$. Element ${i }$ of ${\nabla \scriptsize x\normalsize g(x)}$ is partial derivative of ${g}$ w.r.t ${x_{i}}$. Thus, partial derivative of our function f in the direction ${[\frac {1}{\sqrt{5}},\frac{2}{\sqrt{5}}]^{T}}$ is just ${[\frac {1}{\sqrt{5}},\frac{2}{\sqrt{5}}].[2x_{1}, 2x_{2}] = \frac{2x_{1}}{\sqrt{5}}+\frac{4x_{2}}{\sqrt{5}}}$.&lt;/p&gt;
&lt;p&gt;More formally, directional derivative of a function ${f}$ in direction ${u}$ is derivative of ${f(x + \alpha u)}$ with respect to ${\alpha}$, calculated at ${\alpha = 0}$. Let ${(x + \alpha u) = p}$. Then ${\large \frac{\delta p}{\delta \alpha} = u . \therefore \frac{\delta f(x+\alpha u)}{\delta \alpha} = \frac{\delta f(p)}{\delta \alpha} = \frac{\delta f(p)}{\delta p}\times \frac{\delta p}{\delta \alpha} = \small\nabla_{p} f(p).u = u^{T}\nabla_{x+\alpha u}f(x + \alpha u)}$ (using the chain rule of calculus). Putting ${\alpha = 0}$ we get ${u^{T}\nabla_{x}f(x)}$&lt;/p&gt;
&lt;h3 id=&quot;Why-do-we-take-a-step-in-negative-direction-of-the-gradient-?&quot;&gt;Why do we take a step in negative direction of the gradient ?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-do-we-take-a-step-in-negative-direction-of-the-gradient-?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Earlier we saw that the directional derivative of a function ${f}$ in the direction ${u}$ is ${u^{T}\nabla_xf(x)}$ or ${u.\nabla_xf(x)}$. This fact can give us the answer to where should we move while minimsing a function. During learning, our aim is to minimise an objective function. Suppose we are at a certain point in the input space of function ${f}$ and we want to decide where should we move so that that the function decreases the fastest. In other words we want to move in direction u such that ${u.\nabla_xf(x)}$ is minimum. By writing ${u.\nabla_xf(x)}$ as ${\lVert u\rVert\lVert \nabla_{x}f(x)\rVert cos\theta}$, we can see that this quantity is minimum when ${cos\theta = -1}$ or ${\theta = 180^{\circ}}$. This means ${u}$ should be in opposite direction of the gradient. This is the reason we take a step in the direction of negative gradient during learning/training. Therefore, the gradient descent algorithm proposes to take a step ${\theta\leftarrow \theta- \epsilon \nabla_{\theta}f(\theta)}$ where ${\epsilon}$ is called the learning rate.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-Second-derivative-of-a-function-and-how-does-it-affect-the-gradient-step&quot;&gt;2. Second derivative of a function and how does it affect the gradient step&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-Second-derivative-of-a-function-and-how-does-it-affect-the-gradient-step&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The second derivative of a fnction with respect to it's input tells us about the curvature of that fnction. Let's take three fnctions ${p(x) = 2x, g(x) = x^{3}, h(x) = -x^{2}}$.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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
&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Let's say we are at the point ${x = 1}$. The second derivative of the fnctions at this point are ${p^{''}(x) = 0, g^{''}(x) = 6x, h^{''}(x) = -2}$. The value of ${p^{''}(x)}$ tells us that it's gradient doesn't change at all. We can verify this by seeing that gradient of p(x) is 2 everywhere. The second derivative of g(x) increases as x increases. This shows that gradient of g increases as we move rightwards from any point. The positive value of second derivative of g(x) for positive x shows that fnction curves upwards when x is positive. The negative value of second derivative of h(x) tells us that gradient of h(x) decreases as x increases. Therfore, the fnction curves downwards.
We are at point x = 1 and we want to minimise the fnctions p,g and h. Gradient descent tells us to move in the direction of negative gradient. The gradient of the fnctions at ${x = 1}$ are ${p^{'}(1) = 2, g^{'}(1) = 3, h^{'}(1) = -2}$. The value of 2 for ${p^{'}(1)}$ tells us that if we change x by a small amount, the value of p(x) should change by twice that amount. The same goes for g(x) and h(x). Let's see if that's the case.&lt;/p&gt;
&lt;p&gt;Take  ${\epsilon = 0.001}$
In case of p(x), we move such that ${x = x - 2\epsilon = 1 - 0.001\times2 = 0.998}$. New value of p(x) is ${2\times0.998 = 1.996}$. Thus, we have successflly decreased the value of p(x) and we can see that it decreased by 0.004 which is exactly twice the amount of change in x. This is what we expected from the gradient.&lt;/p&gt;
&lt;p&gt;For g(x), ${x = x- 3\epsilon = 1 - 3\times0.001 = 0.997}$. The new value of g(x) is 0.991026973. x changes by 0.003 so according to gradient information g(x) should change by 0.009. We can see that g(x) changes by 0.008973027 which is little less than the value 0.009 predicted by the gradient.&lt;/p&gt;
&lt;p&gt;For h(x), ${x = x- (-2)\epsilon = 1 + 2\times0.001 = 1.002}$. The new value of h(x) is -1.004004. x changes by 0.002 so according to gradient information h(x) should change by 0.004. We can see that h(x) changes by 0.004004 which is little more than the value 0.004 predicted by the gradient.&lt;/p&gt;
&lt;p&gt;More generally we can see the effect of second derivative more clearly by looking at the taylor series approximation of a function at a point ${x^{\omicron}}$. The function f at a point ${x^{\omicron}}$ can be approximated upto three terms as follows :&lt;/p&gt;
&lt;p&gt;${f(x)\approx f(x^{\omicron}) + \large\frac{(x-x^{\omicron})}{1!}\small f^{'}(x^{\omicron}) + \large\frac{(x-x^{\omicron})^{2}}{2!}\small f^{''}(x^\omicron)}$ - ${\textbf Eq(1)}$.&lt;/p&gt;
&lt;p&gt;The gradient descent tells us that in order to reduce ${f(x)}$ we should go from ${x^\omicron}$ to ${x^\omicron - \epsilon f^{'}(x^\omicron)}$. Putting ${x = x^\omicron - \epsilon f^{'}(x^\omicron)}$ in Eq(1):&lt;/p&gt;
&lt;p&gt;${f(x^\omicron - \epsilon f^{'}(x^\omicron)) = f(x^\omicron) - \epsilon (f^{'}(x^\omicron))^{2} + 0.5 (\epsilon f^{'}(x^\omicron))^{2}f^{''}(x^\omicron)}$.&lt;/p&gt;
&lt;p&gt;From above equation we can see that the second term in above equation is the reduction in value of ${f(x^\omicron)}$ and third term is correction we apply to account for the curvature of the function. If ${f^{''}(x^\omicron) = 0}$ then the third term vanishes as happened in case of function ${p(x)}$ above. In this case gradient information correctly predicts the change in value of function.&lt;/p&gt;
&lt;p&gt;But if second derivative of a function is non-zero, there are second order effects too which have to be accounted for. Gradient descent is unaware of these second order effects. If second derivative of a function is too large at some point, a step taken in opposite direction of gradient can even increase the value of function if learning rate isn't too small. If second derivative of a function is too large at some point we need to keep learning rate too small to avoid the second order effects, but a small learning rate will lead to longer training time.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://abhimanyu08.github.io/blog/images/ls.JPG" /><media:content medium="image" url="https://abhimanyu08.github.io/blog/images/ls.JPG" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What Does Label Smoothing Do?</title><link href="https://abhimanyu08.github.io/blog/deep-learning/2020/05/17/final.html" rel="alternate" type="text/html" title="What Does Label Smoothing Do?" /><published>2020-05-17T00:00:00-05:00</published><updated>2020-05-17T00:00:00-05:00</updated><id>https://abhimanyu08.github.io/blog/deep-learning/2020/05/17/final</id><content type="html" xml:base="https://abhimanyu08.github.io/blog/deep-learning/2020/05/17/final.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-05-17-final.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;Introduction&lt;a class=&quot;anchor-link&quot; href=&quot;#Introduction&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Label smoothing was introduced by Szegedy et.al in the paper &lt;a href=&quot;https://arxiv.org/abs/1512.00567&quot;&gt;Rethinking the Inception Architecture for Computer Vision&lt;/a&gt;. Since then, this trick has been used in many papers to improve the accuracy of various architectures. Although being widely used, there was less insight as to why this technique helps the model to perform better, but the paper by Rafael Mller et.al &lt;a href=&quot;https://arxiv.org/abs/1906.02629v2&quot;&gt;When does Label Smoothing Help?&lt;/a&gt; answers the question of &quot;What does label smoothing do?&quot; and &quot;Why does it help the model?&quot;. This blog post is an attempt to explain the main result of the paper.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.-What-Is-Label-Smoothing?&quot;&gt;1. What Is Label Smoothing?&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-What-Is-Label-Smoothing?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Generally, in a classification problem, our aim is to maximize the log-likelihood of our ground-truth label. In other words, we want our model to assign maximum probability to the true label given the parameters and the input i.e we want ${P(y\mid x,\theta)}$ to be high, where the ${y}$ is known beforehand. We motivate our model to achieve this by minimizing the cross-entropy loss between the predictions our model outputs and the ground truth labels. Cross entropy loss is defined by the equation:
${L(y,\hat y)=-\sum_{i=1}^{n} y_{i} \times log(\hat y_{i}) }$ where n is the number of classes, ${y_{i}}$ is 1 if image belongs to class ${i}$ and 0 otherwise, and ${\hat y_{i}}$ is probability of ${y_{i}}$ being 1. Don't be intimidated by the daunting equation and jargon because in reality the calculation of loss is very easy. Suppose you build a model for task of image-classification where an image can belong to one of the 3 classes. For every image as the input the model outputs a 3-dimensional vector. Let's say for a particular image the model's normalised output is 
${\hat y = [0.2, 0.7,0.1]}$ and the image belongs to category 2. Therefore, the target vector for that image will be ${y = [0,1,0]}$. The loss for this image will be ${-(0\times \log 0.2 + 1\times \log 0.7 + 0\times \log 0.1) = -\log 0.7=0.35}$ which is low because our model assigns high probability to groud truth label. If instead our predictions are ${
\hat y=[0.8,0.1,0.1]}$, then the loss will be ${-\log 0.1=2.3}$ which is high because now our model assigns low probability to ground-truth label.&lt;/p&gt;
&lt;p&gt;There is little more to how the normalised predictions of the model are calculated. The model's predictions are calulated by applying the Softmax activation on the last layer's output. The model outputs a 3-dimensional vector and each of the element of the vector is called 'logit'. For the logits to represent a valid probability distribution over the classes they should sum to 1. This is accomplished by passing the logits through a softmax layer. Let's say the output vector for a certain image as input is ${z = [z_{1}, z_{2},...,z_{n}]}$ then the predictions are calculated as ${\hat y = \text Softmax \left(z \right) = \large [\frac {e^{z_{1}}}{\sum_{i=1}^{n} e^{z_{i}}}, \frac {e^{z_{2}}}{\sum_{i=1}^{n} e^{z_{i}}},...,
\frac {e^{z_{n}}}{\sum_{i=1}^{n} e^{z_{i}}}]}$.${(Eq\,1.1)}$
Notice that sum of all the elements of ${\hat y}$ is 1. Suppose the ground truth label for the image is 2, then the target vector is ${[0,1,0,0,....0]}$ (The length of target vector is n as well). Thus, the Cross-entropy loss for this image, in it's full glory is written as ${\text loss\left(y,z\right) = -1 \times \normalsize \log \frac {e^{z_{2}}}{\sum_{i=1}^{n} e^{z_{i}}} = \log {\sum_{i=1}^{n} e^{z_{i}}} - z_{2}}$. Minimising this loss encourages ${z_{2}}$ to be as high as possible while ${z_{i}}$ for ${i\ne2}$ are encouraged to be low. Szegedey et.al highlight two problems with this approach:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The first problem with this approach is that model becomes over-confident for it's predictions as it learns to assign nearly 100% probability to the ground-truth label. Szegedy et. al argue that this can lead to overfitting and model may not be able to generalize well. Intuitively this makes sense. for.eg Let's say our dataset contains two symantically similar classes class1 and class2(&lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/pets/&quot;&gt;pets dataset&lt;/a&gt; has plenty of those). Unfortunately our dataset contains many instances of class1 but relatively less instances of class2. Suppose image1 belongs to class1 and image2 to other. Because these images are very similar, the output logits of these images would be very similar. Our over-confident model may assign class1 to the image2 with high confidence(close to 100% probability) and this can incur heavy validation loss.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The other problem with this approach is the vanishing gradient. The gradient of our loss w.r.t logit of correct class label k is ${\large \frac {e^{z_{k}}}{\sum_{i=1}^{n} e^{z_{i}}}-1}$ and w.r.t other logits is ${\large \frac {e^{z_{i}}}{\sum_{i=1}^{n} e^{z_{i}}}}$. Minimising the cross-entropy loss leads to logit corresponding to correct class to be much higher than other logits. This leads to vanishing of gradients of loss w.r.t other logits and thus it hinders the model's ability to adapt.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What can we do to counteract these two problems? Szegedy et.al suggest that we shouldn't provide sparse one-hot encoded vectors as target. Instead we should &quot;smoothen&quot; them. This is done by replacing the probability distribution over labels from dirac delta distribution to a linear combination of dirac delta distribution and a uniform distribution. This may sound incredibly complex to hear but in reality is very easy to implement. Let's define what the above jargon means.&lt;/p&gt;
&lt;p&gt;Dirac delta function denoted by ${\delta _{i,l}}$ is a function which is 1 for ${i=l}$ and 0 everywhere else. If an image has class ${l=3}$ as it's label and there are ${k=4}$ classes in total, then the target vector for that image has the probability distribution ${\delta _{i,3}}$ for ${i=1\,to\,k}$. ${i}$ represents index of the target vector (I haven't used 0 indexing) and therefore the target vector is ${[0,0,1,0]}$. Notice that ${\delta _{i,l}}$ is a valid probability distribution as it sums to 1 over it's domain. A uniform distribution is a distribution which has a constant value over it's domain. Let's say our domain consists of ${\{x\epsilon[1,4]: x\epsilon I\}}$. This is read as x belongs to 1 to 4 both included such that x is an integer. So, ${x\epsilon \{1,2,3,4\}}$. Uniform distribution over this domain is denoted as ${U\left(x\right)}$. ${\therefore U\left(1\right) = U\left(2\right) = U\left(3\right) = U\left(4\right) = c}$. The sum over the domain i.e ${\sum_{i=1}^{4} U(i)}$ is 4c. For ${U(x)}$ to be a valid probability distribution ${4c}$ should equal to 1. ${\therefore c=0.25}$. More generally we can say that if there are ${k}$ points in our domain, then uniform distribution over the domain would be ${U(i)=\frac{1}{k}}$ where ${i}$ is any point in the domain.&lt;/p&gt;
&lt;p&gt;Let's denote the distribution over our labels for a particular image as ${q\left(i\right)}$ where ${i=1\,to\,k}$. ${k}$ denotes the total no of classes and ${l}$ denotes the true label for the image. Generally, ${q\left(i\right) = \delta _{i,l}}$. Szegedy et. al propose to replace ${\delta _{i,l}}$ with ${(1-\varepsilon)\delta _{i,l} + \varepsilon U\left(i\right)\,for\,i=1\,to\,k}$ where ${\varepsilon}$ is a hyperparameter. As explained above, value of ${U\left(i\right)}$ should be ${\frac {1}{k}\,for\,i=1\,to\,k}$. Then, our new distribution over labels is ${q'(i) = (1-\varepsilon)\delta _{i,l} + \frac{\varepsilon}{k}}$ $(Eq\,1.1)$. Let's see how to do this using an example.&lt;/p&gt;
&lt;p&gt;Suppose distribution over target labels of an image say image1 for a classification task which has ${k=4}$ classes is ${q\left(i,2\right)=\delta_{i,2}\, for \,i = 1\,to\,k}$. ${i}$ here represents index of target vector. Thus, target vector will be ${y^{h}=[0,1,0,0]}$. Then, our new distribution over labels according to ${Eq\,1.1}$ is
${q'\left(i,2\right) = (1-\varepsilon)\delta _{i,2} + \frac{\varepsilon}{4}}$ for ${i=1\,to\,4}$. Subsequently, smoothened target vector ${y^{l}}$ will be ${[\frac{\varepsilon}{4},(1-\varepsilon)+\frac{\varepsilon}{4},\frac{\varepsilon}{4},\frac{\varepsilon}{4}]}$ = ${[0.25\varepsilon, 1-\varepsilon+0.25\varepsilon, 0.25\varepsilon,0.25\varepsilon]}$. If ${\varepsilon = 0.2}$,then ${y^{l} =[0.05,0.85,0.05,0.05]}$. Notice that elements of new smoothened label vector still sum to 1, which confirms that ${(1-\varepsilon)\delta _{i,l}+\varepsilon U\left(i\right)}$ is a valid probability distribution over the labels.&lt;/p&gt;
&lt;p&gt;Let's see what difference does it make to change the labels in the way shown above. Suppose our model outputs the prediction vector ${p_{1}=[0.05,0.9,0.03,0.02]}$ for image1. So the model is really confident that this image has label 2 which is a good thing since this image really does has label 2. The loss with smoothened labels ${y^{l}}$ will be ${L(y^{l},p_{1})= -(0.05\log 0.05+0.85\log0.9+0.05\log0.03+0.05\log0.02)= 0.61}$. Now suppose our model didn't output ${p_{1}}$ but ${p_{2}=[0.01,0.79,0.15,0.5]}$. In this case it is less sure that the image has label 2. Loss will be ${L(y^{l},p_{2})= -(0.05\log0.01+0.85\log0.79+0.05\log0.15+0.05\log0.5)=0.56}$ which is less than the loss with ${p_{1}}$ ! This goes on to show that smooth labels want the model to be confident about it's predictions but not over-confident.&lt;/p&gt;
&lt;p&gt;Intuitively we can think of label smoothing as a process to reduce the confidence of model in it's ground truth labels.The ground truth labels may sometimes be awry owing to errors in data labelling or data collection process. Label smoothing can make the model robust against those incorrect labels.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-Implementation-In-Code&quot;&gt;2. Implementation In Code&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-Implementation-In-Code&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;To implement label smoothing, we don't change every label individually but we define a new loss function. Loss function is still Cross-entropy loss but our target vector for every image changes. Our new target vector for a particular image is ${y^{l} = [\frac {\varepsilon}{k},\frac {\varepsilon}{k},...,(1 - \varepsilon) + \frac{\varepsilon}{k},\frac {\varepsilon}{k},\frac {\varepsilon}{k},...k times]}$. Let's assume the image belongs to class ${j}$. Normal one hot encoded target label will have 1 at j position and 0 everywhere else. Let's denote it as ${y^{h}}$. So, ${y^{h} = [0,0,0,...,1,0,...0]}$.&lt;/p&gt;
&lt;p&gt;The loss with ${y^{h}}$ is ${L(y^{h},\hat y)= -\log \hat y_{j}}$. ${Eq\,2.1}$&lt;/p&gt;
&lt;p&gt;The loss with new smoothened labels is ${L\left(y^{l},\hat y\right) = \sum_{i=1}^{k} -y_{i}^{l}\log \hat y_{i}}$ = ${- \left( \frac {\varepsilon}{k}\log\hat y_{1} +...+ \left(1-\varepsilon+ \frac{\varepsilon}{k}\right)\log\hat y_{j}+\frac {\varepsilon}{k}\log\hat y_{j+1}+...+\frac {\varepsilon}{k}\log\hat y_{k}\right)}$. We can rewrite this as ${L\left(y^{l},\hat y\right) = -\left(1-\varepsilon\right)\times\log\hat y_{j} - \frac{\varepsilon}{k}\times\left(\sum_{i=1}^{k} \log\hat y_{i}\right)}$. Eagle eyed reader can notice that the term which is multiplied by ${\left(1 - \varepsilon\right)}$ is the cross-entropy loss calculated with one hot encoded target vector. Therefore, ${L\left(y^{l},\hat y\right) = \left(1-\varepsilon\right)L\left(y^{h},\hat y\right)-\frac{\varepsilon}{k}\left(\sum_{i=1}^{k} \log\hat y_{i}\right)}$. ${Eq\,2.2}$&lt;/p&gt;
&lt;p&gt;So, we only need to modify the loss function of our model and we are good to go. The implementation of this in code is shown below. The code snippet below uses Pytorch framework and implementation is copied from the &lt;a href=&quot;https://www.fast.ai/&quot;&gt;fast.ai&lt;/a&gt; course.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot; open=&quot;&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#collapse-show&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lin_comb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;#39;&amp;#39;&amp;#39;This function calculates linear combination of &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    two quantities a1 and a2 where the respective&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    coeffecients are factor and (1-factor)&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;factor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;reduce_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;#39;&amp;#39;&amp;#39;We need this function because we generally calcualate &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    losses for a batch of images and take the mean or sum all the &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    losses. But throughout this blog we input only a single image &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    in the model so you can ignore this fuction and just assume &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    that this funtion does nothing. for.eg reduce_loss(2)=2&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;mean&amp;#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;sum&amp;#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;    

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LabelSmoothing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;fm&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;fm&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#factor for linear combination&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#You can safely ignore this&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;targ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#this line of code implements Eq 1.1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
        &lt;span class=&quot;c1&quot;&gt;#this line of code calculates the sum part of second term in Eq 2.2&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduce_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#this line of code calculates Eq 2.1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nll_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#finally this line implements Eq 2.2&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lin_comb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-How-And-Why-Does-It-Work?&quot;&gt;3. How And Why Does It Work?&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-How-And-Why-Does-It-Work?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Label smoothing goes against the conventional practice of maximising the likelihood of ground truth label. Instead, it punishes the model if the logits which don't correspond to correct label get too low. This can be seen by the second term in equation of loss mentioned above i.e ${-\frac{\varepsilon}{k}\left(\sum_{i=1}^{k} \log\hat y_{i}\right)}$. We can see that if ${\hat y_{i}\, for\, i = {1,2,...,k}}$ go too close to 0 then the loss goes up (${\log}$ of something close to 0 is a large negative number). In contrast, maximising the likelihood of one-hot encoded ground-truth label encourages the logits that don't correspond to correct label to go as low as possible. With smooth labels ${y^{l}}$ our aim is to maximise ${P(y^{l}\mid x,\theta)}$. Let's see why maximising the likelihood of smooth labels instead of maximising the likelihood of one-hot encoded labels is benificial for our model.&lt;/p&gt;
&lt;h3 id=&quot;Calculating-Loss-Without-Label-Smoothing&quot;&gt;Calculating Loss Without Label Smoothing&lt;a class=&quot;anchor-link&quot; href=&quot;#Calculating-Loss-Without-Label-Smoothing&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Let's imagine that we have a task to build a model for image classification task where each image can have one of three labels. This means our model will output a 3-dimensional vector containing our three logits. Assume that penultimate layer of the model has 4 activations. We put in an image in this model which has a target vector ${y^{h} = [0,1,0]^{T}}$. The penultimate layer's activations are ${X = [x_{1},x_{2},x_{3},x_{4}]^{T}}$, the last layer's outputs are ${Z = [z_{1},z_{2},z_{3}]^{T}}$ (A single vector is conventionally written as column vector, therefore, ${X}$, ${Z}$ and ${y^{h}}$ are written as transpose of row vectors). ${Z}$ is calculated from the penultimate layer's activation using the equation ${Z = W\star X}$ (${\star}$ here denotes matrix multiplication). Bias is ignored for sake of brevity. ${W}$ is the weight matrix connecting penultimate layer and output layer. ${W = \left[
         \begin{array}{ccc}
         w_{11} &amp;amp; w_{12} &amp;amp; w_{13} &amp;amp; w_{14}          \\
         w_{21} &amp;amp; w_{22} &amp;amp; w_{23} &amp;amp; w_{24}\\
         w_{31} &amp;amp; w_{32} &amp;amp; w_{33} &amp;amp; w_{34}
        \end{array}
    \right]}$. Shortly weight matrix can be written as ${W = [w_{1},w_{2},w_{3}]^{T}}$ where ${w_{i} = [w_{i1},w_{i2},w_{i3},w_{i4}]}$. The output vector ${Z}$ is calculated as ${W\star X = \left[
         \begin{array}{ccc}
         w_{11}\times x_{1} &amp;amp; w_{12}\times x_{2} &amp;amp; w_{13}\times x_{3} &amp;amp; w_{14}\times x_{4}          \\
         w_{21}\times x_{1} &amp;amp; w_{22}\times x_{2} &amp;amp; w_{23}\times x_{3} &amp;amp; w_{24}\times x_{4}\\
         w_{31}\times x_{1} &amp;amp; w_{32}\times x_{2} &amp;amp; w_{33}\times x_{3} &amp;amp; w_{34}\times x_{4}
        \end{array}
    \right]}$. In short this can be written as ${Z = \left[\begin{array}{ccc}
         z_{1}          \\
         z_{2}           \\
         z_{3}
        \end{array}
    \right] = \left[
         \begin{array}{ccc}
         w_{1}X^{T}          \\
         w_{2}X^{T}           \\
         w_{3}X^{T}
        \end{array}
    \right]}$ where ${w_{i}X^{T}}$ denotes inner product between ${w_{i}}$ and ${X^{T}}$. ${Z}$ is a vector of logits and is un-normalised. To get our prediction vector we would have to normalise this by passing ${Z}$ through a softmax layer. Our prediction vector would be ${\hat y = \left[
         \begin{array}{ccc}
         \frac {e^{w_{1}X^{T}}}{e^{w_{1}X^{T}}+e^{w_{2}X^{T}}+e^{w_{3}X^{T}}}          \\
         \frac {e^{w_{2}X^{T}}}{e^{w_{1}X^{T}}+e^{w_{2}X^{T}}+e^{w_{3}X^{T}}}           \\
         \frac {e^{w_{3}X^{T}}}{e^{w_{1}X^{T}}+e^{w_{2}X^{T}}+e^{w_{3}X^{T}}}
        \end{array}
    \right]}$. As given before, our target vector is ${y^{h} = [0,1,0]^{T}}$. So, our cross-entropy loss will be ${L\left(y^{h},\hat y\right) = -\log \left(\frac {e^{w_{2}X^{T}}}{e^{w_{1}X^{T}}+e^{w_{2}X^{T}}+e^{w_{3}X^{T}}}\right)}$. For preserving our sanity let's denote ${e^{w_{1}X^{T}}+e^{w_{2}X^{T}}+e^{w_{3}X^{T}}}$ by ${S}$. Then, ${L\left(y^{h},\hat y\right) = -\log \left(\frac {e^{w_{2}X^{T}}}{S}\right) = \log {S}-{w_{2}X^{T}}}$&lt;/p&gt;
&lt;h3 id=&quot;Calculating-Loss-With-Label-Smoothing&quot;&gt;Calculating Loss With Label Smoothing&lt;a class=&quot;anchor-link&quot; href=&quot;#Calculating-Loss-With-Label-Smoothing&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Our prediction vector is same as before, but our target vector changes. Let's denote our label smoothed target vector as ${y^{l}}$. So, ${y^{l} = [\frac {\varepsilon}{3}, 1-\varepsilon + \frac {\varepsilon}{3}, \frac {\varepsilon}{3}]^{T}}$. Then, our new loss will be ${L\left(y^{l},\hat y\right) = -\frac {\varepsilon}{3}\times \log\frac {e^{w_{1}X^{t}}}{S}-\left(1-\varepsilon+\frac{\varepsilon}{3}\right)\times \log\frac {e^{w_{2}X^{t}}}{S}-\frac {\varepsilon}{3}\times \log\frac {e^{w_{3}X^{t}}}{S}}$. Grouping the varibles appropriately,&lt;/p&gt;
&lt;p&gt;${L\left(y^{l},\hat y\right)= -\left(1-\varepsilon\right)\times \log\frac {e^{w_{2}X^{t}}}{S}-\frac {\varepsilon}{3}\times\left(\log\frac {e^{w_{1}X^{t}}}{S}+\log\frac {e^{w_{2}X^{t}}}{S}+\log\frac {e^{w_{3}X^{t}}}{S}\right)}$. Remember that ${\log a + \log b = \log ab}$. Utilising this rule, loss can be written as ${L\left(y^{l},\hat y\right)=\left(1-\varepsilon\right)\left(\log S-w_{2}X^{T}\right)-\frac{\varepsilon}{3}\times{\log\left(\frac{e^{w_{1}X^{T}+w_{2}X^{T}+w_{3}X^{T}}}{S^{3}}\right)}}$. To further reduce this equation, we need to know two more rules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;${\log\frac{a}{b}=\log a-\log b}$ and &lt;/li&gt;
&lt;li&gt;${\log a^{b}=b\log a}$. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then, ${L\left(y^{l},\hat y\right)=\left(\log S-w_{2}X^{T}\right)-\varepsilon\left(\log S - w_{2}X^{T}\right)-\frac{\varepsilon}{3}\left(w_{1}X^{T}+w_{2}X^{T}+w_{3}X^{T}\right)+\frac{\varepsilon}{3}\log\left(S^{3}\right)}$.&lt;/p&gt;
&lt;p&gt;Expanding the second term in this expression and using rule 2 we get,
${L\left(y^{l},\hat y\right)=\left(\log S-w_{2}X^{T}\right)-\varepsilon\log S+\varepsilon\left(w_{2}X^{T}\right)-\frac{\varepsilon}{3}\left(w_{1}X^{T}+w_{2}X^{T}+w_{3}X^{T}\right)+{\varepsilon}\log\ S}$. Notice, that first term of the last expression is our ${L\left(y^{h},\hat y\right)}$. Therefore, our loss with smooth labels can be finally written as ${L\left(y^{l},\hat y\right)=L\left(y^{h},\hat y\right)+\frac{\varepsilon}{3}\left(2w_{2}X^{T}-w_{1}X^{T}-w_{3}X^{T}\right)}$.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.-Geometric-Point-Of-View&quot;&gt;4. Geometric Point Of View&lt;a class=&quot;anchor-link&quot; href=&quot;#4.-Geometric-Point-Of-View&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;(X -&amp;gt; Penultimate layer's activation)&lt;/p&gt;
&lt;p&gt;Our last layer's output for the image we input earlier is ${Z= \left[
         \begin{array}{ccc}
         w_{1}X^{T}          \\
         w_{2}X^{T}           \\
         w_{3}X^{T}
        \end{array}
    \right]}$. Since this image belongs to class 2, minimising any of the loss functions calculated above increases ${w_{2}X^{T}}$ while ${w_{1}X^{T}}$ and ${w_{3}X^{T}}$ are decreased. More generally, if an image belongs to class ${k}$ then in minimising the loss, ${z_{k}=w_{k}X^{T}}$ is increased while every other logit is decreased. Also, notice a pattern that ${w_{i}}$ produces logits for class ${i}$ using the operation ${w_{i}X^{T}}$. Hence, ${w_{i}}$ can be thought of as a template for class ${i}$. So,from now on I'll sometimes refer to ${w_{i}}$ as template for class ${i}$. Let's try to view the process of minimising or maximising ${w_{i}X^{T}}$ geometrically.&lt;/p&gt;
&lt;h4 id=&quot;Euclidean-Norm&quot;&gt;Euclidean Norm&lt;a class=&quot;anchor-link&quot; href=&quot;#Euclidean-Norm&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Euclidean norm of two vectors is simply the distance between the two vectors in their space. Euclidean Norm for two vectors ${a}$ and ${b}$ can be calculated as: ${\lVert a-b\rVert=\left(a^{T}\star a-2a^{T}\star{b}+b^{T}\star b\right)^{\frac{1}{2}}}$. ${\therefore \lVert a-b\rVert^{2}= a^{T}\star a-2a^{T}\star{b}+b^{T}\star b}$. (Remeber that ${\star}$ denotes matrix multiplication.)&lt;/p&gt;
&lt;h4 id=&quot;Loss-Mimisation-as-Distance-Minimisation/Maximisation&quot;&gt;Loss Mimisation as Distance Minimisation/Maximisation&lt;a class=&quot;anchor-link&quot; href=&quot;#Loss-Mimisation-as-Distance-Minimisation/Maximisation&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Now that we know how to calculate the euclidean norm, let's calculate it for ${w_{i}}$ and ${X}$. ${\lVert w_{i}-X\rVert^{2}= w_{i}^{T}\star w_{i}-2w_{i}^{T}\star{X}+X^{T}\star X= w_{i}^{T}\star w_{i}-2w_{i}{X}^{T}+X^{T}\star X}$. (For any two vectors ${a}$ and ${b}$, ${a\star b=a.b^{T}}$ where ${\star}$ and ${.}$ denote matrix multiplication and inner product respectively). Geometrically, this quantity is square of the distance between template for class ${i}$ and penultimate layer's activation ${X}$.&lt;/p&gt;
&lt;p&gt;Notice the second term inside the expression of ${\lVert w_{i}-X\rVert^{2}}$ which is ${2w_{i}X^{T}}$. If this term increases, the distance between ${w_{i}}$ and ${X}$ decreases and whenever it decreases the mentioned distance increases. But notice that this second term is just the same as ${2\times z_{i}}$. This means whenever ${z_{i}}$ increases/decreases, distance between ${w_{i}}$ i.e tempelate for class ${i}$ and ${X}$ i.e penultimate layer's output vector decreases/increases. If an image belongs to class ${k}$, minimising the loss increases ${z_{k}}$ and decreases every other logit. This means that minimising the loss is same as minimising the distance between penultimate layer's output ${X}$ and template for correct class ${w_{k}}$ and maximising the distance between ${X}$ and template for every incorrect class i.e ${w_{i}}$ where ${i \neq k}$.&lt;/p&gt;
&lt;p&gt;Thus, we can infer that minimising ${L\left(y^{h},Z\right)}$ or ${L\left(y^{l},Z\right)}$ produces the same effect which is to bring ${w_{k}}$ close to ${X}$ when image belongs to class ${k}$ and taking ${w_{i}}$ where ${i\neq k}$ far from ${X}$. The different performance of these two losses stem from the manner in which they go about doing this which is explained below.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;5.-Derivatives-Of-Losses-Tell-The-Difference.&quot;&gt;5. Derivatives Of Losses Tell The Difference.&lt;a class=&quot;anchor-link&quot; href=&quot;#5.-Derivatives-Of-Losses-Tell-The-Difference.&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Now let's, painstakingly write the two losses without any abridgement.&lt;/p&gt;
&lt;p&gt;${L\left(y^{h},\hat y\right)=\log\left(e^{w_{1}X^{T}}+e^{w_{2}X^{T}}+e^{w_{3}X^{T}}\right)-w_{2}X^{T}}$&lt;/p&gt;
&lt;p&gt;${L\left(y^{l},\hat y\right)= \log\left(e^{w_{1}X^{T}}+e^{w_{2}X^{T}}+e^{w_{3}X^{T}}\right)-w_{2}X^{T}+\frac{\varepsilon}{3}\left(2w_{2}X^{T}-w_{1}X^{T}-w_{3}X^{T}\right)}$.&lt;/p&gt;
&lt;p&gt;The reason we wrote the losses like this is because written this way, it'll be easy to take their derivatives w.r.t any term we want. We know that we minimise the loss using gradient descent. Imagine the loss surface as a convex surface (like a hollow ball cut in half and it's upper hemisphere removed). Our aim is to go to the lowest point in this convex region where the loss is lowest. We go to this point by continously changing our parameters using the gradient descent rules. Now, at this point we need to remember some rules from calculus.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Let's say there's a function ${f\left(x\right)}$. It's derivative w.r.t x, ${\normalsize\frac{df}{dx}}$ can be denoted as ${f'(x)}$. Notice that derivative is also a function of x. Suppose ${f\left(x\right)}$ is at it's minimum at point ${x^{\ast}}$. Then, ${f'(x^{\ast})=0}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;${\normalsize\frac{d\log x}{dx}=\frac{1}{x}}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;Derivative-of-${L\left(y^{h},\hat-y\right)}$&quot;&gt;Derivative of ${L\left(y^{h},\hat y\right)}$&lt;a class=&quot;anchor-link&quot; href=&quot;#Derivative-of-${L\left(y^{h},\hat-y\right)}$&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Let's imagine that we trained our model using the loss ${L\left(y^{h},\hat y\right)}$ and through meticulous training we have reached the minimum point on our loss surface i.e our loss is lowest it can be (Sadly, in practice this doesn't happen but we still assume this because by doing so we can infer how the parameters behave in order to reach the holy grail i.e global minima or a satisfactory local minima). The value of ${W}$ at minima is ${W^{\ast}=\left[
         \begin{array}{ccc}
         w_{1}^{\ast}          \\
         w_{2}^{\ast}          \\
         w_{3}^{\ast}
        \end{array}
    \right]}$.&lt;/p&gt;
&lt;p&gt;Now we take derivative of ${L\left(y^{h},\hat y\right)}$ w.r.t ${W}$. The derivative is written as ${L'_{h}\left(W\right)=\frac{\delta L_{h}}{\delta W}= \left[
         \begin{array}{ccc}
         \frac{\delta L_{h}}{\delta w_{1}}          \\
         \frac{\delta L_{h}}{\delta w_{2}}          \\
         \frac{\delta L_{h}}{\delta w_{3}}
        \end{array}
    \right]}$ (${L_{h}}\, denotes\, L\left(y^{h},\hat y\right) $). Since ${L_{h}}$ is composed of two variables ${X}$ and ${w_{i}}$, it's derivative w.r.t one of the variables is written with delta (${\delta}$) sign. This sign simply denotes that while taking derivative of a function w.r.t to a variable treat the other variable as constant. Since we are taking derivative w.r.t ${w_{i}}$ we will treat ${X}$ as constant.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;${\large\frac{\delta L_{h}}{\delta w_{1}}=\frac{e^{w_{1}X^{T}} X^{T}}{S}}$.&lt;/li&gt;
&lt;li&gt;${\large\frac{\delta L_{h}}{\delta w_{2}}=\frac{e^{w_{2}X^{T}} X^{T}}{S}-\normalsize X^{T}}$.&lt;/li&gt;
&lt;li&gt;${\large\frac{\delta L_{h}}{\delta w_{3}}=\frac{e^{w_{3}X^{T}} X^{T}}{S}}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now from rules of calculus we know that ${\large\frac{\delta L_{h}}{\delta w_{1}}=\large\frac{\delta L_{h}}{\delta w_{2}}=\large\frac{\delta L_{h}}{\delta w_{3}}=\normalsize0}$ at ${W^{\ast}}$.&lt;/p&gt;
&lt;p&gt;${\therefore\large\frac{e^{w_{1}^{\ast}X^{T}} X^{T}}{S}=0 \implies e^{w_{1}^{\ast}X^{T}}=0\implies \normalsize w_{1}^{\ast}X^{T}=-\infty\,(Eq.1)}$. Similarly, ${\normalsize w_{3}^{\ast}X^{T}=-\infty\,(Eq.2)}$. The case is different with ${w_{2}}$ though because it is the tempelate corresponding to the correct class. ${\large\frac{e^{w_{2}^{\ast}X^{T}} X^{T}}{S}-\normalsize X^{T}=0\implies \frac{e^{w_{2}^{\ast}X^{T}}}{e^{w_{1}^{\ast}X^{T}}+e^{w_{2}^{\ast}X^{T}}+e^{w_{3}^{\ast}X^{T}}}=1}$ ${(Eq\,3)}$. ${(Eq\,3)}$ implies that ${w_{1}^{\ast}}$ and ${w_{3}^{\ast}}$ are negligible when compared to ${w_{2}^{\ast}}$. ${Eq.1}$ and ${Eq.2}$ show that ${L_{h}}$ is minimum when the distance between tempelate of incorrect labels ($w_{1}^{\ast}$,$w_{2}^{\ast}$) and ${X}$ is ${\infty}$.
From this we can infer the behaviour inflicted upon the weights connecting penultimate layer and Final Layer by reducing the loss ${L_{h}}$. Minimising this loss takes the weights corresponding to incorrect class away from penultimate layer's activations &lt;strong&gt;without any bounds&lt;/strong&gt;. i.e ${X}$ and the templates of incorrect classes really begin to hate each other and go as far away from each other as possible.&lt;/p&gt;
&lt;h3 id=&quot;Derivative-of-${L\left(y^{l},\hat-y\right)}$&quot;&gt;Derivative of ${L\left(y^{l},\hat y\right)}$&lt;a class=&quot;anchor-link&quot; href=&quot;#Derivative-of-${L\left(y^{l},\hat-y\right)}$&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;This time we train the model using the loss ${L\left(y^{l},\hat y\right)}$ and again reach the impractical situation where we are at the global minimum or a satisfactory local minimum of the loss surface. The ${W}$ at this point is ${W^{\star}}$. Note that this ${W^{\star}}$ is different from ${W^{\star}}$ of previous subsection because our loss surface is different. (Apologies if you get confused due to notation. ${W}$ is a variable while ${W^{\star}}$ is a fixed value of that variable which occurs at minima).&lt;/p&gt;
&lt;p&gt;The derivative of loss w.r.t ${W}$ is given as ${L'_{l}\left(W\right)=\frac{\delta L_{l}}{\delta W}= \left[
         \begin{array}{ccc}
         \frac{\delta L_{l}}{\delta w_{1}}          \\
         \frac{\delta L_{l}}{\delta w_{2}}          \\
         \frac{\delta L_{l}}{\delta w_{3}}
        \end{array}
    \right]}$ (${L_{l}}\, denotes\, L\left(y^{l},\hat y\right)$).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;${\large\frac{\delta L_{l}}{\delta w_{1}}=\frac{e^{w_{1}X^{T}} X^{T}}{S}-\frac{\varepsilon}{3}\normalsize X^{T}}$.&lt;/li&gt;
&lt;li&gt;${\large\frac{\delta L_{l}}{\delta w_{2}}=\frac{e^{w_{2}X^{T}} X^{T}}{S}-\normalsize X^{T}+\large \frac {2\varepsilon X^{T}}{3}}$.&lt;/li&gt;
&lt;li&gt;${\large\frac{\delta L_{l}}{\delta w_{3}}=\frac{e^{w_{3}X^{T}} X^{T}}{S}-\frac{\varepsilon}{3}\normalsize X^{T}}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We know that ${\large\frac{\delta L_{l}}{\delta w_{1}}=\large\frac{\delta L_{l}}{\delta w_{2}}=\large\frac{\delta L_{l}}{\delta w_{3}}=0}$ at ${W^{\star}}$.&lt;/p&gt;
&lt;p&gt;${\therefore \large\frac{e^{w_{1}^{\ast}X^{T}}X^{T}}{S}-\frac{\varepsilon X^{T}}{3}=0 \implies e^{w_{1}^{\ast}X^{T}}=\frac{S\large\varepsilon}{3}\implies \normalsize w_{1}^{\ast}X^{T}=\log \frac{\normalsize S\varepsilon}{3}}$ ${(Eq.3)}$. Similarly, ${\normalsize w_{3}^{\ast}X^{T}}=\large\log \frac{S\varepsilon}{3}$ ${(Eq.4)}$. In case of  ${\large\frac{\delta L_{l}}{\delta w_{2}}}$, ${\large\frac {e^{w_{2}^{\ast}X^{T}}X^{T}}{S}-\normalsize X^{T}+\large\frac {2\varepsilon X^{T}}{3} =0\implies \large e^{w_{2}^{\ast}X^{T}}= \normalsize S(1-\frac{2\varepsilon}{3})}$ ${(Eq.5)}$.&lt;/p&gt;
&lt;p&gt;To interpret ${Eq.5}$ let's put in the value of ${\varepsilon}$. Generally, ${\varepsilon}$ is taken as 0.1. Putting that in ${Eq.5}$, ${\large e^{w_{2}^{\ast}X^{T}}= \normalsize S(\frac{2.8}{3})=\normalsize0.93(e^{w_{1}^{\ast}X^{T}}+e^{w_{2}^{\ast}X^{T}}+e^{w_{3}^{\ast}X^{T}})\implies 0.07(e^{w_{2}^{\ast}X^{T}})=e^{w_{1}^{\ast}X^{T}}+e^{w_{3}^{\ast}X^{T}}}$. This shows that ${w_{2}^{\ast}X^{T}}$ is still large compared to  ${w_{1}^{\ast}X^{T}}$ and  ${w_{3}^{\ast}X^{T}}$. But there's one thing different, ${Eq.3}$ and ${Eq.4}$ show that at optimal point ${w_{1}^{\ast}X^{T}}$ and ${w_{3}^{\ast}X^{T}}$ are not ${-\infty}$ but a finite quantity i.e ${\normalsize\log \frac{S\varepsilon}{3}}$. This shows that minimising ${L_{l}}$ doesn't decrease ${w_{1}^{\ast}X^{T}}$ and ${w_{3}^{\ast}X^{T}}$ without any bounds, but decreases them upto a certain point which is same for ${w_{1}^{\ast}X^{T}}$ and ${w_{3}^{\ast}X^{T}}$.  We can see that ${X}$ is equidistant from both ${w_{1}^{\ast}}$ and ${w_{3}^{\ast}}$.&lt;/p&gt;
&lt;p&gt;Geometrically, we can say that minimising ${L_{l}}$ decreases the distance between tempelate of correct class and penultimate layer's activation (${X}$), and also encourages ${X}$ to go far from tempelates of incorrect classes but also remain equidistant from them. In this case ${X}$ hates the templates of incorrect classes but not as much as the ${X}$ of the previous subsection. Also, it hates all the incorrect class templates equally and tries to remain equidistant from them. The ${X}$ and template of correct class in this section love each other but not as strongly as those of previous section. (Apologies for the cheesy interpretation)&lt;/p&gt;
&lt;p&gt;So this is where ${L_{l}}$ is different from ${L_{h}}$.&lt;/p&gt;
&lt;p&gt;I hope now you can make sense of the main statement of the paper by Rafael Mller et.al which I quote verbatim: &lt;strong&gt;&quot;label smoothing encourages the activations of the penultimate layer to be close to the template of the correct class and equally distant to the templates of the incorrect classes.&quot;&lt;/strong&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;6.-Okay,-So-How-Does-It-Help-My-Model?&quot;&gt;6. Okay, So How Does It Help My Model?&lt;a class=&quot;anchor-link&quot; href=&quot;#6.-Okay,-So-How-Does-It-Help-My-Model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;(In this section ${Xi}$ will denote the penultimate layer's activation when an image belonging to class ${i}$ is input in the model.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;${L_{h}}$ = Loss calculated with one-hot encoded target vectors&lt;/li&gt;
&lt;li&gt;${L_{l}}$ = Loss calculated with target vectors with smooth labels&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's go with the above scenario that we have a task of image classifiaction where a given image can belong to 3 classes. Suppose that class1 and class2 among these are symantically very similar (for.eg toy poodle and miniature poodle class of &lt;a href=&quot;http://www.image-net.org/&quot;&gt;ImageNet&lt;/a&gt;). This means that if you input an image belonging to class1 and another image belonging to class2, their penultimate layer's activation can be very similar. Now, you prepare your dataset but you unfortunately forget to shuffle it randomly and so, all the images belonging to class1 are placed before all the images of class2 in the dataset. You begin training on this dataset by using a suitable batch size and using the loss ${L_{h}}$. By the time a batch of images belonging to class2 goes inside your model, the model has already been partially tuned by class1 images. Since you are training with ${L_{h}}$ loss, the ${X1}$ which derive from images of class1 have been dragged extremely far away from template of class2 and class3. Now, a batch of images belonging to class2 goes inside the model. Since class2 is symantically similar to class1, images belonging to this class have their penultimate layer's activation ${X2}$ very similar to ${X1}$. Because of this, these images will show a very strong affinity for class1 and despise being predicted that they belong to class2 because ${w_{1}(X2)^{T}}$ will be high for these images and at same time ${w_{2}(X2)^{T}}$ will be low. This will incur large value of loss which is bad for model. To remedy this, the model will need to take large steps and will take longer time to reduce the huge loss value. Instead, if we had trained on ${L_{l}}$, our model wouldn't have to work as hard to adapt since model itself is not entirely confident that if penultimate layer's activation are similar to ${X1}$ then the label is class1. Since ${w_{2}}$ is not dragged too far away from ${X1}$ and thus consequently ${X2}$ , the loss wouldn't be as high as in the previous case and the model will have the ability to adapt quickly. (Maybe this example also shows the importance of randomly shuffling your data).&lt;/p&gt;
&lt;p&gt;Another advantage is incurred in classification. Suppose that this time learning from our previous mistake we shuffled the data randomly but trained the model with loss ${L_{h}}$. Since images belonging to class1 and class2 are very similar, both ${X1}$ and ${X2}$ are close to both ${w_{1}}$ and ${w_{2}}$. Also both (${X1}$) and (${X2}$) are far away from ${w_{3}}$. Although this model will accurately differentiate between class1 and class3 or class2 and class3,it may also sometimes misclassify images, if the image belonging to class1 or class2 is fed into the model. Because their ${X's}$ are so similar, model may assign class2 to an image belonging to class1 or vice versa.
But instead if we train with ${L_{l}}$ loss, ${X1}$ will be equidistant from ${w_{2}}$ and ${w_{3}}$. Similarly,  ${X2}$ will be equidistant from ${w_{1}}$ and ${w_{3}}$. Then, if we put an image belonging to class2 and it produces ${X2}$ from it's penultimate layer, model will correctly assign it class2 and will not confuse it with class1. (If distance of ${X2}$ from ${w_{1}}$ and ${w_{3}}$ is sufficiently large).&lt;/p&gt;
&lt;p&gt;Sometimes, during our data labelling process, some images may get incorrect labels due to human error or other factors. In that case you don't want the penultimate layer activations of your images to cling too tightly to template of incorrectly labelled class, which would inevitably happen if you use ${L_{h}}$ loss. To make the modul robust against these incorrect labels, Label smoothing can come in handy because it decreases the model's confidence in it's incorrect ground-truth labels and doesn't let the ${X's}$ of images get too close to their incorrect label templates. Even though ${X}$ will get close to the tempelate of it's incorrect labels, it would be easier to modify if model is trained with ${L_{l}}$ instead of ${L_{h}}$.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;7.-Conclusion&quot;&gt;7. Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#7.-Conclusion&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We may conclude that if our dataset has symantically different classes and is correctly labelled (for.eg &lt;a href=&quot;https://github.com/fastai/imagenette&quot;&gt;Imagenette dataset&lt;/a&gt; by fastai), then our normal loss function may perform well. But if it has symantically similar classes (for.eg &lt;a href=&quot;https://github.com/fastai/imagenette#imagewoof&quot;&gt;Imagewoof dataset&lt;/a&gt; by fastai) or has incorrect labels, then you may want to use Label Smoothing.(Also, don't forget to randomly shuffle your data ;).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;em&gt;If you notice a mistake in this blog post please mention them in the comment section or email them to me at iamabhimanyu08@gmail.com, I'll make sure to correct them right away.&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://abhimanyu08.github.io/blog/images/ls.JPG" /><media:content medium="image" url="https://abhimanyu08.github.io/blog/images/ls.JPG" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Bayes Theorem</title><link href="https://abhimanyu08.github.io/blog/probability-theory/2020/03/23/final.html" rel="alternate" type="text/html" title="Bayes' Theorem" /><published>2020-03-23T00:00:00-05:00</published><updated>2020-03-23T00:00:00-05:00</updated><id>https://abhimanyu08.github.io/blog/probability-theory/2020/03/23/final</id><content type="html" xml:base="https://abhimanyu08.github.io/blog/probability-theory/2020/03/23/final.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-03-23-final.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.-Notations&quot;&gt;1. Notations&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-Notations&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;P(X)&lt;/strong&gt; = Probality of event X.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;P(X|Y)&lt;/strong&gt; = Probability of X given that Y has occured.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-Pre-requisites&quot;&gt;2. Pre-requisites&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-Pre-requisites&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;You need to have knowledge of basic probability theory. If you are comfortable in calculating probabilities of discrete events and comfortable with the sum rule and product rule then you're good to go (If you're not, don't worry, I've tried to give a terse explaination using an example below). Try out this question.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Que&lt;/strong&gt; - Bag A has 5 red and 3 blue balls, Bag B has 6 red and 4 blue balls. The probability that a person chooses Bag A is 0.3 and he'll choose Bag B with probability 0.7. What is the probability of a person selecting a blue ball from bag B? What is the total probability of him coming out with a red ball?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ans&lt;/strong&gt; -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Probability of choosing blue ball from B = (He chooses B) and (and then he selected blue ball from B)&lt;/em&gt; i.e ${P(B)\times P(blue\mid B)}$. If your answer is 0.28, then you're comfortable with the product rule.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Probability of him coming out with a red ball = (He chooses red ball from A) or (he chooses red ball from B)&lt;/em&gt; i.e ${P(A)\times P(red\mid A)+ P(B)\times P(red\mid B)}$. If your answer comes out to be 0.6075, then you're comfortable with the sum rule.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Intuitively, sum rule comes in play when there is a choice between mutually exclusive events  (these events are generally seperated by a &lt;strong&gt;or&lt;/strong&gt; between them) for eg. The event of (either choosing A and then a red ball) &lt;strong&gt;or&lt;/strong&gt; (B and then a red ball) (as shown in second bullet point above). Product rule comes into play when two events occur simultaneously for eg. choosing bag B &lt;strong&gt;and&lt;/strong&gt; then selecting a blue ball from it (as shown in first bullet point above).&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-Formula-and-basic-jargon&quot;&gt;3. Formula and basic jargon&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-Formula-and-basic-jargon&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Let me state the formula for Bayes' theorem quickly. Subsequently, I'll explain every term of the formula in detail using an example and introduce some basic jargon along the way.&lt;/p&gt;
&lt;p&gt;${\displaystyle \large P(A\mid B)={\frac {P(B\mid A)P(A)}{P(B)}}}$   (3.0)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Let's try to understand this formula by investigating a theft incidence.&lt;/p&gt;
&lt;p&gt;There are two secret vaults A and B. Vault A contains 6 gold bars and 2 diamonds. Vault B contains 2 gold bars and 8 diamonds. Vault A is easier to break open and therefore if a thief encounters both vaults he's more likely to break into A than in B. Let's say a thief will break in A with probability 0.7 and in B with probability 0.3. Unfortunately, despite all the security, a theft occurs. Due to security alarms, thief had to rush and could only take whichever piece of ornament he could get his hands on (In other words, he didn't have the time to think that diamond is more precious so I should pick diamond instead of gold). After the heroic efforts of the police, the thief is caught and he's found having a diamond. You are a detective but like Sherlock Holmes, unauthorised. You have a personal history with the thief and want to get some more lead into this case by knowing from which vault the theft occured. Sadly, being unauthorised, you're not allowed to enter the crime scene. Thus, you would have to guess the vault from which the thief has stolen the diamond? Let's say you hypothesise that he must have stolen from A because it was easier to break into. Subsequently, you go home and then sit down with a pen and paper to test the validity of your hypothesis using the Bayes' theorem&lt;/p&gt;
&lt;p&gt;Let us define some symbols first,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;${P(A)}$ = probability of breaking into A which is given as 0.7.&lt;/li&gt;
&lt;li&gt;${P(B)}$ = probability of breaking into B which is given as 0.3.&lt;/li&gt;
&lt;li&gt;${P(G)}$ = probability of stealing the gold bar.&lt;/li&gt;
&lt;li&gt;${P(D)}$ = probability of stealing the diamond.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You have to compute probability of thief having broken into A &lt;strong&gt;given that&lt;/strong&gt; diamond was found in his hand i.e ${P(A\mid D)}$.
Expanding this term using the formula 3.0 gives us:
                      ${\displaystyle P(A\mid D)={\frac {P(D\mid A)P(A)}{P(D)}}}$   (3.1)&lt;/p&gt;
&lt;p&gt;The sentence &quot;The thief was found having a diamond&quot; is called &lt;strong&gt;Evidence&lt;/strong&gt;. Probability of finding the &lt;strong&gt;Evidence&lt;/strong&gt; is written mathematically as ${P(D)}$ which is the denominator of our Eq.3.1.
To solve the question, we begin by listing all the ways through which &lt;strong&gt;Evidence&lt;/strong&gt; could have occured. There are two cases in which diamond could've been stolen:-&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;He broke into A &lt;strong&gt;and&lt;/strong&gt; then picked up a diamond i.e ${P(A)\times P(D\mid A)}$ &lt;strong&gt;or&lt;/strong&gt;  2. He broke into B &lt;strong&gt;and&lt;/strong&gt; then picked up a diamond i.e ${P(B)\times P(D\mid B)}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All these listed ways make up your ${P(D)}$ in Eq.3.1. Thus ${P(D)}$ written mathematically will be equal to -&amp;gt;
                       ${P(A)\times P(D\mid A) + P(B)\times P(D\mid B)}$ (3.2)&lt;/p&gt;
&lt;p&gt;Now, You will write down your &lt;strong&gt;Prior&lt;/strong&gt; belief in the correctness of your hypothesis &lt;strong&gt;if you had not seen&lt;/strong&gt; the &lt;strong&gt;evidence&lt;/strong&gt;. What are the odds that thief broke into A? The odds are simply ${P(A)}$ or 0.7. Thus, ${P(A)}$ is called &lt;strong&gt;Prior&lt;/strong&gt;, because it reflects your &lt;em&gt;prior belief&lt;/em&gt; about the correctness of your hypothesis &lt;em&gt;before seeing the&lt;/em&gt; &lt;strong&gt;Evidence&lt;/strong&gt;.
Now, what are the chances that he stole the diamond &lt;strong&gt;given that&lt;/strong&gt; he infiltrated A? In other words,what are the chances of seeing the &lt;strong&gt;Evidence&lt;/strong&gt; given that your hypothesis is true? Mathematically speaking, ${P(D\mid A)}$. This entity is called &lt;strong&gt;Likelihood&lt;/strong&gt;. Likelihood can be described as answer to the question &quot;How likely is that evidence occurs if I assume my hypothesis is true?&quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Likelihood&lt;/strong&gt; and &lt;strong&gt;Prior&lt;/strong&gt; make up the numerator in (3.1). Thus your numerator is 
                                             ${P(A)\times P(D\mid A)}$. (3.3)&lt;/p&gt;
&lt;p&gt;Why is the entity (3.3) our numerator and why not anything else? If you notice carefully (3.3) is probability of our case no 1 (He broke into A &lt;strong&gt;and&lt;/strong&gt; then picked up a diamond).
Intuitively, we can think of it as following:-
Out of all the cases that make up your evidence ${P(D)}$ you are only interested in the ones in which your hypothesis holds true. Thus only the case no 1 from above list of cases interests you and you put that in your numerator. And if you remember, that is basic probability; we calculate probability using the formula -&amp;gt; &lt;strong&gt;cases that interest us/total no of cases&lt;/strong&gt;, for eg. What are the odds of selecting a red card from a deck of cards -&amp;gt; 26/52 or 0.5.&lt;/p&gt;
&lt;p&gt;Thus,finally after putting all the pieces together we can calculate&lt;/p&gt;
&lt;p&gt;${\displaystyle P(A\mid D) = {\frac {P(A)\times P(D\mid A)}{P(D)}} = {\displaystyle \frac {Equation 3.3}{Equation 3.2}} = {\frac {P(A)\times P(D\mid A)}{P(A)\times P(D\mid A) + P(B)\times P(D\mid B)}} =  0.42}$&lt;/p&gt;
&lt;p&gt;The chances that your hypothesis is true is 42%. Put in other words, the chances of your hypothesis being wrong are 58%. Hence,it is more likely that he broke into B and not A! Now, reflecting on the details of the theft, you suddenly realise that &quot;Ah! That makes sense. There are more diamonds in B than in A and the thief hurriedly picked up whatever ornament he could get his hands on and got diamond. Thus,if he broke into B he had more chances of picking a diamond blindly than doing so after breaking into A. So he must have stolen from B&quot;
The calculated probability &lt;strong&gt;${P(A\mid D)}$&lt;/strong&gt; is called the &lt;strong&gt;Posterior&lt;/strong&gt;. This probability is an updated version of our &lt;strong&gt;Prior&lt;/strong&gt; based on new &lt;strong&gt;evidence&lt;/strong&gt;. Now that we found the evidence that thief had a diamond, we believe that he broke into A with probability 0.42 (&lt;strong&gt;posterior&lt;/strong&gt;) instead of 0.7 (&lt;strong&gt;prior&lt;/strong&gt;). In other words your belief in your hypothesis went down in the light of new evidence. This is the main motive of Bayes' theorem, It helps us update our &lt;strong&gt;Prior&lt;/strong&gt; beliefs continuously by collecting new &lt;strong&gt;Evidence&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&quot;Quick-summary-and-technique-to-solve-problems-involving-Bayes'-Theorem-:&quot;&gt;Quick summary and technique to solve problems involving Bayes' Theorem :&lt;a class=&quot;anchor-link&quot; href=&quot;#Quick-summary-and-technique-to-solve-problems-involving-Bayes'-Theorem-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Find out what is &lt;strong&gt;given&lt;/strong&gt; in the problem. The given part serves as &lt;strong&gt;Evidence&lt;/strong&gt; which aids us in assessing our &lt;strong&gt;Hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;List out all the ways in which &lt;strong&gt;Evidence&lt;/strong&gt; could have occured, calculate the probabilities of those ways using product rule and sum rule and write them as denominator.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pick out the way amongst the list of ways in which your &lt;strong&gt;Hypothesis&lt;/strong&gt; holds true and put the probability of that in the numerator.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.-Interesting-Study-Demonstrating-The-Counter-Intuitiveness-Of-The-Bayes'-Theorem&quot;&gt;4. Interesting Study Demonstrating The Counter-Intuitiveness Of The Bayes' Theorem&lt;a class=&quot;anchor-link&quot; href=&quot;#4.-Interesting-Study-Demonstrating-The-Counter-Intuitiveness-Of-The-Bayes'-Theorem&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;em&gt;(This part of blog is inspired from a great video by &lt;a href=&quot;https://youtu.be/HZGCoVF3YvM&quot;&gt;3 Blue 1 Brown&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let me ask you an interesting question.
&lt;strong&gt;Steve is very shy and withdrawn, invariably helpful but with little interest in people or in the world of reality. A meek and tidy soul, he has a need for order and structure, and a passion for detail.&quot;&lt;/strong&gt; Having read this sentence what do you think is the profession of Steve, a &lt;em&gt;librarian&lt;/em&gt; or a &lt;em&gt;farmer&lt;/em&gt; ?&lt;/p&gt;
&lt;p&gt;(&lt;em&gt;This quesion was asked by Nobel Laureate &lt;a href=&quot;https://en.wikipedia.org/wiki/Daniel_Kahneman&quot;&gt;Daniel Kahneman&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Amos_Tversky&quot;&gt;Amos Tversky&lt;/a&gt; in the studies which they conducted that showed that humans are intuitively bad staticians (even those who had PhDs in the field of statistics) and sometimes overestimate the correctness of their prior beliefs. Daniel Kahneman has written about these studies in his book &quot;Thinking ,fast and slow&quot;.&lt;/em&gt;)&lt;/p&gt;
&lt;p&gt;Most people would guess that Steve is a librarian because he fits in the stereotypical image of one. Let's look at this problem with a Bayesian perspective. Let's say that the sentence written in bold above is our &lt;strong&gt;evidence&lt;/strong&gt;. Now we &lt;strong&gt;hypothesise&lt;/strong&gt; that &lt;em&gt;Steve is a librarian&lt;/em&gt;. Let's calculate the validity of our hypothesis.&lt;/p&gt;
&lt;p&gt;Steve is a random person taken from a representative sample.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Let's say the probability of observing the above traits in a random person are ${P(E)}$.&lt;/li&gt;
&lt;li&gt;Let the probability of a random person being a farmer be ${P(F)}$.&lt;/li&gt;
&lt;li&gt;Let the probability of a random person being a librarian be ${P(L)}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We would have to consider following questions to calculate the probability of our hypothesis given the evidence :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Out of 100 librarians how many do you think fit the description given above in bold typeface? We are allowed to incorporate our stereotypes in estimating the answer to this question. Let's say 85 out of 100 librarians fit the evidence. Mathematically speaking, &lt;strong&gt;given that&lt;/strong&gt; a person is a librarian, the probability of him fiiting the above evidence (he is shy and a &quot;meek and tidy soul&quot;) is  ${P(E\mid L)}$ = 0.85&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Out of 100 farmers how many do you think fit the description given above in bold typeface? Let's say 30 farmers fit the evidence (beacuse we all stereotypically think that farmers are less likely to be shy or a &quot;meek and tidy soul&quot;). Mathematically speaking, &lt;strong&gt;given that&lt;/strong&gt; a person is a farmer, the probability of him fiting the above evidence is ${P(E\mid F)}$ = 0.3&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We also need to take into account some statistical facts to decide our prior beliefs. At the time of conduction of this study, there were 20 farmers for every 1 librarian in america. Thus, out of 210, 10 people were librarian and 200 are farmers.Therefore, probability of a random person being a farmer i.e ${P(F)}$ = 0.95 and probability of a random person being a librarian i.e ${P(L)}$ is 0.05 (assuming our representative sample has only farmers and librarians).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Listing all the ways in which the evidence can occur:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The person selected at random is a librarian &lt;strong&gt;and&lt;/strong&gt; he is a &quot;meek and tidy soul&quot; (${P(L)\times P(E\mid L)}$) &lt;strong&gt;or&lt;/strong&gt; 2. The person selected at random is a farmer &lt;strong&gt;and&lt;/strong&gt; he is a &quot;meek and tidy soul&quot; (${P(F)\times P(E\mid F)}$).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Writing this mathematically -&amp;gt; &lt;strong&gt;${P(E) = P(L)\times P(E\mid L) + P(F)\times P(E\mid F)}$&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The case which interests us is case 1. Thus, &lt;strong&gt;${\displaystyle P(L\mid E) = \frac{P(L)\times P(E\mid L)}{P(L)\times P(E\mid L) + P(F)\times P(E\mid F)}}$&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After doing the above calculation we find out that probability of Steve being a librarian is a mere 13 %. In other words, if you assemble 100 meek and tidy souls like Steve only 13 of them would turn out to be librarians. This seems surprising and counter-intuitive because we incorporated our stereotypes in our calculations (by saying that 85 out of 100 librarians fit the evidence), yet the final calculations conclude that our hypothesis (which complied with our stereotypes) was wrong.&lt;/p&gt;
&lt;p&gt;An intutive way of thinking about this is as following:&lt;/p&gt;
&lt;p&gt;There are way more farmers in general population than librarians, therefore there are way more &quot;meek and tidy souls&quot; ploughing the fields (77 out of 100 as per our calculations) than those who are meticulously keeping the books in the library. Take a sample of 210 people for example out of which 10 are librarians and 200 are farmers. According to our stereotypical estimates 85% of 10 librarians or ~9 librarians are shy, while only 30% of 200 lirarians or ~60 farmers are shy.Hence,out of 210 people 69 people are shy and tidy souls, majority of which are farmers. Thus, if we randomly picked a guy named Steve and he comes out as shy, he probably belongs to the group of 60 farmers.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;5.-Bayes'-Theorem-As-A-Way-Of-Updating-Our-Priors-And-Belief-Systems.&quot;&gt;5. Bayes' Theorem As A Way Of Updating Our Priors And Belief Systems.&lt;a class=&quot;anchor-link&quot; href=&quot;#5.-Bayes'-Theorem-As-A-Way-Of-Updating-Our-Priors-And-Belief-Systems.&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;em&gt;(This part of blog is inspired from this great video by &lt;a href=&quot;https://youtu.be/R13BD8qKeTg&quot;&gt;Veritasium&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Suppose, you go to a doctor and he tells you that results of your test for a disease are unfortunately positive. It is known that 0.1% of the population might have the disease. You know that the tests you took give correct results 99% of the time. Thus, you may be disheartened because such an accurate test has declared you of being sick from a rare disease. Intuitively, you would think that there is a 99% chance of you having this disease. But, let's look at this from a bayesian perspective.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt; -&amp;gt; The test shows positive. ${P(E)}$ = 0.99&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hypothesis&lt;/strong&gt; -&amp;gt; You have the disease &lt;strong&gt;given&lt;/strong&gt; the evidence. ${P(D\mid E)}$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prior belief before seeing the evidence&lt;/strong&gt; -&amp;gt; Probaility of you having the disease before you went for tests. ${P(D)}$ = 0.001 (because 0.1% of the population has it and you're part of the population)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Ways In Which Evidence Can Be Observed (Test result Can Come Out As Positive)&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You have the disease &lt;strong&gt;and&lt;/strong&gt; test comes as positive (${P(D)\times P(E\mid D)}$). &lt;strong&gt;or&lt;/strong&gt; 2. You don't have the disease &lt;strong&gt;and&lt;/strong&gt; test shows positive (incorrectly) (${P(\neg D).P(E\mid \neg D)}$).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mathematically -&amp;gt; &lt;strong&gt;${P(D) = P(D)\times P(E\mid D) + P(\neg D).P(E\mid \neg D)}$&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are interested in case 1.&lt;/p&gt;
&lt;p&gt;Thus, probability of you having the disease &lt;strong&gt;given&lt;/strong&gt; positive test results ${P(D\mid E)}$ = ${\displaystyle \frac {P(D)\times P(E\mid D)}{P(D)\times P(E\mid D) + P(\neg D)\times P(E\mid \neg D)}}$.&lt;/p&gt;
&lt;p&gt;After calculations, the probability of you having the disease comes out to be a mere 9%, which again seems counter-intuitive. Even after being declared positive by a pretty accurate test you are probably healthy and test is False!&lt;/p&gt;
&lt;p&gt;This counter-intuitive result stems from the fact that probability of our &lt;strong&gt;hypothesis given the evidence&lt;/strong&gt; is directly proportional to our &lt;strong&gt;prior&lt;/strong&gt; i.e probability of our hypothesis being correct without the evidence (${P(D)}$ in above calculation). In this particular example, the probability of us having the disease without having the test results in our hand was so low (0.001) that even the new strong evidence couldn't vote in favour of our hypothesis that we have the disease.&lt;/p&gt;
&lt;p&gt;Think of just 1000 people which also includes you. According to given data, 1 out of these 1000 is sick from the disease. Let's say that he goes for the test and is correctly identified as positive. The other 999 also go for tests. The test will falsely identify 1% of 999 healthy people, i.e 10 healthy people are shown positive. So now, there are 11 people in entire population with positive test results and you are one of them. Out of these 11 positive test results only 1 is correct. That's why having a positive result in first trial is not as bad as you might think!&lt;/p&gt;
&lt;h3 id=&quot;But-What-If-You-Took-A-Second-Test-And-It-Comes-As-Positive&quot;&gt;But What If You Took A Second Test And It Comes As Positive&lt;a class=&quot;anchor-link&quot; href=&quot;#But-What-If-You-Took-A-Second-Test-And-It-Comes-As-Positive&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Suppose just to be sure, you go through tests from a different lab and the result again comes out as positive (assuming that this lab also gives correct results 99% of the times). Now, what are the chances that you have the disease. Let's agai hypothesise that you have the disease and test the validity of out hypothesis. Everything remains the same in terms of data except the &lt;strong&gt;prior&lt;/strong&gt;. The basic definition of the prior is &lt;strong&gt;&quot;Probability that your hypothesis is true before collecting the evidence&quot;&lt;/strong&gt;. Thus, in this case the &lt;strong&gt;prior&lt;/strong&gt; is probability of you having the disease without having seen the results from second test. Therefore, prior should be 9% or 0.09 for the second case (&lt;strong&gt;Posterior&lt;/strong&gt; from the first test). Even though the earlier test was likely to be false, it served us by updating our &lt;strong&gt;prior&lt;/strong&gt; from 0.001 to 0.09 by providing us with a strong evidence.&lt;/p&gt;
&lt;p&gt;The probability of having the disease &lt;strong&gt;given&lt;/strong&gt; that second test result is also positive = 
${\displaystyle \frac {0.99\times 0.09}{0.99\times 0.09 + 0.01\times 0.91}}$ = ${91 \%}$.&lt;/p&gt;
&lt;p&gt;Thus, now you have 91% chances of being sick and intuitively this makes sense because the chances of two such accurate tests showing false results are pretty low.&lt;/p&gt;
&lt;p&gt;You had a hypothesis that you are sick with 0.1 % odds. Then, you collected a evidence by going through a test and that evidence updated your belief in your hypothesis to 9%. Subsequently, you went out to collect another evidence by going through another test. That test further updated your belief in hypothesis to 91%.&lt;/p&gt;
&lt;p&gt;This case shows that Bayes' theorem serves us by updating our priors with help of new evidences. The posteriors serve as priors for the next time any evidence is collected. This process iteratively helps in scientifically solidifying or falsifying our hypotheses by regularly collecting new evidences and updating our Priors subsequently.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;em&gt;If you notice a mistake in this blog post please mention them in the comment section or email them to me at iamabhimanyu08@gmail.com, I'll make sure to correct them right away.&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://abhimanyu08.github.io/blog/images/1.jpg" /><media:content medium="image" url="https://abhimanyu08.github.io/blog/images/1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>