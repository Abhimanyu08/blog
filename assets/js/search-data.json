{
  
    
        "post0": {
            "title": "What Does Label Smoothing Do?",
            "content": "Introduction . Label smoothing was introduced by Szegedy et.al in the paper Rethinking the Inception Architecture. Since then this trick has been used in many papers to improve the SOTA results on many datasets in various architectures. Although being widely used, there is less insight as to why this technique helps the model to perform better. The paper by Rafael Müller et.al When does Label Smoothing Help? provides insight into this question. This blog post is an attempt to explain the main result of the paper. . What Is Label Smoothing? . Generally, in a classification problem, our aim is to maximize the log-likelihood of our label where label is based upon the ground truth. In other words, we want our model to assign maximum probability to the true label given the parameters and the input i.e ${P( hat y mid x, theta)}$ where the true label is known beforehand. We motivate our model to achieve this by minimizing the cross-entropy loss between the predictions and the ground truth labels. Cross entropy loss is defined by the equation: ${- sum_{i=1}^{n} y_{i} times log( hat y_{i}) }$ where n is the number of classes for.eg for Imagenet n = 1000. Don&#39;t be intimidated by the daunting equation and jargon because in reality the calculation of loss becomes very easy as the labels are provided as one-hot encoded vectors. Suppose you build a model for task of image-classification between 3 classes. For every image as the input the model predicts a 3 length vector. Let&#39;s say for image 1 the model&#39;s normalised predictions are ${ hat y = [0.2, 0.7,0.1]}$ and the image belongs to category 2. Therefore, the target vector will be ${y = [0,1,0]}$. The loss for this image will be ${-(0 times log 0.2 + 1 times log 0.7 + 0 times log 0.1) = - log 0.7}$. There is little more to how the normalised predictions of the model are calculated. The model&#39;s predictions are calulated by using the activation Softmax in the last layer&#39;s output. The model outputs a length 3 vector and each of the element of the vector is called &#39;logit&#39;. For the logits to represent a valid probability distribution over the classes they should sum to 1. This is accomplished by passing the logits through a softmax layer. Let&#39;s say the output vector for a certain image as input is ${z = [z_{1}, z_{2},...,z_{n}]}$ then the predictions are calculated as ${ hat y = text Softmax left(z right) = large [ frac {e^{z_{1}}}{ sum_{i=1}^{n} e^{z_{i}}}, frac {e^{z_{2}}}{ sum_{i=1}^{n} e^{z_{i}}}... frac {e^{z_{n}}}{ sum_{i=1}^{n} e^{z_{i}}}]}$. Notice that sum of all the elements of ${ hat y}$ is 1. Suppose the ground truth label for the image is 2, then the target vector is ${[0,1,0,0,....0]}$ (The length of target vector is n as well). Thus, the Cross-entropy loss for this image,in it&#39;s full glory is written as ${ text loss left(y,z right) = -1 times normalsize log frac {e^{z_{2}}}{ sum_{i=1}^{n} e^{z_{i}}} = log { sum_{i=1}^{n} e^{z_{i}}} - z_{2}}$. Minimising this loss encourages ${z_{2}}$ to be as high as possible while ${z_{i}}$ for ${i ne2}$ are encouraged to be close to 0. Szegedey et.al highlight two problems with this approach . The problem with this approach is that model becomes over-confident for it&#39;s predictions as it assigns nearly 100% probability to the ground label. Szegedy et. al argue that this can lead to overfitting and model may not be able to generalize well. Intuitively this makes sense. for.eg Let&#39;s say our dataset contains two symantically similar classes (pets dataset has plenty of those). Suppose image1 belongs to one of the classes and image2 to other. Because these images are very similar, the output logits of these images would be very similar. Our over-confident model may assign other class to the images with high confidence(close to 100% probability) and thus our validation loss will be very high. . The other problem with this approach is the vanishing gradient. The gradient of our loss w.r.t logit of correct class label k is ${ large frac {e^{z_{k}}}{ sum_{i=1}^{n} e^{z_{i}}}-1}$ and w.r.t other logits is ${ large frac {e^{z_{i}}}{ sum_{i=1}^{n} e^{z_{i}}}}$. Minimising the Cross-entropy loss leads to logit corresponding to correct class to be much higher than other logits. This leads to vanishing of gradients of loss w.r.t other logits and thus it hinders the model&#39;s ability to adapt. . What can we do to cunteract these two problems. The Label smoothing paper suggests that we shouldn&#39;t provide sparse one-hot encoded vectors as target. Instead we should smoothen them. This is done by replacing the probability distribution over labels from dirac delta function to a linear combination of dirac delta distribution and a uniform distribution. This may sound incredibly complex to hear but in reality is very easy to implement. Let&#39;s define what the above jargon means. . Dirac delta function denoted by ${ delta _{k,y}}$ is a function which is 1 for ${k=y}$ and 0 everywhere else. (So,it&#39;s a fancy name for one-hot encoded vector). If a image has class 3 as it&#39;s label and there are 4 classes in total, then the target vector for that image has the probability distribution ${ delta _{k,3} = [0,0,1,0]}$. Notice that ${ delta _{k,y}}$ is a valid probability distribution as it sums to 1 over it&#39;s domain. A uniform distribution is a distribution which has a constant value over it&#39;s domain. Let&#39;s say our domain consists of ${[1,2,3,4]}$. Uniform distribution is denoted as ${U left(x right)}$. For uniform distribution ${U left(1 right) = U left(2 right) = U left(3 right) = U left(4 right) = c}$. The value of c should be ${ frac {1}{ text total ,number ,of ,domain ,points} = 0.25}$ so that ${ sum_{i=1}^{4} U left(i right)}$ is 1. . Let&#39;s denote our target vector for a particular image as ${q left(k,y right)}$.Here ${k}$ denotes the total no of classes and ${y}$ denotes the true label for the image. In case of one hot-encoded target vector ${q left(k,y right) = delta _{k,y}}$. Szegedy et. al propose to replace ${ delta _{k,y}}$ with ${(1- epsilon) times delta _{k,y} + epsilon times U left(k right)}$. As explained above value of ${U left(k right)}$ should be ${ frac {1}{k}}$. Thus our target vector ${q left(k,y right) = (1- epsilon) times delta _{k,y} + epsilon times frac{1}{k}}$. Let&#39;s try to smooth the labels of a concrete example. . Suppose target vector of an image for a classification task which has ${k=4}$ classes is ${q left(k,y right)= delta_{k,2} = [0,1,0,0]}$.A valid uniform distribution over the labels is defined as ${U left(k right) = frac{1}{k} = 0.25}$.Then,our smoothed target vector is ${q left(k,y right) = (1- epsilon) times delta _{k,2} + epsilon times U left(k right)}$ = ${(1- epsilon) times[0,1,0,0] + epsilon times [0.25,0.25,0.25,0.25]}$ = ${[0.25 epsilon, 1- epsilon+0.25 epsilon, 0.25 epsilon,0.25 epsilon]}$. If ${ epsilon = 0.2}$,then ${q left(k,y right)=[0.05,0.85,0.05,0.05]}$. Notice that new smoothened labels still sum to 1, which confirms that ${(1- epsilon) times delta _{k,y} + epsilon times U left(k right)}$ is a valid probability distribution over the labels. . Intuitively we can think label smoothing as a process to reduce the confidence of model in it&#39;s ground truth labels.The ground truth labels may sometimes be awry owing to errors in data labelling or data collection process. Label smoothing can make the model robust against incorrect labels. . Implementation In Code . To implement label smoothing, we don&#39;t change every label individually but we define a new loss function. Loss function is still Cross-entropy loss. Our new target vector for a particular image is ${ y = [ frac { epsilon}{k}, frac { epsilon}{k},...,(1 - epsilon) + frac{ epsilon}{k}, frac { epsilon}{k}, frac { epsilon}{k},...k times]}$. Let&#39;s assume the image belongs to class ${j}$. Normal one hot encoded target label will have 1 at j position and 0 everywhere else. Let&#39;s denote one hot encoded target vector as ${y^{h}}$. So, ${y^{h} = [0,0,0,...,1,0,...0]}$ The loss is ${L left( hat y ,y right) = sum_{i=1}^{k} -y_{i} times log hat y_{i}}$ = ${- left( frac { epsilon}{k} times log hat y_{1} + frac { epsilon}{k} times log hat y_{2} + ...+ left(1- epsilon+ frac{ epsilon}{k} right) times log hat y_{j}+ frac { epsilon}{k} times log hat y_{j+1}+...+ frac { epsilon}{k} times log hat y_{k} right)}$. We can rewrite this as ${L left( hat y ,y right) = left(1- epsilon right) times log hat y_{j} + frac{ epsilon}{k} times left( sum_{i=1}^{k} hat y_{i} right)}$. Eagle eyed reader can notice that term which is multiplied by ${ left(1 - epsilon right)}$ is the same loss we calculated with one hot encoded target vector. Therefore, ${L left(y, hat y right) = left(1- epsilon right) times L left(y^{h}, hat y right)+ frac{ epsilon}{k} times left( sum_{i=1}^{k} hat y_{i} right)}$. . Thus, we only need to modify the loss function of our model and we are good to go. The implementation of this in code is shown below. The code snippet below uses Pytorch framework and implementation is copied from the fast.ai course. . def lin_comb(a1,a2,factor): return factor*a1 + (1-factor)*a2 class LabelSmoothing(nn.Module): def __init__(self, f:float=0.1, reduction = &#39;mean&#39;): super().__init__() self.f = f self.reduction = reduction def forward(self,pred,targ): ls = F.log_softmax(pred, dim = 1) l1 = reduce_loss(ls.sum(1), self.reduction) l2 = F.nll_loss(ls, targ,reduction= self.reduction) return lin_comb(-l1/pred.shape[-1],l2,self.f) .",
            "url": "https://abhimanyu08.github.io/blog/deep-learning/2020/05/17/final.html",
            "relUrl": "/deep-learning/2020/05/17/final.html",
            "date": " • May 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Bayes' Theorem",
            "content": "1. Pre-requisites . You need to have knowledge of basic probability theory. If you are comfortable in calculating probabilities of discrete events and comfortable with the sum rule and product rule then you&#39;re good to go (If you&#39;re not, don&#39;t worry, I&#39;ve tried to give a terse explaination using an example below). Try out this question. | . Que - Bag A has 5 red and 3 blue balls, Bag B has 6 red and 4 blue balls. The probability that a person chooses Bag A is 0.3 and he&#39;ll choose Bag B with probability 0.7. What is the probability of a person selecting a blue ball from bag B? What is the total probability of him coming out with a red ball? . Ans - . Probability of choosing blue ball from B = (Probability of choosing B).(Probability of him picking blue ball given that he selected B). If your answer is 0.28, then you&#39;re comfortable enough with the product rule!! . | Probability of him coming out with a red ball = (Prob. of choosing red ball from A) + (Prob. of choosing red ball from B).(Notice that you can calculate entities inside bracket using the concept in first bullet point).If your answer comes out to be 0.6075, then you&#39;re comfortable enough with the sum rule to go through this blog post !! . | . Intuitively, sum rule comes in play when there is a choice between independent events (these events are generally seperated by a or between them) for eg. The event of either choosing A and then a red ball or B and then a red ball (as shown in second bullet point above). Product rule comes into play when two events occur consecutively for eg. choosing bag B and then selecting a blue ball from it (as shown in first bullet point above). . 2. Notations . P(X) = Probality of event X. for eg. in above question ${P(A)}$ = 0.3 where A = Event of person selecting bag A | P(X|Y) = Probability of X given that Y has occured. for eg. in above question Probability of him picking blue ball given that he selected bag B = ${P(blue mid B)}$. | . 3. Formula and basic jargon . Let me state the formula for Bayes&#39; theorem quickly. Subsequently, I&#39;ll explain every term of the formula in detail using an example and introduce some basic jargon along the way. . ${ displaystyle large P(A mid B)={ frac {P(B mid A)P(A)}{P(B)}}}$ (3.0) . Let&#39;s try to understand this formula by investigating a theft incidence. . There are two secret vaults A and B. Vault A contains 6 gold bars and 2 diamonds. Vault B contains 2 gold bars and 8 diamonds. Vault A is easier to break open amongst the two and thus if a thief encounters both vaults he&#39;s more likely to break into A than in B. Let&#39;s say a thief will break in A with probability 0.7 and in B with probability 0.3. Unfortunately, despite all the security, a theft occurs. Due to security alarms, thief had to rush and could only take whichever piece of ornament he could get his hands on (In other words, he didn&#39;t have the time to think that diamond is more precious so I should pick diamond instead of gold). After the heroic efforts of the police, the thief is caught and he&#39;s found having a diamond. You are a detective but like Sherlock Holmes, unauthorised. You have a personal history with the thief and want to get some more lead into this case by knowing from which vault the theft occured. Sadly, being unauthorised, you&#39;re not allowed to enter the crime scene. Thus, you would have to guess from which vault the thief has stolen the diamond,A or B? Let&#39;s say you hypothesise that he must have stolen from A because it was easier to break into. Subsequently, you go home and then sit down with a pen and paper to test the validity of your hypothesis using the Bayes&#39; theorem . Let us define some symbols first, . ${P(A)}$ = probability of breaking into A which is given as 0.7. | ${P(B)}$ = probability of breaking into B which is given as 0.3. | ${P(G)}$ = probability of stealing the gold bar. | ${P(D)}$ = probability of stealing the diamond. | . You have to compute probability of thief having broken into A given that diamond was found in his hand i.e ${P(A mid D)}$. Expanding this term using the formula 3.0 gives us: ${ displaystyle P(A mid D)={ frac {P(D mid A)P(A)}{P(D)}}}$ (3.1) . The sentence &quot;The thief was found having a diamond&quot; is called Evidence. Probability of finding the Evidence is written mathematically as ${P(D)}$ which is the denominator of our Eq.3.1. To solve the question, we begin by listing all the ways through which Evidence could have occured. There are two cases in which diamond could&#39;ve been stolen:- . He broke into A and then picked up a diamond ${P(A) times P(D mid A)}$ or 2. He broke into B and then picked up a diamond ${P(B) times P(D mid B)}$. | All these listed ways make up your ${P(D)}$ in Eq.3.1. Thus ${P(D)}$ written mathematically will be equal to -&gt; ${P(A) times P(D mid A) + P(B) times P(D mid B)}$ (3.2) . Now, You will write down your Prior belief in the correctness of your hypothesis if you had not seen the evidence. What are the odds that thief broke into A?. The odds are simply ${P(A)}$ or 0.7. Thus, ${P(A)}$ is called Prior, because it reflects your prior belief about the correctness of your hypothesis before seeing the Evidence. Now, what are the chances that he stole the diamond given that he infiltrated A? In other words,what are the chances of seeing the Evidence given that your hypothesis is true? Mathematically speaking, ${P(D mid A)}$. This entity is called Likelihood. . Likelihood and Prior make up the numerator in (3.1). Thus your numerator is ${P(A) times P(D mid A)}$. (3.3) . Why is the entity (3.3) our numerator and why not anything else? If you notice carefully (3.3) is probability of our case no 1 (He broke into A and then picked up a diamond). Intuitively, we can think of it as following:- Out of all the cases that make up your evidence ${P(D)}$ you are only interested in the ones in which your hypothesis holds true. Thus only the case no 1 from above list of cases interests you and you put that in your numerator. And if you remember, that is basic probability; we calculate probability using the formula -&gt; cases that interest us/total no of cases, for eg. What are the odds of selecting a red card from a deck of cards -&gt; 26/52 or 0.5. . Thus,finally after putting all the pieces together we can calculate . ${ displaystyle P(A mid D) = { frac {P(A) times P(D mid A)}{P(D)}} = { displaystyle frac {Equation 3.3}{Equation 3.2}} = { frac {P(A) times P(D mid A)}{P(A) times P(D mid A) + P(B) times P(D mid B)}} = 0.42}$ . The chances that your hypothesis is true is 42%. Put in other words, the chances of your hypothesis being wrong are 58%. Hence,it is more likely that he broke into B and not A! Now, reflecting on the details of the theft, you suddenly realise that &quot;Ah! That makes sense, there are more diamonds in B than in A and the thief hurriedly picked up whatever ornament he could get his hands on and got diamond. Thus,if he broke into B he had more chances of picking a diamond blindly than doing so after breaking into A.&quot; The calculated probability ${P(A mid D)}$ is called the Posterior. This probability is an updated version of our Prior based on new evidence. Now that we found the evidence that thief had a diamond, we believe that he broke into A with probability 0.42 (posterior) instead of 0.7 (prior). This is the main motive of Bayes&#39; theorem, It helps us update our Prior beliefs continuously by collecting new Evidence. . Quick summary and technique to solve problems involving Bayes&#39; Theorem : . Find out what is given in the problem. The given part serves as Evidence which aids us in assessing our Hypothesis. . | List out all the ways in which Evidence could have occured, calculate the probabilities of those ways using product rule and sum rule and write them as denominator. . | Pick out the way amongst the list of ways in which your Hypothesis holds true and put the probability of that way in the numerator. . | . 4. Interesting Study Demonstrating The Counter-Intuitiveness Of The Bayes&#39; Theorem . (This part of blog is inspired from a great video by 3 Blue 1 Brown). . Let me ask you an interesting question. Steve is very shy and withdrawn, invariably helpful but with little interest in people or in the world of reality. A meek and tidy soul, he has a need for order and structure, and a passion for detail.&quot; Having read this sentence what do you think is the profession of Steve, a librarian or a farmer ? . (This quesion was asked by Nobel Laureate Daniel Kahneman and Amos Tversky in the studies which they conducted that showed that humans are intuitively bad staticians (even those who had PhDs in the field of statistics) and sometimes overestimate the correctness of their prior beliefs. Daniel Kahneman has written about these studies in his book &quot;Thinking ,fast and slow&quot;.) . Most people would guess that Steve is a librarian because he fits in the stereotypical image of one. Let&#39;s look at this problem with a Bayesian perspective. Let&#39;s say that the sentence written in bold above is our Evidence. Now we Hypothesise that Steve is a librarian. Let&#39;s calculate the validity of our hypothesis. . Steve is a random person taken from a representative sample. . Let&#39;s say the probability of observing the above traits in a random person are ${P(E)}$. | Let the probability of a random person being a farmer be ${P(F)}$. | Let the probability of a random person being a librarian be ${P(L)}$. | . We would have to consider following questions to calculate the probability of our hypothesis given the evidence : . Out of 100 librarians how many do you think fit the description given above in bold typeface? We are allowed to incorporate our stereotypes in estimating the answer to this question. Let&#39;s say 85 librarians fit the evidence. Mathematically speaking, given that a person is a librarian, the probability of him fiiting the above evidence (he is shy and a &quot;meek and tidy soul&quot;) is ${P(E mid L)}$ = 0.85 . | Out of 100 farmers how many do you think fit the description given above in bold typeface? Let&#39;s say 30 farmers fit the evidence (beacuse we all stereotypically think that farmers are less likely to be shy or a &quot;meek and tidy soul&quot;). Mathematically speaking, given that a person is a farmer, the probability of him fiting the above evidence is ${P(E mid F)}$ = 0.3 . | We also need to take into account some statistical facts to decide our prior beliefs. At the time of conduction of this study, there were 20 farmers for every 1 librarian in america. Thus, out of 210, 10 people were librarian and 200 are farmers.Therefore, probability of a random person being a farmer i.e ${P(F)}$ = 0.95 and probability of a random person being a librarian i.e ${P(L)}$ is 0.05 (assuming our representative sample has only farmers and librarians). . Listing all the ways in which the evidence can occur: . The person selected at random is a librarian and he is a &quot;meek and tidy soul&quot; (${P(L) times P(E mid L)}$) or 2. The person selected at random is a farmer and he is a &quot;meek and tidy soul&quot; (${P(F) times P(E mid F)}$). | Writing this mathematically -&gt; ${P(E) = P(L) times P(E mid L) + P(F) times P(E mid F)}$ . The case which interests us is case 1. Thus, ${ displaystyle P(L mid E) = frac{P(L) times P(E mid L)}{P(L) times P(E mid L) + P(F) times P(E mid F)}}$ . After doing the above calculation we find out that probability of Steve being a librarian is a mere 13 %. In other words only 13 out of 100 &quot;meek and tidy souls&quot; are librarians. This seems surprising and counter-intuitive because we incorporated our stereotypes in our calculations (by saying that 85 out of 100 librarians fit the evidence), yet the final calculations conclude that our hypothesis (which complied with our stereotypes) was wrong. . An intutive way of thinking about this is as following: . There are way more farmers in general population than librarians, therefore there are way more &quot;meek and tidy souls&quot; ploughing the fields (77 out of 100 as per our calculations) than those who are meticulously keeping the books in the library. Take a sample of 210 people for example out of which 10 are librarians and 200 are farmers. According to our stereotypical estimates 85% of 10 librarians or ~9 librarians are shy, while only 30% of 200 lirarians or ~60 farmers are shy.Hence,out of 210 people 69 people are shy and tidy souls, majority of which are farmers. Thus, if we randomly picked a guy named Steve and he comes out as shy, he probably belongs to the group of 60 farmers. . 5. Bayes&#39; Theorem As A Way Of Updating Our Priors And Belief Systems. . (This part of blog is inspired from this great video by Veritasium) . Suppose, you go to a doctor and he tells you that results of your test for a disease are unfortunately positive. It is known that 0.1% of the population might have the disease. You know that the tests you took give correct results 99% of the time. Thus, you may be disheartened because such an accurate test has declared you of being sick from a rare disease. Intuitively, you would think that there is a 99% chance of you having this disease. But, let&#39;s look at this from a bayesian perspective. . Evidence -&gt; The test shows positive. ${P(E)}$ = 0.99 | Hypothesis -&gt; You have the disease given the evidence. ${P(D mid E)}$ | Prior belief before seeing the evidence -&gt; Probaility of you having the disease before you went for tests. ${P(D)}$ = 0.001 (because 0.1% of the population has it and you&#39;re part of the population) | . Ways In Which Evidence Can Be Observed (Test result Can Come Out As Positive): . You have the disease and test comes as positive (${P(D) times P(E mid D)}$). or 2. You don&#39;t have the disease and test shows positive (incorrectly) (${P( neg D).P(E mid neg D)}$). | Mathematically -&gt; ${P(D) = P(D) times P(E mid D) + P( neg D).P(E mid neg D)}$ . We are interested in case 1. . Thus, probability of you having the disease given positive test results ${P(D mid E)}$ = ${ displaystyle frac {P(D) times P(E mid D)}{P(D) times P(E mid D) + P( neg D) times P(E mid neg D)}}$. . After calculations, the probability of you having the disease comes out to be a mere 9%, which again seems counter-intuitive. Even after being declared positive by a pretty accurate test you are probably healthy and test is False! . This counter-intuitiveness stems from the fact that probability of our hypothesis given the evidence depends heavily on our prior i.e probability of our hypothesis being correct without the evidence ${(P(D)}$ in above calculation). In this particular example, the probability of us having the disease without having the test results in our hand was so low (0.001) that even the new strong evidence couldn&#39;t vote in favour of our hypothesis that we have the disease. . Think of just 1000 people which also includes you. According to given data, 1 out of these 1000 is sick from the disease. Let&#39;s say that he goes for the test and is correctly identified as positive. The other 999 also go for tests. The test will falsely identify 1% of 999 healthy people, i.e 10 healthy people are shown positive. So now, there are 11 people in entire population with positive test results and you are one of them. Out of these 11 positive test results only 1 is correct. That&#39;s why having a positive result is not as bad as you might think! . But What If You Took A Second Test And It Comes As Positive . Suppose just to be sure, you go through tests from a different lab and that result also comes out as positive (assuming that that lab also gives correct results 99% of the times). Now, what are the chances that you have the disease.Everything remains the same in terms of data except the prior. The basic definition of the prior is &quot;Probability that your hypothesis is true before collecting the evidence&quot;. Thus, in this case the prior is probability of you having the disease without having seen the results from second test. Therefore, prior should be 9% or 0.09 for the second case (Posterior from the first test). Even though the earlier test was likely to be false, it served us by updating our prior from 0.001 to 0.09 by providing us with a strong evidence. . The probability of having the disease given that second test result is also positive = ${ displaystyle frac {0.99 times 0.09}{0.99 times 0.09 + 0.01 times 0.91}}$ = ${91 %}$. . Thus, now you have 91% chances of being sick and intuitively this makes sense because the chances of two such accurate tests showing false results are pretty low. . You had a hypothesis that you are sick with 0.1 % odds. Then, you collected a evidence by going through a test and that evidence updated your belief in your hypothesis to 9%. Subsequently, you went out to collect another evidence by going through another test. That test further updated your belief in hypothesis to 91%. . This case shows that Bayes&#39; theorem serves us by updating our priors with help of new evidences. The posteriors serve as priors for the next time any evidence is collected. This process iteratively helps in scientifically solidifying or falsifying our hypotheses by regularly collecting new evidences and updating our Priors subsequently. .",
            "url": "https://abhimanyu08.github.io/blog/probability-theory/2020/03/23/final.html",
            "relUrl": "/probability-theory/2020/03/23/final.html",
            "date": " • Mar 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://abhimanyu08.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}