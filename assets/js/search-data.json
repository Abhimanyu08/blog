{
  
    
        "post0": {
            "title": "Why Squared Error Minimization = Maximum Likelihood Estimation",
            "content": "Pre-requisites . Interchange between notations . Before starting out, I would just like to familiarise the readers with some notational somersaults we might perform in this blog. . Let ${S = x_{1}^{2} + x_{2}^{2} +x_{3}^{2}+...+ x_{m}^{2} }$. Another way of writing this equation can be . ${S = Sigma_{i=1}^{m} space x_{i}^{2}}$. . If we collect ${x_{1}, x_{2},...,{x_m}}$ in a single vector ${ small X}$, such that ${ small X = left[ begin{array}{ccc} x_{1} x_{2} . . x_{m} end{array} right]}$, then ${ small S}$ can be written as ${X^{T}X}$ . Minimizing the squared error loss . Minimizing the Squared Error is the technique anyone uses while tackling a regression problem in which the target is a continuous variable. Let&#39;s say we have collected all our input variables in a matrix ${X}$ of shape (m,n) where m is the number of training examples and ${i^{th}}$ row of ${X}$ or ${i^{th}}$ training example is represented by a n dimensional vector ${x_{i}}$. All the target variables are represented by a vector ${Y}$ of length m (each training example has a real number as it&#39;s target) and ${i^{th}}$ element of ${Y}$ is denoted by ${y_{i}}$. Our task is to find a vector ${ hat beta}$ of length n such that ${X hat beta = Y}$. Now this task can simply be solved by saying ${ hat beta = X^{-1}Y}$ but this is the correct answer only in the case when ${X}$ is a squared matrix and ${Y}$ lies in the column space of ${X}$ i.e m =n and Y is simply a weighted sum of columns of ${X}$. If that&#39;s not the case ${X hat beta}$ can never be equal to ${Y}$. All we can hope to do is minimize the distance between the two vectors ${X hat beta}$ and ${Y}$. The distance we choose to minimize is ${(Y-X hat beta)^{T}(Y-X hat beta)}$ which can also be written as ${ Sigma_{i=1}^{m} space (y_{i}-x_{i}^{T} hat beta)^{2}}$. Now, if you are like me, you may wonder why don&#39;t we simply minimize ${Y - X hat beta}$ or some other measure of distance. This is perfectly valid question to ask and we are going to explore this in this blog. For now, just remember that ${ hat beta}$ is the value of ${ beta}$ that minimises the squared error; written mathematically as : ${ hat beta = argmin_{ beta} space(Y-X beta)^{T}(Y-X beta)}$. . We can find ${ hat beta}$ using calculus. . ${ frac{ partial ((Y-X beta)^{T}(Y-X beta))}{ partial beta} = 0}$ at ${ hat beta}$ . ${ therefore small -X^{T}(Y - X hat beta) - X^{T}(Y - X hat beta) = 0 implies hat beta = (X^{T}X)^{-1}X^{T}Y}$ . Maximum Likelihood Estimation . Another way of looking at the regression task is that we have m observations ${(x_{1},y_{1}), (x_{2},y_{2}), ..., (x_{m},y_{m})}$ where each ${x_{i}}$ is a n dimensional vector and ${y_{i}}$ is it&#39;s corresponding scalar target value. These m data points can be thought of as m random samples from an unknown data distribution ${P(X,Y space; space theta)}$ where ${ theta_{true}}$ is the parameter of the distribution. If this doesn&#39;t make sense let&#39;s take an example. . Let&#39;s say we want to predict how many goals a soccer player will score in his next season by just looking at the player&#39;s age and number of goals he scored in last season. Therefore, our independent variable is random variable ${X = [ text Age, text # of goals ,scored , in , last , season]}$ and dependent variable is ${Y =}$ number of goals scored in next season where ${0 leq Y leq 100}$. Messi may be represented by vector ${(33,40)}$ and Ronaldo by ${(34,37)}$. Now, what do you think ${P([18,1], 25)}$ will be? In other words, what is the probability that a player aged 18 and having scored just one goal in previous season scores 25 in his next season? We can safely say that this a highly unlikely event since the player is inexperienced and the number of goals he scored in previous season suggests that he may be a defender. In the same way, we would expect ${P([28,40], 32)}$ to be high since the player is at an age when soccer players achieve their peak and is a proven goalscorer and therefore is highly likely to score 32 in next season. . Now instead of guessing, we want to construct a probability distribution over random variables ${X}$ and ${Y}$. Once we have such a distribution we can simply plug in age and goals scored in past season of any player and calculate ${P([ text age, text ranking], y space ; space theta_{true})}$ for each ${y}$ in ${(0,100)}$ and report the ${y}$ which gave the highest probability. Unfortunately, we can&#39;t know true distribution since for that we would have to collect data from every single active player which can be a lot. But, we can estimate the true distribution. Any distribution is entirely characterised by it&#39;s parameters. for eg a Gaussian distribution is characterised by it&#39;s mean and variance and is denoted as ${N (x; mu, sigma^{2})}$. If we can estimate the parameters of a distribution, then we&#39;re golden. Let&#39;s denote the estimate of parameter ${ theta_{true}}$ as ${ hat theta}$. . To construct such estimate of a distribution we collect the age and goals scored in previous season of m players and observe them for the next season and note down the number of goals they scored. Let each of these observations be represented by ${(x_{i}, y_{i})}$. We can see that some players may have the same ${x}$ either by virtue of error in data collection or they are of same age and scored the same number of goals in last season. Let&#39;s assume every observation is independent of any other observation. Given each ${x_{i}}$ we should predict ${y_{i}}$ and this can only happen when ${P(x_{i}, y_{i} space ; space hat theta)}$ is greater than all other ${P(x_{i}, y space ; space hat theta)}$ where ${y neq y_{i}}$. The quantity ${P(x_{i}, y_{i})}$ is known as likelihood of the observation ${(x_{i}, y_{i})}$. Since we want likelihood of all the observations to be maximum, we might as well say that we we want the quantity ${P(x_{1}, y_{1}) cdot P(x_{2}, y_{2}) cdot P(x_{3}, y_{3}) ... P(x_{m}, y_{m})}$ to be maximum which in short can be written as ${ Pi_{i=1}^{m}P(x_{i}, y_{i} space ; space theta)}$. Then ${ hat theta}$ will be whatever value of ${ theta}$ which can achieve this feat. . ${ therefore hat theta = argmax_{ theta} space Pi_{i=1}^{m} P(x_{i}, y_{i} space ; space theta)}$. . In real world since product of too many numbers which are less than one can lead to numeical underflow, we adjust the objective function as below: . ${ hat theta = argmax_{ theta} space log space Pi_{i=1}^{m} P(x_{i}, y_{i} space ; space theta) = argmax_{ theta} space Sigma_{i=1}^{m}log space P(x_{i}, y_{i} space ; space theta)}$. . This method of estimating ${ theta_{true}}$ is called maximum likelihood estimation and obtained ${ hat theta}$ is called maximum likelihood estimate of ${ theta_{true}}$. Maximum likelihood estimates of true parameters have many desirable properties, most important of which is: As we increase the number of training examples, the probability of estimates being close to true parameter increases. Well, this should ofcourse be true. What&#39;s special about the maximum likelihood estimates is that they do so the fastest of any other type of estimate. In other words, if we are using maximum likelihood estimates to estimate our parameters and someone else is using other methods, then our performance on prediction will improve faster on collecting more training data. . Minimising Squared Error ${ rightarrow}$ Maximising Likelihood . Let&#39;s get back to minimising squared error and see through a sequence of logical steps how it is maximum likelihood estimation in disguise. . ${ hat beta = argmin_{ beta} space(y_{1}-x_{1}^{T} beta)^2 + (y_{2}-x_{2}^{T} beta)^{2} + ... + (y_{m}-x_{m}^{T} beta)^2 = small {argmin_{ beta} space(Y-X beta)^{T} space I space(Y-X beta)}}$ where ${I}$ is the identity matrix. . As mentioned in previous section, two or more observations may have same ${x_{i}}$ but different ${y_{i}}$ in the dataset. For generality let&#39;s assume that only p out of m (where p &lt;m) ${x_{i}}$&#39;s are unique. We can then collect duplicate ${x}$&#39;s in a seperate matrix for each one and their correspoding targets can be arranged in a vector. This way we&#39;ll have p matrices viz. ${X_{1}, X_{2}, ..., X_{p}}$ each containing different number of rows where all the rows of a given ${X_{i}}$ are identical (since they carry all the duplicate ${x_{i}}$). . We can understand what we just did by a concrete examples. Let&#39;s say we have 5 observations: ${[(1,2), 10], , [(1,3),9], , [(1,2), 12], , [(1,4),15], , [(1,3),11]}$. We can see some duplicate ${x}$&#39;s in these observations. There are only three unique values of ${x}$&#39;s. Arranging the duplicate ${x}$&#39;s in a matrix and their corresponding labels in vectors we get: ${X_{1} = left[ begin{array}{ccc} 1 &amp; 2 1 &amp; 2 end{array} right]}$, ${X_{2} = left[ begin{array}{ccc} 1 &amp; 3 1 &amp; 3 end{array} right]}$, ${X_{3} = left[ begin{array}{ccc} 1 &amp; 4 end{array} right]}$; ${Y_{1} = left[ begin{array}{ccc} 10 12 end{array} right]}$, ${Y_{2} = left[ begin{array}{ccc} 9 11 end{array} right]}$, ${Y_{3} = left[ begin{array}{ccc} 15 end{array} right]}$ . By doing such an operation squared loss can be re-written as: . ${ hat beta = argmin_{ beta} space (Y_{1} - X_{1} beta)^{T}(Y_{1} - X_{1} beta) space+ space ... space + space (Y_{p} - X_{p} beta)^{T}(Y_{p} - X_{p} beta)}$ . Now we&#39;ll go modify this objective function through some steps which may look nonsensical at first but will yield an fascinating interpretation in the end. So, please bear with me for a while. . Mutltiplying a function by a negative number turns it&#39;s minimas to maximas and vice versa. So let&#39;s multiply our function by ${-0.5}$ and change argmin to argmax. . ${ therefore hat beta = argmax_{ beta} space -0.5[ space(Y_{1} - X_{1} beta)^{T}(Y_{1} - X_{1} beta) space+ space ... space + space (Y_{p} - X_{p} beta)^{T}(Y_{p} - X_{p} beta) space]}$ . Exponentiating the function and then taking it&#39;s log returns the function itself, because log and exp are inverse functions of each other. . ${ therefore hat beta = argmax_{ beta} space log space e^{-0.5[ space(Y_{1} - X_{1} beta)^{T}(Y_{1} - X_{1} beta) space+ space ... space + space (Y_{p} - X_{p} beta)^{T}(Y_{p} - X_{p} beta) space]}}$ . ${ therefore hat beta = argmax_{ beta} space log space e^{ small -0.5 space(Y_{1} - X_{1} beta)^{T}(Y_{1} - X_{1} beta)} space+ space ... space + space log space e^{ small -0.5 space(Y_{p} - X_{p} beta)^{T}(Y_{p} - X_{p} beta) space}}$ . Subtracting a constant from a function doesn&#39;t change the location of it&#39;s maximas and minimas. So, let&#39;s subtract ${log (2 pi)^{0.5m}}$ where m is the number of observations in our dataset . ${ therefore hat beta = argmax_{ beta} space log space e^{ small -0.5 space(Y_{1} - X_{1} beta)^{T}(Y_{1} - X_{1} beta)} space+ space ... space + space log space e^{ small -0.5 space(Y_{p} - X_{p} beta)^{T}(Y_{p} - X_{p} beta) space} - log (2 pi)^{0.5m}}$. . We can see that ${log (2 pi)^{0.5m} = log(2 pi)^{0.5(k_{1} space + space ... space + space k_{p})} = log(2 pi)^{0.5k_{1}} space + space ... space+ space log(2 pi)^{0.5k_{p}}}$ where ${k_{i}}$ is length of vector ${Y_{i}}$. . ${ therefore hat beta = argmax_{ beta} space [ space log space e^{ small -0.5 space(Y_{1} - X_{1} beta)^{T}(Y_{1} - X_{1} beta)} - log(2 pi)^{0.5k_{1}}] space+ space ... space + space [log space e^{ small -0.5 space(Y_{p} - X_{p} beta)^{T}(Y_{p} - X_{p} beta) space} - log(2 pi)^{0.5k_{p}}]}$. . Solving further, . ${ hat beta = argmax_{ beta} space log space frac{e^{ small -0.5 space(Y_{1} - X_{1} beta)^{T}(Y_{1} - X_{1} beta)}}{(2 pi)^{0.5k_{1}}} space+ space ... space + space log space frac{e^{ small -0.5 space(Y_{p} - X_{p} beta)^{T}(Y_{p} - X_{p} beta)}}{(2 pi)^{0.5k_{p}}}}$. . Just to jog your memory, let me write the expression of probability density of a random variable ${W}$ which has a multivariate Normal distribution whose mean vector is ${ mu}$ and covariance matrix is ${I}$ . ${P_{normal}( small W space ; mu, small I) = frac{ normalsize exp frac{-1}{2}(W- mu)^{T} space(W- mu)}{ normalsize{(2 pi)^{0.5k}}}}$ where k is the length of mean vector. This equation looks strikingly familiar to the equation of our morphed square minimization objective. . ${ therefore hat beta = argmax_{ beta} space log P_{normal}(Y_{1} space ; space X_{1} beta, I) space + space ... + space log P_{normal}(Y_{p} space ; space X_{p} beta, I)}$ . This means that by minimising squared error we were indeed maximising the log-likelihood of several random variables. Also, we were implicitly assuming that given a particular ${x}$, labels associated with it are gaussian random variable with of variance 1. . Now we have turned the task of finding the argument that minimises squared error into the task of finding the argument that maximises the likelihood of multivariate gaussian distribution which is very easy if you remember what a Gaussian curve looks like. If not, given below is the picture of a graph of probablity density of a random variable with gaussian distribution which has scalar mean ${ mu}$ and scalar variance ${ sigma}$ i.e ${P_{normal}(X space ; space mu, sigma)}$ . . In this picture, we can see that maximum of the distribution occurs at the mean of random variable also called expectation of random variable and denoted as ${E[X]}$. This means that ${P_{normal}(Y_{i} space ; space X_{i} beta, I)}$ is maximised when ${X_{i} beta}$ is mean of variable ${Y_{i}}$. . ${ therefore X_{i} hat beta = E[Y_{i}]}$. . This means that for a given ${x}$ in the dataset if we predict the mean of all the labels it is associated with, the squared error will be at it&#39;s minimum. . A Small Example . Let&#39;s take our previous dataset ${([(1,2), 10], , [(1,3),9], , [(1,2), 12])}$ and try to fit a linear model on it by minimizing the squared error. Arranging input and target variables seperately we have ${X = left[ begin{array}{ccc} 1 &amp; 2 1 &amp; 3 1 &amp; 2 end{array} right]}$ and ${Y = left[ begin{array}{ccc} 10 9 12 end{array} right]}$. As we saw earlier we can use a simple formula to find the vector ${ hat beta}$ such that ${X hat beta approx Y}$ which is ${ hat beta = (X^{T}X)^{-1}X^{T}Y}$ . . But why go this route when we can calculate ${X hat beta}$ in our heads. For a given ${x}$, just predict the mean of all the ${y}$&#39;s it is associated with. . Therefore for ${(1,2)}$ we predict mean of 10 and 12 which is 11 and for ${[1,3]}$ we predict just 9. . ${ therefore X hat beta = left[ begin{array}{ccc} 11 9 11 end{array} right]}$ . Let&#39;s verify that the normal equations give the same results: . import numpy as np X = np.array([[1,2], [1,3], [1,2]]) Y = np.array([10,9,12]) B = np.linalg.inv((X.transpose()@X))@(X.transpose()@Y) print(X@B) . [11. 9. 11.] . Conclusion . Since Squared Error Minimization is just Maximum Likelihood Estimation in disguise, we can see the reason for it&#39;s widespread adoption for regression tasks. Maximum likelihood estimates of the parameters of models have many desirable properties which is why we design our minimization objectives around them. Of course, in large datasets things aren&#39;t as simple as just applying normal equations to get to minimum of squared error, since the&#39;re may not be a linear relation between inputs and outputs in the first place or our implicit bias that given an input it&#39;s labels are a gaussian random variable with unit variance may be flawed. Therefore, in those cases ${ hat beta}$ is found out by descending the gradient of the objective function. .",
            "url": "https://abhimanyu08.github.io/blog/deep-learning/mathematics/2021/06/18/final.html",
            "relUrl": "/deep-learning/mathematics/2021/06/18/final.html",
            "date": " • Jun 18, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "King is to queen as man is to?",
            "content": "Introduction . Let&#39;s imagine that we are the first research scientists ever who are attempting to design some algorithms which will make our computer able to make some sense of human language (We&#39;ll select English as the human language for sake of this blog). We start by first finding tasks which our computer should perform with certain level of competence if it finally gets the abilities we want to bless it with. Trying to solve the problem of analogies seems like a reasonable task for this purpose. If we ask our computer king is to queen as man is to ___ ? Hopefully, our computer should reply &quot;woman&quot;? . Looking at the task at hand closely, we realize that we can&#39;t just type in our computer &quot;King is to queen as man is to what?&quot; and expect it to answer. Our computer isn&#39;t a human which can make sense of english symbols yet. The written language is a symbolic representation of our thoughts. We encode our thoughts in symbols so that we can convey those to other human. People who are unable to speak encode their thoughts in form of sign languages. There is one problem with process of conveying thoughts to other person by encoding them in symbols. The other person should know how to decode those symbols. You are able to decode the symbolic representation of English which you are so effortlessly doing right now while reading this blog. But if this blog was written in a language you didn&#39;t know, then this whole blog will just be a collection of meaningless symbols to you. Therefore, we have to encode our thoughts or messages by the rules which the receiver knows how to decode. In our case, the receiver is a computer, therefore, we have to convey our query to the computer in a form which it can understand. . Let&#39;s suppose in the end we design a function find_analogy which takes in three arguments arg1, arg2 and arg3. It does some mathematical computations on these three arguments and returns a word such that the word is answer to question arg1 is to arg2 as arg3 is to what. If we want answer to our question &quot;King is to queen as man is to what?&quot; we need to run the line of code find_analogy(king, queen, man). But our computer doesn&#39;t know what the word &quot;king&quot;,&quot;queen&quot; and &quot;man&quot; mean. We have to encode the words in a form the computer understands. Well, we know that a computer can make sense of mathematical entities like scalars and vectors. Why don&#39;t we encode every word in dictionary so that each is represented by a unique scalar/vector. . Let&#39;s start with simplest solution and represent our words with scalars such that each word is represented by it&#39;s position in the standard English dictionary. So the word &#39;aardvark&#39; is represented by scalar 1 and word &quot;zebra&quot; is represented by scalar 273000 (approximate number of words in Oxford dictionary). We can immediately sense something wrong about this representation. What would adding or subtracting representations of two words even mean? The analogy task by which we intend to challenge understanding of our computer requires having a sense of meaning. It should be able to reason that king-queen relationship is same as man-woman since we remove abstract concept of masculinity from king and man and add another abstract concept of femininity to arrive at queen and woman respectively. If we trick our computer by asking a different question like &quot;king is to man as queen is to what?&quot; then it should be able to do some computation which resembles subtracting abstract concept of royalty from king and queen to arrive at man and woman respectively. And there&#39;s no way a computer can capture such abstract concepts if we just encode each word by it&#39;s position in the dictionary. Thus, with a little bit of thought we come to conclusion that representing words with vectors makes more sense. . And so, we arrive at our next problem. How do we find good vector representations of words? Let&#39;s answer this question by trying out different possibilities. . Let |V| be the length of our vocabulary/dictionary. We can represent each word by a |V|- dimensional vector which has 1 at index which is equal to position of word in dictionary and 0 everywhere else. Therefore, representation for word &#39;aardvark&#39; is ${ left[ begin{array}{ccc}1 0 0 . 0 end{array} right]}$ and &#39;Zebra&#39; will be represented by ${ left[ begin{array}{ccc}0 0 0 . 1 end{array} right]}$. We&#39;ll call this encoding the &#39;one-hot&#39; representation of our words. . Now, digressing a little bit, let us ponder on the question that given two vectors how do we know if those two vectors point in somewhat same direction? We can quantify this notion by calculating the cosine of angle between those two vectors. Let ${a}$ and ${b}$ be two vectors and ${ theta}$ be the acute angle between them. Then, ${ cos theta = large frac{a.b}{ Vert a Vert Vert b Vert}}$. If ${ cos theta = 1}$ then vectors point in the same direction. If it&#39;s $0$ then the vectors are perpendicular to each other and if it&#39;s $-1$ then the vectors point in opposite directions. . Note: We&#8217;ll restrict the word vectors to be unit vectors. Then, calculating cosine similarity is same as taking dot-product. . How does this relate to our problem in hand? Well, we can say that we want that the notion of similarity between two words to be captured by an operation as simple as dot-product of the vectors of these two words. In other words, we want vectors of similar meaning to point in somewhat same direction. This way we can design a function called is_similar which takes in vectors for two words and will compute their dot product. If the dot product is close to 1 then the function will say yes these two words are similar. So, we hope that we can design word vectors in such a way that vectors of synonymous words point in somewhat same direction (and antonyms in the opposite). . Now, knowing the English language we know that words &#39;motel&#39; and &#39;hotel&#39; are very similar and so we want the vectors of these two words to give a dot-product close to 1, according to concept we just discussed. But, if we take the dot product of vectors from our &#39;one-hot&#39; representation of these words, it comes out as 0. In-fact dot-product of any two word vectors from our one-hot representation is 0. This fact is disheartening since this means that one-hot representation are not capable of capturing similarity between two words. Each word-vector is orthogonal to every other word-vector in this representation which is same as saying no two words are similar to each other nor are they oppoisite which isn&#39;t the case in real world. So, we part our ways with the one-hot representation and search for other alternatives. . Count-based Methods: . We are looking for some concept about words which we can latch onto to form good vector representations of words. Through a fortunate stroke of serendipity we come across a quote by English linguist John Rupert Firth: &quot;A word is characterzied by the company it keeps&quot;. This quote implies that a word&#39;s meaning is decided by the context in which it appears. We can validate this statement by looking at some examples. Notice the use of word &quot;bank&quot; in following two sentences. . I deposited the cheque in the bank. | I was sitting alongside a river bank. | In the two sentences above we can see that word &quot;bank&quot; takes on two completely different meaning based on the context in which it appears. In first sentence the word &quot;bank&quot; means a building where financial operations are carried out while in the other sentence it means the ground alongside a river. Therefore, meaning of word bank changed depending on the words it was surrounded by (context words). Moreover, consider the following examples: . The service at hotel we stayed in was very good. | The motel we are going to stay at is famous for the excellent service it provides. | Since, motel and hotel are pretty much synonymous, they tend to be surrounded by similar words like &#39;stay&#39;, &#39;service&#39; etc. Let&#39;s refer surrounding words of a word as &quot;context words&quot;. We can say that two words are similar if they have very similar context words. . To make use of this concept let&#39;s just count co-occurences of words. We first collect a large corpus of text. We then design a two dimensional matrix ${M}$ (let&#39;s call it count matrix) where ${M_{ij}}$ = Number of times the $j^{th}$ word of dictionary occurred in the context of $i^{th}$ word of dictionary in the corpus of text. Each row of this matrix will represent a word in vocabulary. The first row will represent the word &#39;aardvark&#39; and first cell of this row will be the number of times word &#39;aardvark&#39; occurred in context of word &#39;aardvark&#39;. Similarly, the last cell of this row will represent the number of times word &#39;zebra&#39; occurred in context of word &#39;aardvark&#39;. Let $|V|$ be the number of words in our vocabulary. Thus each row contains ${|V|}$ cells and there are ${|V|}$ such rows. Therefore, ${M}$ is a ${|V| times|V|}$ matrix. We need to specify one more detail to design this matrix; what does it mean by occurring in context of something? To formally specify this we design a hyperparameter called &quot;context window&quot; and denote it by letter c. c is the number of words either side of a word that classify as it&#39;s context words for.eg. in the sentence &quot;I hope this year is better than the previous one.&quot; if we choose c = 2 and our center word is &#39;year&#39; then words &#39;hope&#39;, &#39;this&#39;, &#39;is&#39; and &#39;better&#39; are it&#39;s context words. . To efficiently fill out the cells in the matrix we first make a ${|V| times|V|}$ matrix filled with 0&#39;s. Then, we take the first word in the our large corpus and collect it&#39;s context words. Then we increment the count in respective cells. We do this step for each word in the corpus. . We can see intutively that synonymous words will have very similar rows because in a large corpus of text they will have similar context words. We can take the row for any word, normalise it (to turn it into a unit vector) and declare that row as the vector representation of that word. Then, taking the dot-product of vectors of similar words will result in a number close to 1. . This marks our first breakthrough!!! We have devised a representation of words which our computer can understand and then tell us which words are similar by doing an operation as simple as a dot-product. But it is only when we put this algorithm to practice in real-world do we notice that it has numerous shortcomings: . Every word is represented by a ${|V|}$ dimensional vector. ${|V|}$ can be of the order of millions because it is the length of our vocabulary. Storing vector for every word takes a toll on memory of our computer. | ${M}$ is a huge ${|V| times |V|}$ matrix which is sparse. This means that most of the entries of our matrix are 0. This is because most of the words don&#39;t occur in context of some particular words for.eg. words &#39;summer&#39; and &#39;snow&#39; probably never occur in context of each other. | Some of the words occur numerous times in context of other words which isn&#39;t very informative for.eg words like &#39;the&#39; and &#39;is&#39; occur in contexts of almost every word many times. This leads to drastic imbalance in word-frequency. | New words are constantly being added to vocabulary. Words like &#39;tweet&#39; and &#39;google&#39; may not be in vocabulary of ancient texts but they are so prevalent now that they need their own definitions in a standard dictionary. | Let&#39;s try to tackle these shortcomings ony by one. . We have the concept of Singular Value Decomposition (SVD) to our rescue for tackling the first shortcoming. Let&#39;s understand what SVD does by taking a simple example. We can write a large number like 68 as $2 times2 times17$. Writing down $68$ in factorized form reveals a lot of facts like it is divisible by $2,4$ and 17, it is multiple of a prime number etc. Similarly, factoring a matrix can reveal a lot of facts about it. SVD says that any matrix A can be factorized into three simpler matrices ${A, Sigma}$ and ${B}$ as ${A Sigma B^{T}}$ where $A$ and $B$ are orthogonal matrices and ${ Sigma}$ is a diagonal matrix. . Let&#39;s do this with our count matrix ${M}$. We can write $M$ as ${A Sigma B^{T}}$. Here, each of ${A, Sigma}$ and ${B}$ is a ${|V| times |V|}$ matrix. Then, we take only the first $k$ columns of ${A}$ which results in ${|V| times k}$ matrix. We declare the rows of this matrix as vector representations of our words. This now solves our first problem because each word is now a ${k}$ dimensional vector where ${k}$ can be chosen by us based on some threshold. . But the method of SVD which we used to circumvent one of our problems has some shortcomings in itself. SVD is not a trivial operation to perform. It does not scale well for large matrices because the amount of time it takes to perform SVD increases in quadratic fashion with respect to size of matrix. The matrix on which we want to perform SVD is already a huge matrix which will only get larger as new words are added in the vocabulary. These shortcomings are severe enough to compel us to look for other ways to find good vector representation of our words. . Iteration-based methods: . Although our previous method didn&#39;t work out in the end, we found that guiding principle of &quot;A word is characterized by the company it keeps&quot; is pretty good. We would want to continue on this train of thought to devise other methods of finding word vectors. How else does the company of word characterize it? We can notice one fact that context words are pretty good predictors of a center word. That&#39;s what we have been doing since childhood when we are solving &#39;Fill in the blanks&quot; type questions in our exams. Try to complete the following sentence: &quot;A ___ was seen flying in the sky after taking off from airport.&quot; If you just read till the word &quot;sky&quot; there would be many possibilities in your mind for the word that fills in the blank for eg &#39;bird&#39;, &#39;airplane&#39;, &#39;birds&#39;, &#39;helicopter&#39; etc. If you read the whole sentence and see the word &quot;airport&quot; you immediately lock in &quot;airplane&quot; as the most likely answer. We can also see it other way round. Given a word, say &quot;football&quot; you would assume that words like &quot;kick&quot;, &quot;score&quot;, &quot;goal&quot; will be in it&#39;s vicinity in a large corpus, rather than words like &quot;racket&quot;, &quot;serve&quot; etc. . This shows that: . 1. A word is a good predictor of it&#39;s context words. . 2. Context words are good predictor of a center word. . These two concepts give us the foundation for building good vectors for our words. . Skip-gram model: . Let&#39;s go with the first concept above and see what we can conjure up. We can start by taking the first word in a large corpus and then try to predict it&#39;s context words using it&#39;s word vector. If predictions are correct then everything is fine, but if they are not then there must be something wrong with the word vectors. In that case, try to modify the vectors so that next time they don&#39;t commit the same mistake again. Do this for every word in the corpus and hopefully we have corrected all the bad word vectors. But, to correct bad word vectors we need to have them in the first place. Therefore, initially we&#39;ll initialize d-dimensional word vectors for every word randomly and we&#39;ll improve them over time using the algorithm we just envisaged. . I&#39;ve highlighted the words predict and modify because it is not clear how would we carry out these operations. . Let&#39;s tackle the problem of prediction first. How do we predict the context words from a given word? For that we can change our notion of dot-product. Earlier we considered dot-product of vectors of two words to be a measure of similarity of those two words. Now, let&#39;s see the dot product of two word vectors as a measure of probability of them existing in the vicinity of each other. In other words, if dot-product of word vectors of two words, say &#39;a&#39; and &#39;b&#39; is close to 1 then that means they are very likely to occur nearby in the corpus, which means when we see &#39;a&#39; we can safely predict that &#39;b&#39; will be nearby. . Note: - I am using terms &quot;dot-product&quot; to mean cosine similarity between the vectors. If vectors are of unit length cosine similarity is the same as dot-product . Let&#39;s see how we can convert dot-product of word vectors to probabilities with an example. Take word vector of &quot;deep&quot; and take it&#39;s dot product with vector of every word in the vocabulary and collect the results in a list. This will give us a list of length ${|V|}$. We can see this list as a &quot;score&quot; for probability of each word in vocabulary occurring with word &quot;deep&quot;. We hope that score for word &quot;learning&quot; is close to 1 (because if we see the word &quot;deep&quot; there&#39;s high chances that word &quot;learning&quot; is nearby in a corpus). Wouldn&#39;t it be great if we could turn this list of scores in a list of probabilities depending on the score. This list of probabilities would then tell the probability of each word in vocabulary occurring in context of word &quot;deep&quot;. Since all the scores are between -1 and 1 let&#39;s exponentiate them to turn them into positive numbers (because ${e^{x}}$ is always positive and if ${x lt y implies e^{x} lt e^{y}}$ which will make sure larger scores turn into larger positive numbers). Now the list is a collection of positive numbers with no upper bound. To squeeze these numbers between 0 and 1 let&#39;s divide each entry in the list by the sum of all the entries in the list. This way all the numbers sum to 1 which makes them a valid probability distribution. The sequence of operations we did to convert scores to probabilities is called Softmax and takes a single line of code to implement. . Let&#39;s write down what we did in mathematical notations. . Let ${u}$ denote the word vector of word &quot;deep&quot; and ${v_{1}, v_{2}, ..., v_{|V|}}$ be the word vectors of every word in vocabulary. . Take the dot product of each ${v_{i}}$ with ${u}$ and collect the results in a list called ${scores_{deep}}$. . ${scores_{deep} = left[u.v_{1}, space u.v_{2}, ..., space u.v_{|V|} right]}$. . | Exponentiate each of the entry in ${scores_{deep}}$ . ${scores_{deep}} = { left[exp(u.v_{1}), space exp(u.v_{2}), ..., space exp(u.v_{|V|}) right]}.$ . Store sum of all the entries of ${scores_{deep}}$in a variable &#39;Sum&#39; . Sum = ${exp(u.v_{1}) , + , exp(u.v_{2}) , + , ... ,+ ,exp(u.v_{|V|}) = large Sigma_{i=1}^{|V|} exp(u.v_{i})}$ . | Divide each entry by Sum and rename the list to $probabilities_{deep}$ because the entries of list now convey the probabilities of words occurring alongside &quot;deep&quot;. . ${probabilities_{deep}} = { left[ large frac{exp(u.v_{1})}{Sum}, space frac{exp(u.v_{2})}{Sum}, ..., space frac{exp(u.v_{|V|})}{Sum} right]}.$ . | If we want the probability of word &quot;aardvark&quot; occurring in context of &quot;deep&quot; we&#39;ll just index the first element from list $probabilities_{deep}$ and voila!! we&#39;ve got our answer which is $ large frac{exp(u.v_{1})}{ Sigma_{i=1}^{|V|} exp(u.v_{i})}$. This quantity is basically $P( aardvark | deep)$ i.e. probability of seeing word &quot;aardvark&quot; given that we have just seen word &quot;deep&quot;. The lists like $probabilities_{deep}$ are called a &quot;probability distribution&quot; over all the words in vocabulary. How do we predict then? Well, we can just sample a word from dictionary based on probability distribution we just got. . Now that we&#39;ve worked out a way to convert dot-product to probability let&#39;s see how we can design good word vectors for our words. . Let &quot; A blog about word vectors&quot; be a string of words in our big corpus. Since words &quot;A&quot;, &quot;about&quot;, &quot;word&quot; and &quot;vectors&quot; are in context of word &quot;blog&quot;, we would want $P(a, about, word, vectors | blog)$ to be close to 1. . Let’s make a “naive” assumption that context words are independent of each other given the center word. . Then, according to chain rule of probability: . $P(a, about, word, vectors | blog) = P(a | blog) times P(about | blog) times P(word| blog) times P(vectors| blog)$. Let’s denote this quantity by $L$. Our objective will be to bring $L$ closer to $1$. . $L = P(a | blog) times P(about | blog) times P(word| blog) times P(vectors| blog)$ . We have already devised a way to calculate each of the four factors above. . $L = Large frac{exp(u_{a}.v_{blog})}{ Sigma_{i=1}^{|V|} exp(u_{i}.v_{blog})} times Large frac{exp(u_{about}.v_{blog})}{ Sigma_{i=1}^{|V|} exp(u_{i}.v_{blog})} times Large frac{exp(u_{word}.v_{blog})}{ Sigma_{i=1}^{|V|} exp(u_{i}.v_{blog})} times Large frac{exp(u_{vectors}.v_{blog})}{ Sigma_{i=1}^{|V|} exp(u_{i}.v_{blog})}$ . When calculating $L$ on our computer, we encounter one problem; $L$ is product of numbers which are between 0 and 1. Multiplying many such numbers together leads to underflow errors. To circumvent this, we instead try to maximize $log L$. Taking log would turn products of numbers between $0$ and $1$ into sum of large negative numbers which our computer would be able to handle. . Writing our new objective down: . $logL = log frac{exp(u_{a}.v_{blog})}{ Sigma_{i=1}^{|V|} exp(u_{i}.v_{blog})} + log frac{exp(u_{about}.v_{blog})}{ Sigma_{i=1}^{|V|} exp(u_{i}.v_{blog})} + log frac{exp(u_{word}.v_{blog})}{ Sigma_{i=1}^{|V|} exp(u_{i}.v_{blog})} + log frac{exp(u_{vectors}.v_{blog})}{ Sigma_{i=1}^{|V|} exp(u_{i}.v_{blog})}$ . Now that, we&#39;ve worked out a method to calculate our objective using an example, let&#39;s try to write it in more general terms. . Since we are going through an entire corpus word by word, predicting context for every word, we&#39;ll likely encounter every word in two scenarios. One in which that word will be the center word and other in which it&#39;ll be the context word for some other word. Let&#39;s use ${v_{w}}$ to denote the embedding for a word ${w}$ when it acts as a centre word and ${u_{w}}$ to denote it&#39;s embedding when it acts as context word. . Let the size of context window be c which means for each word we consider c words to it&#39;s left and c words to it&#39;s right as it&#39;s context. Let $w_{i}$ and ${v_{w_i}}$ be the center word and it&#39;s word vector respectively. Let ${u_{w_{i}}}$ be the vector for word ${w_{i}}$ and ${u_{k}}$ be the vector for word in $k^{th}$ position in dictionary. . Then our objective for this particular word is : . $logL_{ large w_{i}} = sum_{j=-c}^{c} logP(w_{i+j}| w_{i}) = sum_{j=-c}^{c} log Large frac{exp(u_{w_i+j}.v_{w_i})}{ sum_{k=1}^{|V|}exp(u_{k}.v_{w_i})}$ . Simplifying further, . $logL_{w_{i}}= normalsize sum_{j=-c}^{c}u_{w_i+j}.v_{w_i} - 2c sum_{k=1}^{|V|}exp(u_{k}.v_{w_i})$ . Instead of maximizing $logL_{w_{i}}$ let&#39;s minimize $-logL_{w_{i}}$. Both are essentially the same operation but we choose to go the later way because now we can refer $-logL_{w_{i}}$ as $Loss$ which is appealing intuitively (we can say that we are minimising the $Loss$) . So, $Loss =- normalsize sum_{j=-c}^{c}u_{w_i+j}.v_{w_i} + 2c ,log sum_{k=1}^{|V|}exp(u_{k}.v_{w_i})$ . The problem we face now is how do we minimize it? Well, we can see that Loss depends on vectors $u_{1},...,u_{|V|}$ and $v_{i}$. We could try to tinker with these to minimize the $Loss$. The way we do that is by using an algorithm called Gradient Descent. . Suppose we want to modify ${v_{i}}$ to decrease $Loss$. According to gradient descent we should first take the gradient of $Loss$ with respect to $v_{i}$ which we can denote by $ large frac{ delta Loss}{ delta v_{i}}$. Then we should modify $v_{i}$ using the following rule : . $v_{i} leftarrow v_{i} - alpha frac{ delta Loss}{ delta v_{i}}$ where $ alpha$ is called the &#39;learning rate&#39;. . We can modify $u_{1},...,u_{|V|}$ similarly. . Summary: . For every word in large corpus of words: . Collect context for word. | Calculate $Loss$ using the word vectors of context words and word. | Modify those word vectors using the gradient descent. | Hopefully, at the end of this procedure, we will have satisfactory word vectors. . Continuous bag of words: . Now, let&#39;s try to go the other way around and predict a word using it&#39;s context words. Let be a string of words from our corpus. $w_{i-c}, w_{i-c+1},..., w_{i-1},w_{i}, w_{i+1},...,w_{i+c}$ be a string of words from our corpus. This time the thing we are trying to maximize is $P( w_{i} | w_{i-c}, w_{i-c+1},..., w_{i-1},w_{i+1},...,w_{i+c})$ where $w_{i}$ is any word in the corpus and $w_{i-c}, w_{i-c+1},..., w_{i-1},w_{i+1},...,w_{i+c}$ are $2c$ context words from it&#39;s left and right context. . How do we express $P( w_{i} | w_{i-c}, w_{i-c+1},..., w_{i-1},w_{i+1},...,w_{i+c})$ in terms of word vectors of these words ? Well, we can design a single vector for all the context word vectors by averaging them together. Let that single vector be denoted by $u_{c}$. . $u_{c} = Large frac{u_{w_{i-c}} ,+ ,... ,+ {u_{w_{i-1}} ,+ ,{u_{w_{i+1}} ,+... ,+{u_{w_{i+c}}}}}}{2c}$. Let $v_{w_{i}}$ be the word vector for word $w_{i}$ and let ${u_{k}}$ be the vector for $k^{th}$ word in vocabulary. Then, we can calculate $P( w_{i} | w_{i-c}, w_{i-c+1},..., w_{i-1},w_{i+1},...,w_{i+c})$ . as follows: . $P( w_{i} | w_{i-c}, w_{i-c+1},..., w_{i-1},w_{i+1},...,w_{i+c}) = frac{ large exp(v_{w_{i}}.u_{c})}{ large sum_{k=1}^{|V|}exp(u_{k}.u_{c})}$. Same as before, we try minimize negative log of this quantity which we have aptly named $Loss$. . $Loss = -log frac{ large exp(v_{w_{i}}.u_{c})}{ large sum_{k=1}^{|V|}exp(u_{k}.u_{c})} = -v_{w_{i}}.u_{c} + log sum_{k=1}^{|V|}exp(u_{k}.u_{c})$ . We now modify word vectors by Gradient descent as described above. . Summary: . For every word in large corpus of words: . Treat that word as a blank | Collect context words and take average of their word vectors. | Calculate $Loss$. | Modify those word vectors using the gradient descent. | Hopefully, at the end of this procedure, we will have satisfactory word vectors. . Glove: . Having found some very good methods to design vectors for our words we can now sit back and relax. But, being research scientists we are itching to find some shortcomings in our methods and improve upon them. Let&#39;s see if we can find any. If we analyse carefully at one of our loss functions we&#39;ll notice the term $sum_{k=1}^{|V|}exp(u_{k}.v_{w_{i})$ there. This term is computationally expensive to calculate since we are taking dot-product of $v_{w_{i}}$ with vector of every word. We need to get rid of this term. .",
            "url": "https://abhimanyu08.github.io/blog/deep-learning/natural-language-processing/2021/01/04/final.html",
            "relUrl": "/deep-learning/natural-language-processing/2021/01/04/final.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Revelations Of The Gradients And Hessians",
            "content": "Notations . If ${ textbf A}$ is matrix then ${ textbf A_{i,:}}$ denotes the i-th row of ${ textbf A}$ and ${ textbf A_{:,i}}$ denotes the i-th column of ${ textbf A}$. . | The dot product of two vectors ${a}$ and ${b}$ can be written as ${a.b}$ or ${a^{T}b}$ or ${ lVert a rVert lVert b rVert cos theta}$ where ${ theta}$ is the angle between ${a}$ and ${b}$ and ${ lVert . rVert}$ denotes the Frobenius norm of a quantity. . | Pre-Requisites . You can watch the amazing series Essence of Linear Algebra by 3 Blue 1 Brown instead of reading this section to gain better understanding of the linear algebra needed for this blog. In fact, most of the material written in this section is inspired from that series . 1. Eigenvectors And Eigenvalues: . Multiplying a matrix by a vector rotates and shrinks/stretches that vector, for eg. ${ left[ begin{array}{ccc} -1 &amp; 0 0 &amp; -1 end{array} right] left[ begin{array}{ccc} 2 3 end{array} right] = left[ begin{array}{ccc} 2 4 end{array} right]}$. Eigenvectors of a matrix ${ textbf A}$ are those vectors that are only stretched/shrinked when multipled by ${ textbf A}$. The amount by which the matrix strech/shrink a eigenvector is called eigenvalue for that particular vector. for eg ${ left[ begin{array}{ccc} -1 &amp; 0 0 &amp; -1 end{array} right] left[ begin{array}{ccc} 2 0 end{array} right] = left[ begin{array}{ccc} -2 0 end{array} right] = -1 left[ begin{array}{ccc} 2 0 end{array} right]}$. Thus ${ left[ begin{array}{ccc} 2 0 end{array} right]}$ is eigenvector of matrix ${ left[ begin{array}{ccc} -1 &amp; 0 0 &amp; -1 end{array} right]}$ with eigenvalue -1. Notice that ${ left[ begin{array}{ccc} 3 0 end{array} right]}$ is also a eigenvector with same eigenvalue. More generally if ${ textbf v}$ is a eigenvetor of a matrix then so is ${k textbf v}$ (where k is a scalar) with same eigenvalue. Since multiples of eigenvectors are also eigenvectors of the matrix we only consider the eigenvector with unit magnitude. A matrix can have multiple eigenvalues. A matrix with only positive/negative eigenvalues is called positive/negative definite matrix. A matrix with non-negative/non-positive eigenvalues is called positive/negative semidefinite matrix. . 2. Inverse Of A Matrix: . The inverse of matrix ${ textbf A}$ is a matrix which when multiplied by ${ textbf A}$ produces ${ textbf I}$ where ${ textbf I}$ is the identity matrix. The inverse of matrix ${ textbf A}$ is denoted by ${ textbf A^{-1}}$. . 3. Transpose Of A Matrix: . The transpose of matrix ${ textbf A}$ is done by replacing ${ textbf A_{i,j}}$ with ${ textbf A_{j,i}}$ and is denoted by ${ textbf A^{T}}$. for eg ${ left[ begin{array}{ccc} 1 &amp; 2 3 &amp; 4 end{array} right]^{T} = left[ begin{array}{ccc} 1 &amp; 3 2 &amp; 4 end{array} right]}$. The transpose of a row vector is a column vector and vice versa, for eg. ${[1 , ,2]^{T} = left[ begin{array}{ccc}1 2 end{array} right]}$ . 4. Orthogonal Matrix: . A matrix ${ textbf A}$ is orthogonal if ${ textbf A^{T} = textbf A^{-1}}$. The rows and columns of an orthogonal matrix are unit vectors and all the rows and columns of this matrix are orthonormal to each other. Two vectors are called orthonormal when their dot product is 0 (which further means they are perpendicular to each other). Multiplying a orthogonal matrix by a vector only rotates that vector by some angle without changing it&#39;s magnitude. Note - If ${ textbf A}$ is orthogonal then so is ${ textbf A^{T}}$. . When the i-th column of an orthogonal matrix is multiplied by the matrix, the result is a column vector with 1 in i-th position and 0 everywhere else. When the i-th row of an orthogonal matrix is multiplied by the matrix, the result is a row vector with 1 in i-th position and 0 everywhere else. . 5. Diagonal Matrix: . A matrix is a diagonal matrix if it has 0 at every position except it&#39;s diagonal for eg. ${ textbf A = left[ begin{array}{ccc} 1 &amp; 0 0 &amp; 2 end{array} right]}$. Diagonal matrix can be simply denoted by ${diag( lambda_{1}, lambda_{2},..., lambda_{n})}$ where ${( lambda_{1}, lambda_{2},..., lambda_{n})}$ are the diagonal elements of the matrix. for eg ${ textbf A = diag(1,2)}$ . 5. Symmetric Matrix : . A matrix ${ textbf A}$ is called symmetric if ${ textbf A = textbf A^{T}}$, which means ${ textbf A_{i,j} = textbf A_{j,i}}$. Symmetric matrices have a intriguing property that they can be expressed as product of three simple matrices. If ${ textbf A}$ is symmetric then it can be shown that ${ textbf A = textbf Q Lambda textbf Q^{T}}$. ${ textbf Q Lambda textbf Q^{T}}$ is called the eigendecomposition of ${A}$. The columns of ${ textbf Q}$ are the unit eigenvectors of ${ textbf A}$ and ${ Lambda}$ is a diagonal matrix whose diagonal elements are eigenvalues of ${ textbf A}$ i.e ${ Lambda = diag( lambda_{1}, lambda_{2},..., lambda_{n})}$ where ${ lambda_{1}, lambda_{2},..., lambda_{n}}$ are eigenvalues of ${ textbf A}$. The eigenvalue ${ lambda_{i}}$ is associated with column ${i}$ of ${ textbf Q}$. Another important thing to note is that ${ textbf Q}$ here is an orthogonal matrix. . 6. Basis and Coordinates : . Suppose you are given a vector say ${ left[ begin{array}{ccc}2 4 end{array} right]}$. This vector is address of a particular point say ${p}$ in 2-D space. Suppose you are at origin and you want to go to ${p}$. This vector says that to reach that point travel 2 units in direction ${ left[ begin{array}{ccc}1 0 end{array} right]}$ and then 4 units in direction ${ left[ begin{array}{ccc}0 1 end{array} right]}$. Therfore, ${ left[ begin{array}{ccc}2 4 end{array} right]}$ can be written as ${2 left[ begin{array}{ccc}1 0 end{array} right] + 4 left[ begin{array}{ccc}0 1 end{array} right]}$. Notice that we are giving the address of the point in space in terms of how much you should travel in direction ${ left[ begin{array}{ccc}1 0 end{array} right]}$ and ${ left[ begin{array}{ccc}0 1 end{array} right]}$. But what if we want to give the address in terms of different directions say ${ left[ begin{array}{ccc}1 1 end{array} right]}$ and ${ left[ begin{array}{ccc}1 -1 end{array} right]}$. In this case we would say travel 3 units in direction ${ left[ begin{array}{ccc}1 1 end{array} right]}$ and -1 units in direction ${ left[ begin{array}{ccc}1 -1 end{array} right]}$. . So the new address of ${p}$ would be ${ left[ begin{array}{ccc}3 -1 end{array} right]}$ in terms of these new directions. These directions are called basis vectors. If basis vectors are n-dimensional then every point in n-dimensioanl space can be written as linear combination of basis vectors. for eg point ${p}$ can be written as ${3 left[ begin{array}{ccc}1 1 end{array} right] + -1 left[ begin{array}{ccc}1 1 end{array} right]}$. The coordinates of ${p}$ with respect to these bases would be ${ left[ begin{array}{ccc}3 -1 end{array} right]}$. Not all collection of vectors qualify as basis vectors of space. Notice that the coordinate of p corresponding to ${ left[ begin{array}{ccc}1 1 end{array} right]}$ (3) is greater than coordinate corresponding to ${ left[ begin{array}{ccc}1 -1 end{array} right]}$ (-1). This is because if we draw a vector from origin towards ${p}$, that vector is more aligned towards ${ left[ begin{array}{ccc}1 1 end{array} right]}$ than the other basis vector. . The n eigenvectors of a ${n times n}$ symmetric matrix qualify as basis vectors for that n-dimensional space. That means that every point in that n-dimensional space can be written as a linear combination of these n eigenvectors. . 1. Directional Derivative . Let&#39;s say we have a multivariable function with two variables ${f(x_{1},x_{2}) = x_{1}^{2} + x_{2}^{2}}$ where ${x_{1}}$ and ${x_{2}}$ represent the standard axes in a 2 dimensional space. . We can say that f takes in a two-dimensional vector as input where the entries of vector are values of ${x_{1}}$ and ${x_{2}}$ respectively. We want to know how the function changes if we are at point ${(1,1)}$ and move in the direction of ${x_{1}}$ by a little amount. We can answer this question by calculating the partial derivative of ${f}$ with respect to ${x_{1}}$ which is ${2x_{1}}$. Putting ${x_{1} = 1}$, we conclude that if we move a miniscule amount in ${x_{1}}$ direction the function ${f}$ will change by twice that amount. The same goes for the case in which we want to take a tiny step in direction of ${x_{2}}$. But what if we want to move in the direction ${(1,2)}$ ? For calcualting the effect of this movement on ${f}$ we need to calculate the derivative of ${f}$ in direction ${[ frac {1}{ sqrt{5}}, frac{2}{ sqrt{5}}]^{T}}$ (which is a unit vector in direction ${(1,2)}$). . More generally, to find out how does nudging the input of a multivariable function in a certain direction affects the value of that function, we need to calculate the derivative of that function in that direction. This derivative is called the directional derivative of that function for that particular direction. Fortunately, it&#39;s trivially easy to calculate. The directional derivative of a function g in direction ${u}$ is just ${u^{T} nabla scriptsize x normalsize g(x)}$ (matrix mutltiplication) or ${u . nabla scriptsize x normalsize g(x)}$ (dot product) where ${ nabla scriptsize x normalsize g(x)}$ is called gradient of function g. Gradient of g is a vector containing all the partial derivatives of ${g}$ with respect to vector ${ textbf{x}}$. Element ${i }$ of ${ nabla scriptsize x normalsize g(x)}$ is partial derivative of ${g}$ w.r.t ${x_{i}}$. Thus, partial derivative of our function f in the direction ${ left[ begin{array}{ccc} frac {1}{ sqrt{5}} frac{2}{ sqrt{5}} end{array} right]}$ is just ${ left[ begin{array}{ccc} frac {1}{ sqrt{5}} frac{2}{ sqrt{5}} end{array} right]. left[ begin{array}{ccc}2x_{1} 2x_{2} end{array} right] = frac{2x_{1}}{ sqrt{5}}+ frac{4x_{2}}{ sqrt{5}}}$. . More formally, directional derivative of a function ${f}$ in direction ${u}$ is derivative of ${f(x + alpha u)}$ with respect to ${ alpha}$, calculated at ${ alpha = 0}$. Let ${(x + alpha u) = p}$. ${ large frac{ partial p}{ partial alpha} = u . therefore frac{ partial f(x+ alpha u)}{ partial alpha} = frac{ partial f(p)}{ partial alpha}}$. . Then, using the chain rule of calculus ${ large frac{ partial f(p)}{ partial alpha} = frac{ partial f(p)}{ partial p} frac{ partial p}{ partial alpha} = small nabla_{p} f(p).u = u^{T} nabla_{x+ alpha u}f(x + alpha u)}$ - ${ textbf Eq(1)}$. Putting ${ alpha = 0}$, we get directional derivative in direction ${u}$ = ${u^{T} nabla_{x}f(x)}$ . Why do we take a step in opposite direction of the gradient ? . Earlier we saw that the directional derivative of a function ${f}$ in the direction ${u}$ is ${u^{T} nabla_xf(x)}$ or ${u. nabla_xf(x)}$. To emphasize again, directional derivative in a certain direction tells us how much the function changes relative to change in input in that direction. This can give us the answer to where should we move while minimsing a function. During learning, our aim is to minimise an objective function. Suppose we are at a certain point in the input space of function ${f}$ and we want to decide where should we move so that that the function decreases the fastest. In other words we want to move in direction ${u}$ such that ${u. nabla_xf(x)}$ is minimum. By writing ${u. nabla_xf(x)}$ as ${ lVert u rVert lVert nabla_{x}f(x) rVert cos theta}$, we can see that this quantity is minimum when ${cos theta = -1}$ or ${ theta = 180^{ circ}}$. This means ${u}$ should be in opposite direction of the gradient. This is the reason we take a step in the direction of negative gradient during learning/training. Therefore, the gradient descent algorithm proposes update the weights according to rule ${ theta leftarrow theta- epsilon nabla_{ theta}f( theta)}$ where ${ epsilon}$ is called the learning rate. . 2. Second derivative of a function and how does it affect the gradient step . The second derivative of a fnction with respect to it&#39;s input tells us about the curvature of that fnction. Let&#39;s take three fnctions ${p(x) = 2x, g(x) = x^{3}, h(x) = -x^{2}}$. . Let&#39;s say we are at the point ${x = 1}$. The second derivatives of the functions at this point are ${p^{&#39;&#39;}(1) = 0, g^{&#39;&#39;}(1) = 6, h^{&#39;&#39;}(1) = -2}$. The value of ${p^{&#39;&#39;}(x)}$ tells us that it&#39;s gradient doesn&#39;t change at all. We can verify this by seeing that gradient of p(x) is 2 everywhere. The second derivative of g(x) increases as x increases. This shows that gradient of g increases as we move rightwards from any point. The positive value of second derivative of g(x) for positive x shows that function curves upwards when x is positive. The negative value of second derivative of h(x) tells us that gradient of h(x) decreases as x increases. Therefore, the function curves downwards. We are at point x = 1 and we want to minimise the functions p,g and h. Gradient descent tells us to move in the direction of negative gradient. The gradient of the functions at ${x = 1}$ are ${p^{&#39;}(1) = 2, g^{&#39;}(1) = 3, h^{&#39;}(1) = -2}$. The value of 2 for ${p^{&#39;}(1)}$ tells us that if we change x by a small amount, the value of p(x) should change by twice that amount. The same goes for g(x) and h(x). Let&#39;s see if that&#39;s the case. . Let learning rate ${ epsilon = 0.001}$. . In case of p(x), we move to new point ${x = x - 2 epsilon = 1 - 0.001 times2 = 0.998}$. New value of p(x) is ${2 times0.998 = 1.996}$. Thus, we have successfully decreased the value of p(x) and we can see that it decreased by 0.004 which is exactly twice the amount of change in x. This is what we expected from the gradient. . For g(x), ${x = x- 3 epsilon = 1 - 3 times0.001 = 0.997}$. The new value of g(x) is 0.991026973. x changes by 0.003 so according to gradient information g(x) should change by thrice that amount i.e 0.009. But, we can see that g(x) changes by 0.008973027 which is little less than the value 0.009 predicted by the gradient. . For h(x), ${x = x- (-2) epsilon = 1 + 2 times0.001 = 1.002}$. The new value of h(x) is -1.004004. x changes by 0.002 so according to gradient information h(x) should change by 0.004. We can see that h(x) changes by 0.004004 which is little more than the value 0.004 predicted by the gradient. . Generally, we can see the effect of second derivative by looking at the taylor series approximation of a function at a point ${x^{ omicron}}$. The function f at a point ${x^{ omicron}}$ can be approximated upto three terms as follows : . ${f(x) approx f(x^{ omicron}) + large frac{(x-x^{ omicron})}{1!} small f^{&#39;}(x^{ omicron}) + large frac{(x-x^{ omicron})^{2}}{2!} small f^{&#39;&#39;}(x^ omicron)}$ - ${ textbf Eq(2)}$. . The gradient descent tells us that in order to reduce ${f(x)}$ we should go from ${x^ omicron}$ to ${x^ omicron - epsilon f^{&#39;}(x^ omicron)}$. Putting ${x = x^ omicron - epsilon f^{&#39;}(x^ omicron)}$ in Eq(2): . ${f(x^ omicron - epsilon f^{&#39;}(x^ omicron)) = f(x^ omicron) - epsilon (f^{&#39;}(x^ omicron))^{2} + 0.5 ( epsilon f^{&#39;}(x^ omicron))^{2}f^{&#39;&#39;}(x^ omicron)}$. . From above equation we can see that the second term in above equation (${ epsilon (f^{&#39;}(x^ omicron))^{2}}$) is the reduction in value of ${f(x^ omicron)}$ after taking a gradient descent step and third term (${0.5 ( epsilon f^{&#39;}(x^ omicron))^{2}f^{&#39;&#39;}(x^ omicron)}$) is correction we apply to account for the curvature of the function . If ${f^{&#39;&#39;}(x^ omicron) = 0}$ then the third term vanishes as happened in case of function ${p(x)}$ above. In this case gradient information correctly predicts the change in value of function. . But if second derivative of a function is non-zero, there are second order effects too which have to be accounted for. Gradient descent is unaware of these second order effects. If second derivative of a function is too large at some point, a step taken in opposite direction of gradient can even increase the value of function if learning rate isn&#39;t too small. If second derivative of a function is too large at some point we need to keep learning rate too small to avoid the second order effects, but a small learning rate will lead to longer training time. . 3. Hessians . In last section we looked at second derivative of functions which only had a scalar parameter/input. In deep learning, we have to deal with objective functions which take in a vector or matrix as input. These functions don&#39;t have a scalar value as a second derivative and thus it is difficult to judge the second order effects in these scenarios. To analyze these functions, let&#39;s take a function ${f: R^{m} rightarrow R}$ which takes in a m-dimensional vector as input. Let&#39;s denote this m-dimensional vector as ${ textbf x = (x_{1}, x_{2},...,x_{m})}$. The gradient of this function is denoted by m-dimensional vector ${ textbf g}$. The i-th entry of ${ textbf g}$ is equal to ${ large frac{ partial f}{ partial x_{i}}}$. This function will have multiple second derivatives which can be collected together in a matrix called Hessian matrix denoted by ${ textbf H(f)}$. . ${ textbf H(f)_{i,j} = large frac{ delta^{2}}{ delta x_{i}x_{j}} small f}$. Therefore, ${ textbf H(f)}$ will be a ${m times m}$ matrix. Notice that, ${ textbf H(f)_{i,j} = textbf H(f)_{j,i}}$ wherever the partial derivative is continous. This means ${ textbf H(f)}$ is a symmetric matrix. Therefore, ${ textbf H(f)}$ can be written as ${ textbf Q Lambda textbf Q^{T}}$ where columns of ${ textbf Q}$ are eigenvectors of ${ textbf H(f)}$ and ${ Lambda}$ is a diagonal matrix with eigenvalues of ${ textbf H(f)}$ as it&#39;s diagonal elements. If ${( lambda_{1}, lambda_{2},..., lambda_{m})}$ are eigenvalues of ${ textbf H}$, then ${ Lambda = diag( lambda_{1}, lambda_{2},..., lambda_{m})}$ . We want to know the second derivative in direction ${u}$ so that we can infer how does the gradient and our function will behave if we move in that direction. For that we would have to calculate second directional derivative in a particular direction. Second directional derivative in direction ${u}$ can be written as ${ large frac{ partial^{2}}{ partial alpha^{2}} small f(x + alpha u)}$ at ${ alpha = 0}$. Let ${x + alpha u = p, , therefore large frac{ partial p}{ partial alpha} small= u}$. . ${ large frac{ partial^{2}}{ partial alpha^{2}} small f(x + alpha u) = large frac{ partial}{ partial alpha}( frac{ partial}{ partial alpha} small f(p) large) }$ . Using the chain rule, ${ large frac{ partial}{ partial alpha}( frac{ partial}{ partial alpha} small f(p) large)= frac{ partial}{ partial alpha}( frac{ partial f(p)}{ partial p} frac { partial p }{ partial alpha}) = frac{ partial}{ partial alpha} small(u. f^{&#39;}(p)) = u. large frac{ partial}{ partial alpha} small f^{&#39;}(p) = u. large frac{ partial f^{&#39;}(p)}{ partial p} frac { partial p}{ partial alpha} = small u^{2}.f^{&#39;&#39;}(x + alpha u)}$. . Putting ${ alpha = 0}$ we can conclude that second directional derivative of function ${f}$ in direction ${ textbf u}$ is ${ textbf u^{2}f^{&#39;&#39;}( textbf x)}$. Notice that ${ textbf x}$ and ${ textbf u}$ here are vectors. Therefore ${f^{&#39;&#39;}( textbf x)}$ is the hessian matrix ${ textbf H}$. So, the second directional derivative in direction ${ textbf u}$ is written as ${ textbf u^{T}H textbf u}$ . Now that we have derived the formula of second directional derivative in direction ${u}$, let&#39;s calulate it in a certain direction say ${q_{i}}$ where ${q_{i} = textbf Q_{:,i}}$. In other words ${q_{i}}$ is the eigenvector of ${ textbf H}$ associated with eigenvalue ${ lambda_{i}}$. . Directional derivative in direction ${q_{i} = q_{i}^{T}Hq_{i} = q_{i}^{T} textbf Q Lambda textbf Q^{T}q_{i}}$. . ${q_{i}^{T} textbf Q}$ gives a m-dimensional row vector which will have 1 in i-th position and 0 everywhere else. ${ textbf Q^{T}q_{i}}$ gives a m-dimensional column vector which will have 1 in i-th position and 0 everywhere else. . ${ therefore q_{i}^{T}Hq_{i} = [0 ,0 , ... 1 ,... ,0] Lambda[0 ,0 , ... 1 ,... ,0]^{T}}$. This will give back the i-th eigenvalue of ${ textbf H}$ i.e ${ lambda_{i}}$. Thus, value of second directional derivative in the direction of an eigenvector is just the eigenvalue associated with that eigenvector. . What if we want to know the value of second directional derivative in direction ${v}$ which is not one of the eigenvectors of ${ textbf H}$. We know that m eigenvectors of ${ textbf H}$ form a basis for the m-dimensional space. Therefore, ${v}$ can be expressed as linear combination of these m eigenvectors as follows: . ${v = a_{1}q_{1} + a_{2}q_{2} + ... + a_{m}q_{m}}$. This means that ${[a_{1} , a_{2} ,... ,a_{m}]^{T}}$ are the coordinates of point to which ${v}$ points with respect to basis consisting of eigenvectors. Since ${v}$ is a direction ${ lVert v rVert = 1}$. This means ${ sqrt{a_{1}^{2}+a_{2}^{2}+...+a_{m}^{2}} = 1}$. From this we can infer that values of ${a_{1},...,a_{m}}$ lie between -1 and 1. Another point to keep in mind is that ${a_{i} &gt; a_{j}}$ if ${v}$ is more inclined towards ${q_{i}}$ than towards ${q_{j}}$. . Second directional derivative in direction ${v}$ = ${v^{T}Hv = (a_{1}q_{1} +... + a_{m}q_{m})^{T}Q Lambda Q^{T}(a_{1}q_{1} +... + a_{m}q_{m})}$ . Since vector-matrix mulitplication is distributive, . ${v^{T}Hv = (a_{1}q_{1}^{T}Q + ...+ a_{m}q_{m}^{T}Q) , Lambda ,(a_{1}Q^{T}q_{1} +...+a_{m}Q^{T}q_{1})}$ . ${= large( small a_{1}[1 , 0 ,.. ,0] + ... + a_{m}[0 , 0 , ... , 1] large) small , Lambda , large( small a_{1}[1 ,0 ,.. ,0]^{T} + ... + a_{m}[ ,0 , 0 , ... , 1]^{T} large) small = [a_{1} ,a_{2} ,... , a_{m}] Lambda [a_{1} ,a_{2} ,... , a_{m}]^{T}}$ . Solving further, ${[a_{1} ,a_{2} ,... , a_{m}] Lambda [a_{1} ,a_{2} ,... , a_{m}]^{T} = [a_{1} ,a_{2} ,... , a_{m}][a_{1} lambda_{1} , ,a_{2} lambda_{2} , ,... , , a_{m} lambda_{m}]^{T} = a_{1}^{2} lambda_{1} + a_{2}^{2} lambda_{2} + .. + a_{m}^{2} lambda_{m}}$ . ${ therefore v^{T}Hv = a_{1}^{2} lambda_{1} + a_{2}^{2} lambda_{2} + .. + a_{m}^{2} lambda_{m}}$. . This shows that directional second derivative in any direction is weighted average of all the eigenvalues of the Hessian and the value of weights lie between 0 and 1. ${ lambda_{i}}$ receives more weight than ${ lambda_{j}}$ if ${v}$ is more inclined towards ${q_{i}}$ than towards ${q_{j}}$ . Let ${ lambda_{max}}$ and ${ lambda_{min}}$ be the maximum and minimum eigenvalue of the hessian respectively. . Then, ${a_{1}^{2} lambda_{1} + ... + a_{m}^{2} lambda_{m} leq a_{1}^{2} lambda_{max} + ,... , + a_{m}^{2} lambda_{max} = (a_{1}^{2}+ ,... , + a_{m}^{2}) lambda_{max} = lambda_{max}}$ . Also, ${a_{1}^{2} lambda_{1} + ... + a_{m}^{2} lambda_{m} geq a_{1}^{2} lambda_{min} + ,... , + a_{m}^{2} lambda_{min} = (a_{1}^{2}+ ,... , + a_{m}^{2}) lambda_{min} = lambda_{min}}$ . Therefore, second directional derivative in any direction can not be greater than ${ lambda_{max}}$ and can&#39;t be less than ${ lambda_{min}}$ . What&#39;s The Worst That Could Happen ? . Earlier we approximated ${f: R rightarrow R}$ at point ${x^ omicron}$. We do that again here but for function ${f: R^{m} rightarrow R}$. Using the Taylor series expansion, ${f( textbf x)}$ can be approximated at point ${ textbf x^ omicron}$ upto three terms as : . ${f( textbf x) approx f( textbf x^ omicron) + large frac{( textbf x - textbf x^ omicron)^{T}}{1!} small textbf g + large frac{1}{2!} small( textbf x- textbf x omicron)^{T}H( textbf x- textbf x omicron) , , , , textbf Eq(3)}$ , where ${ textbf g}$ and ${ textbf H}$ are gradient vector and Hessian matrix of ${f( textbf x)}$ respectively. . Gradient descent advises us to go to ${ textbf x = textbf x^ omicron - epsilon textbf g}$. Putting ${ textbf x^ omicron - epsilon textbf g}$ in ${ textbf Eq(3)}$ . ${f( textbf x^ omicron - epsilon textbf g) approx f( textbf x^ omicron) - epsilon textbf g^{T} textbf g + frac{1}{2} epsilon^{2} textbf g^{T} textbf H textbf g , , , ,Eq(4)}$. . Again, the second term here is the reduction in the value of function by going to ${ textbf x^ omicron - epsilon textbf g}$ and third term is the correction applied to account for curvature. Eagle eyed reader can notice that in the third term the quantity ${ textbf g^{T} textbf H textbf g}$ is the second directional derivative in the direction ${ textbf g}$ whose maximum value is equal to maximum eigenvector of ${ textbf H}$. The worst case scenario would be when ${ textbf g}$ aligns with the eigenvector of ${H}$ corresponding to the maximum eigenvalue. In this case, ${ textbf g^{T} textbf H textbf g = lambda_{max}}$ and if ${ epsilon}$ isn&#39;t small enough then the third term can increase the value of ${f}$. . In fact we can solve for what should be the optimal ${ epsilon}$. Let optimal value of learning rate be ${ epsilon^{ ast}}$. Then, ${ large frac{ partial}{ partial epsilon} small f( textbf x^{ omicron} - epsilon textbf g)}$ at ${ epsilon^{ ast} = 0}$. . ${ therefore - textbf g^{T} textbf g + epsilon^{ ast} textbf g^{T}H textbf g = 0 implies epsilon^{ ast} = large frac{ textbf g^{T} textbf g}{ textbf g^{T}H textbf g}}$. From here, we can see that in worst case scenario the optimal learning rate is ${ large frac{1}{ lambda_{max}}}$. If maximum eigenvalue of Hessian is large then this learning rate is too small to make significant progress. . Condition Number . (This part of blog is inspired from this brilliant distill-pub post Why Momentum really works) . The condition number of a matrix is the ratio of it&#39;s maximum eigenvalue and minimum eigenvalue. A matrix is said to be suffering from the case of poor condition number if it&#39;s condition number is too large. A Hessian matrix having poor condition number would mean that in directions which are inclined towards the eigenvector corresponding to the large eigenvalues the gradient changes too fast, while in directions inclined towards the eigenvector corresponding to lower eigenvalues the gradient changes slowly. This is probelmatic since gradient descent doesn&#39;t know about this effect. We would prefer to go in directions where gradient remains negative for longer (because negative gradient means value of function decreases by going in these directions). . We can see the ill effects of poor condition number by looking at a concrete example. Let&#39;s minimise the function ${f(w) = frac{1}{2}w^{T}Aw - b^{T}w}$ where ${w in R^{m}}$ and A is symmetric and invertible. Let optimal value of ${w}$ for this function be ${w^ ast}$. We can calculate the value of ${w^ ast}$ by using the fact that ${ large frac{ partial}{ partial w}f}$ at ${w^ ast = 0}$. ${ therefore Aw^ ast - b = 0 implies w^ ast = A^{-1}b}$. We want to arrive at ${w^ ast}$ using gradient descent. The gradient ${g}$ of ${f}$ is ${Aw-b}$ and the Hessian matrix is ${A}$. . We start from the point ${w^ omicron}$. Let&#39;s say at t-th iteration we are at ${w^t}$. The gradient advises us to go in opposite direction of ${Aw^t - b}$. . ${ therefore , w^{t+1} = w^t - epsilon(Aw^t - b) , , , , Eq(5)}$. We can calculate how far we are from optimal value at time t by calculating ${w^t - w^ ast}$. Subtracting ${w^ ast}$ from both sides of ${Eq(5)}$: . ${w^{t+1}- w^{ ast} = (w^t - w^ ast) - epsilon(Aw^t - b)}$ . Subtracting and adding ${w^ ast}$ in ${Aw^t}$ . ${w^{t+1}- w^{ ast}= (w^t - w^ ast) - epsilon large( small A(w^t-w^ ast + w^ ast) - b large) small = (w^t - w^ ast)(I - epsilon A) - epsilon Aw^ ast + epsilon b}$ where ${I}$ is the identity matrix. Since ${Aw^ ast = b}$, the last two terms cancel out. This leaves us with: . ${w^{t+1}- w^{ ast} = (I - epsilon A)(w^t - w^ ast) , , , , Eq(6)}$. . Since ${A}$ is symmetric matrix, we can write eigendecomposition of ${A}$ as ${ textbf Q Lambda textbf Q^{T}}$. Since ${ textbf Q}$ is orthogonal matrix, ${ textbf Q textbf Q^{T} = I}$. We know that multiplying a vector by an orthogonal matrix simply rotates the vector without changing it&#39;s magnitude. In case of vector ${w^{t+1}- w^{ ast}}$, we are only concerned with it&#39;s magnitude because that tells us how far we are from optimal point. So we can define a new variable ${x^{t} = Q^{T}(w^{t}- w^{ ast})}$. ${x^t}$ has same magnitude as $(w^{t}- w^{ ast})$. We do this because it will yield an easy interpretation in the end. . Multiplying both sides of ${Eq(6)}$ with ${Q^{T}}$ and replacing ${A}$ with it&#39;s eigendecomposition we get: . ${Q^{T}(w^{t+1}- w^{ ast}) = Q^{T}(QQ^{T} - epsilon Q Lambda Q^{T})(w^t - w^ ast) = Q^{T}Q(I - epsilon Lambda)Q^{T}(w^t - w^ ast) = (I - epsilon Lambda)x^t}$ . ${x^{t+1} = (I - epsilon Lambda)x^t}$. We can write ${x^{t+1}}$ in terms of ${x^ omicron}$ as: . ${x^{t+1} = (I- epsilon Lambda)^{t}x^ omicron = diag([1- epsilon lambda_{1},...,1- epsilon lambda_{m}])^{t}x^ omicron}$. . If a matrix ${B}$ is a diagonal matrix denoted by ${diag([b_{1}, b_{2},...,b_{n}])}$then ${B^k}$ is equal to ${diag([b_{1}^k, b_{2}^k,...,b_{n}^k])}$ . ${ therefore x^{t+1} = diag([(1- epsilon lambda_{1})^t,...,(1- epsilon lambda_{m})^t])x^ omicron}$. . Let ${x^ omicron_{i}}$ be the i-th element of vector ${x^ omicron}$ . ${x^{t+1} = left[ begin{array}{ccc}(1- epsilon lambda_{1})^t x^ omicron_{1} ... (1- epsilon lambda_{m})^t x^ omicron_{m} end{array} right]}$. . ${x^{t+1} = left[ begin{array}{ccc}x^{t+1}_{1} ... x^{t+1}_{m} end{array} right] = left[ begin{array}{ccc}(1- epsilon lambda_{1})^t x^ omicron_{1} ... (1- epsilon lambda_{m})^t x^ omicron_{m} end{array} right]}$ . This result yields an fascinating interpretation. At any iteration ${t}$, we want our error ${x^{t}}$ to be low. i-th component of error, ${x^{t}_{i}}$ is equal to ${(1- epsilon lambda_{i})^{t-1}x^{ omicron}_{i}}$. If ${(1- epsilon lambda_{i})^{t-1}}$ is small then error in that component is small. More specifically if ${ mid1- epsilon lambda_{i} mid}$ is less than 1, the convergence will be faster for the i-th component. If ${ mid1- epsilon lambda_{i} mid}$ is close to 1 then convergence will be slow. From here we can see that, components corresponding to larger eigenvalues will converge faster and those corresponding to lower eigenvalues will struggle till later iterations. . Let ${ lambda_{min}, lambda_{max}}$ be the least and largest eigenvalues of ${A}$ repectively. The overall convergence rate will be determined by error component which converges the slowest (i.e which is most close to 1). The slowest error component will be either ${ mid1- epsilon lambda_{min} mid}$ or ${ mid1- epsilon lambda_{max} mid}$ (because if ${ lambda_{min} approx 0}$, then ${ mid1- epsilon lambda_{min} mid approx 1}$, or if ${ epsilon =0.01}$ and ${ lambda_{max} = 200}$ then ${ mid1- epsilon lambda_{max} mid approx 1}$). We can adjust the learning rate so that error component corresponding to least and largest eigenvalue converge at same rate. Let optimal learning rate be ${ epsilon^ ast}$. . Then, ${ mid1- epsilon^ ast lambda_{min} mid} = { mid1- epsilon^ ast lambda_{max} mid}$ . ${1- epsilon^ ast lambda_{min} = epsilon^ ast lambda_{max}-1 implies epsilon^ ast = large frac{2}{ lambda_{min} + lambda_{max}}}$ . Then, the optimal convergence rate is ${1- large frac{2 lambda_{max}}{ lambda_{min} + lambda_{max}}}$. . Optimal rate = ${ frac{ large frac{ lambda_{max}}{ lambda_{min}} - 1}{ large frac{ lambda_{max}}{ lambda_{min}} + 1}}$. Let condition number be denoted by ${ kappa}$. . ${ therefore}$ Optimal rate = ${ large frac{ kappa-1}{ kappa+1}}$. This goes on to show that, larger the condition number the slower the gradient descent would be. . This analysis showed again that how the eigenvalues of Hessian Matrix affect the convergence rate and optimal learning rate of our algorithms. . Saddle Points . Suppose we are at a certain point where the gradient vector ${g}$ is ${0}$ (A vector is 0 when all of it&#39;s components are 0) and all the eigenvalues of the Hessian are negative. What does that mean ? It means that in whichever direction we go the gradient decreases. Since the gradient is already 0, it&#39;s components will become negative if we move in any direction. Negative components further mean that moving in any direction will lead to decrease in the value of function. This is an interesting case. Since the value of function is decreasing regardless of which direction we move in, we must be at the point where value of function is greater than the value of function at it&#39;s neighbouring points. This point is called a local maximum or a global maximum. . The opposite happens when the gradient vector ${g}$ is ${0}$ and all the eigenvalues of the Hessian are positive. This means that we are at point where the value of function is less than the value of function at it&#39;s neighbouring points. This point is called local minimum or global minimum. . The procedure where we check the sign of all the eigenvalues of Hessian at points where gradient vector is 0 is called the second derivative test. The second derivative helps us determine whether the point is local maximum or local minimum. . But what happens if all the eigenvalues of the Hessian don&#39;t have the same sign. In this case the second-derivative test is inconclusive. If some eigenvalues are positive it means that derivative and thus consecutively value of function increases in those directions, while the value of functions decreaes in directions corresponding to eigenvectors with negative eigenvalues. This point is called the saddle point of the function. . Let&#39;s take an example for better clarification. Let ${f: R^{2} rightarrow R}$ be a function defined as ${f(x_{1}, x_{2}) = x_{1}^{2} - x_2^2}$. The gradient vector of this function is ${g = left[ begin{array}{ccc} large frac{ partial f}{ partial x_1} large frac{ partial f}{ partial x_2} end{array} right] = left[ begin{array}{ccc}2x_{1} -2x_2 end{array} right]}$. The Hessian for this function is ${H = left[ begin{array}{ccc} large frac{ partial}{ partial x_1 partial x_1}f &amp; large frac{ partial}{ partial x_1 partial x_2}f large frac{ partial}{ partial x_2 partial x_1}f &amp; large frac{ partial}{ partial x_2 partial x_2}f end{array} right] = left[ begin{array}{ccc} 2 &amp; 0 0 &amp; -2 end{array} right]}$. . The gradient ${g}$ is 0 at the origin. We can see that vectors ${ left[ begin{array}{ccc} 1 0 end{array} right]}$ and ${ left[ begin{array}{ccc} 0 -1 end{array} right]}$ are eigenvectors of ${H}$ with eigenvalues 2 and -2 respectively (because when multiplied by Hessians these vectors are scaled by 2 and -2 respectively). Therefore origin is a saddle point because the value of function increases if we go in direction of ${x_1}$ and decreases in the direction ${x_2}$. This function is shown below: . This can be problematic since saddle point is maximum point for some cross section (${x_{2}-f}$ cross section in our case) and minimum point for other (${x_{1}-f}$ cross section in our case). The gradient is 0 at the saddle point and therefore we can&#39;t move anywhere using gradient descent. If the value of function at saddle point is large, then gradient descent can&#39;t help us in escaping from that point. .",
            "url": "https://abhimanyu08.github.io/blog/deep-learning/mathematics/2020/07/20/final.html",
            "relUrl": "/deep-learning/mathematics/2020/07/20/final.html",
            "date": " • Jul 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "What Does Label Smoothing Do?",
            "content": "Introduction . Label smoothing was introduced by Szegedy et.al in the paper Rethinking the Inception Architecture for Computer Vision. Since then, this trick has been used in many papers to improve the accuracy of various architectures. Although being widely used, there was less insight as to why this technique helps the model to perform better, but the paper by Rafael Müller et.al When does Label Smoothing Help? answers the question of &quot;What does label smoothing do?&quot; and &quot;Why does it help the model?&quot;. This blog post is an attempt to explain the main result of the paper. . 1. What Is Label Smoothing? . Generally, in a classification problem, our aim is to maximize the log-likelihood of our ground-truth label. In other words, we want our model to assign maximum probability to the true label given the parameters and the input i.e we want ${P(y mid x, theta)}$ to be high, where the ${y}$ is known beforehand. We motivate our model to achieve this by minimizing the cross-entropy loss between the predictions our model outputs and the ground truth labels. Cross entropy loss is defined by the equation: ${L(y, hat y)=- sum_{i=1}^{n} y_{i} times log( hat y_{i}) }$ where n is the number of classes, ${y_{i}}$ is 1 if image belongs to class ${i}$ and 0 otherwise, and ${ hat y_{i}}$ is probability of ${y_{i}}$ being 1. Don&#39;t be intimidated by the daunting equation and jargon because in reality the calculation of loss is very easy. Suppose you build a model for task of image-classification where an image can belong to one of the 3 classes. For every image as the input the model outputs a 3-dimensional vector. Let&#39;s say for a particular image the model&#39;s normalised output is ${ hat y = [0.2, 0.7,0.1]}$ and the image belongs to category 2. Therefore, the target vector for that image will be ${y = [0,1,0]}$. The loss for this image will be ${-(0 times log 0.2 + 1 times log 0.7 + 0 times log 0.1) = - log 0.7=0.35}$ which is low because our model assigns high probability to groud truth label. If instead our predictions are ${ hat y=[0.8,0.1,0.1]}$, then the loss will be ${- log 0.1=2.3}$ which is high because now our model assigns low probability to ground-truth label. . There is little more to how the normalised predictions of the model are calculated. The model&#39;s predictions are calulated by applying the Softmax activation on the last layer&#39;s output. The model outputs a 3-dimensional vector and each of the element of the vector is called &#39;logit&#39;. For the logits to represent a valid probability distribution over the classes they should sum to 1. This is accomplished by passing the logits through a softmax layer. Let&#39;s say the output vector for a certain image as input is ${z = [z_{1}, z_{2},...,z_{n}]}$ then the predictions are calculated as ${ hat y = text Softmax left(z right) = large [ frac {e^{z_{1}}}{ sum_{i=1}^{n} e^{z_{i}}}, frac {e^{z_{2}}}{ sum_{i=1}^{n} e^{z_{i}}},..., frac {e^{z_{n}}}{ sum_{i=1}^{n} e^{z_{i}}}]}$.${(Eq ,1.1)}$ Notice that sum of all the elements of ${ hat y}$ is 1. Suppose the ground truth label for the image is 2, then the target vector is ${[0,1,0,0,....0]}$ (The length of target vector is n as well). Thus, the Cross-entropy loss for this image, in it&#39;s full glory is written as ${ text loss left(y,z right) = -1 times normalsize log frac {e^{z_{2}}}{ sum_{i=1}^{n} e^{z_{i}}} = log { sum_{i=1}^{n} e^{z_{i}}} - z_{2}}$. Minimising this loss encourages ${z_{2}}$ to be as high as possible while ${z_{i}}$ for ${i ne2}$ are encouraged to be low. Szegedey et.al highlight two problems with this approach: . The first problem with this approach is that model becomes over-confident for it&#39;s predictions as it learns to assign nearly 100% probability to the ground-truth label. Szegedy et. al argue that this can lead to overfitting and model may not be able to generalize well. Intuitively this makes sense. for.eg Let&#39;s say our dataset contains two symantically similar classes class1 and class2(pets dataset has plenty of those). Unfortunately our dataset contains many instances of class1 but relatively less instances of class2. Suppose image1 belongs to class1 and image2 to other. Because these images are very similar, the output logits of these images would be very similar. Our over-confident model may assign class1 to the image2 with high confidence(close to 100% probability) and this can incur heavy validation loss. . | The other problem with this approach is the vanishing gradient. The gradient of our loss w.r.t logit of correct class label k is ${ large frac {e^{z_{k}}}{ sum_{i=1}^{n} e^{z_{i}}}-1}$ and w.r.t other logits is ${ large frac {e^{z_{i}}}{ sum_{i=1}^{n} e^{z_{i}}}}$. Minimising the cross-entropy loss leads to logit corresponding to correct class to be much higher than other logits. This leads to vanishing of gradients of loss w.r.t other logits and thus it hinders the model&#39;s ability to adapt. . | What can we do to counteract these two problems? Szegedy et.al suggest that we shouldn&#39;t provide sparse one-hot encoded vectors as target. Instead we should &quot;smoothen&quot; them. This is done by replacing the probability distribution over labels from dirac delta distribution to a linear combination of dirac delta distribution and a uniform distribution. This may sound incredibly complex to hear but in reality is very easy to implement. Let&#39;s define what the above jargon means. . Dirac delta function denoted by ${ delta _{i,l}}$ is a function which is 1 for ${i=l}$ and 0 everywhere else. If an image has class ${l=3}$ as it&#39;s label and there are ${k=4}$ classes in total, then the target vector for that image has the probability distribution ${ delta _{i,3}}$ for ${i=1 ,to ,k}$. ${i}$ represents index of the target vector (I haven&#39;t used 0 indexing) and therefore the target vector is ${[0,0,1,0]}$. Notice that ${ delta _{i,l}}$ is a valid probability distribution as it sums to 1 over it&#39;s domain. A uniform distribution is a distribution which has a constant value over it&#39;s domain. Let&#39;s say our domain consists of ${ {x epsilon[1,4]: x epsilon I }}$. This is read as x belongs to 1 to 4 both included such that x is an integer. So, ${x epsilon {1,2,3,4 }}$. Uniform distribution over this domain is denoted as ${U left(x right)}$. ${ therefore U left(1 right) = U left(2 right) = U left(3 right) = U left(4 right) = c}$. The sum over the domain i.e ${ sum_{i=1}^{4} U(i)}$ is 4c. For ${U(x)}$ to be a valid probability distribution ${4c}$ should equal to 1. ${ therefore c=0.25}$. More generally we can say that if there are ${k}$ points in our domain, then uniform distribution over the domain would be ${U(i)= frac{1}{k}}$ where ${i}$ is any point in the domain. . Let&#39;s denote the distribution over our labels for a particular image as ${q left(i right)}$ where ${i=1 ,to ,k}$. ${k}$ denotes the total no of classes and ${l}$ denotes the true label for the image. Generally, ${q left(i right) = delta _{i,l}}$. Szegedy et. al propose to replace ${ delta _{i,l}}$ with ${(1- varepsilon) delta _{i,l} + varepsilon U left(i right) ,for ,i=1 ,to ,k}$ where ${ varepsilon}$ is a hyperparameter. As explained above, value of ${U left(i right)}$ should be ${ frac {1}{k} ,for ,i=1 ,to ,k}$. Then, our new distribution over labels is ${q&#39;(i) = (1- varepsilon) delta _{i,l} + frac{ varepsilon}{k}}$ $(Eq ,1.1)$. Let&#39;s see how to do this using an example. . Suppose distribution over target labels of an image say image1 for a classification task which has ${k=4}$ classes is ${q left(i,2 right)= delta_{i,2} , for ,i = 1 ,to ,k}$. ${i}$ here represents index of target vector. Thus, target vector will be ${y^{h}=[0,1,0,0]}$. Then, our new distribution over labels according to ${Eq ,1.1}$ is ${q&#39; left(i,2 right) = (1- varepsilon) delta _{i,2} + frac{ varepsilon}{4}}$ for ${i=1 ,to ,4}$. Subsequently, smoothened target vector ${y^{l}}$ will be ${[ frac{ varepsilon}{4},(1- varepsilon)+ frac{ varepsilon}{4}, frac{ varepsilon}{4}, frac{ varepsilon}{4}]}$ = ${[0.25 varepsilon, 1- varepsilon+0.25 varepsilon, 0.25 varepsilon,0.25 varepsilon]}$. If ${ varepsilon = 0.2}$,then ${y^{l} =[0.05,0.85,0.05,0.05]}$. Notice that elements of new smoothened label vector still sum to 1, which confirms that ${(1- varepsilon) delta _{i,l}+ varepsilon U left(i right)}$ is a valid probability distribution over the labels. . Let&#39;s see what difference does it make to change the labels in the way shown above. Suppose our model outputs the prediction vector ${p_{1}=[0.05,0.9,0.03,0.02]}$ for image1. So the model is really confident that this image has label 2 which is a good thing since this image really does has label 2. The loss with smoothened labels ${y^{l}}$ will be ${L(y^{l},p_{1})= -(0.05 log 0.05+0.85 log0.9+0.05 log0.03+0.05 log0.02)= 0.61}$. Now suppose our model didn&#39;t output ${p_{1}}$ but ${p_{2}=[0.01,0.79,0.15,0.05]}$. In this case it is less sure that the image has label 2. Loss will be ${L(y^{l},p_{2})= -(0.05 log0.01+0.85 log0.79+0.05 log0.15+0.05 log0.05)=0.29}$ which is less than the loss with ${p_{1}}$ ! This goes on to show that smooth labels want the model to be confident about it&#39;s predictions but not over-confident. . Intuitively we can think of label smoothing as a process to reduce the confidence of model in it&#39;s ground truth labels.The ground truth labels may sometimes be awry owing to errors in data labelling or data collection process. Label smoothing can make the model robust against those incorrect labels. . 2. Implementation In Code . To implement label smoothing, we don&#39;t change every label individually but we define a new loss function. Loss function is still Cross-entropy loss but our target vector for every image changes. Our new target vector for a particular image is ${y^{l} = [ frac { varepsilon}{k}, frac { varepsilon}{k},...,(1 - varepsilon) + frac{ varepsilon}{k}, frac { varepsilon}{k}, frac { varepsilon}{k},...k times]}$. Let&#39;s assume the image belongs to class ${j}$. Normal one hot encoded target label will have 1 at j position and 0 everywhere else. Let&#39;s denote it as ${y^{h}}$. So, ${y^{h} = [0,0,0,...,1,0,...0]}$. . The loss with ${y^{h}}$ is ${L(y^{h}, hat y)= - log hat y_{j}}$. ${Eq ,2.1}$ . The loss with new smoothened labels is ${L left(y^{l}, hat y right) = sum_{i=1}^{k} -y_{i}^{l} log hat y_{i}}$ = ${- left( frac { varepsilon}{k} log hat y_{1} +...+ left(1- varepsilon+ frac{ varepsilon}{k} right) log hat y_{j}+ frac { varepsilon}{k} log hat y_{j+1}+...+ frac { varepsilon}{k} log hat y_{k} right)}$. We can rewrite this as ${L left(y^{l}, hat y right) = - left(1- varepsilon right) times log hat y_{j} - frac{ varepsilon}{k} times left( sum_{i=1}^{k} log hat y_{i} right)}$. Eagle eyed reader can notice that the term which is multiplied by ${ left(1 - varepsilon right)}$ is the cross-entropy loss calculated with one hot encoded target vector. Therefore, ${L left(y^{l}, hat y right) = left(1- varepsilon right)L left(y^{h}, hat y right)- frac{ varepsilon}{k} left( sum_{i=1}^{k} log hat y_{i} right)}$. ${Eq ,2.2}$ . So, we only need to modify the loss function of our model and we are good to go. The implementation of this in code is shown below. The code snippet below uses Pytorch framework and implementation is copied from the fast.ai course. . #collapse-show from torch import nn def lin_comb(a1,a2,factor): &#39;&#39;&#39;This function calculates linear combination of two quantities a1 and a2 where the respective coeffecients are factor and (1-factor)&#39;&#39;&#39; return factor*a1 + (1-factor)*a2 def reduce_loss(loss, reduction=&#39;mean&#39;): &#39;&#39;&#39;We need this function because we generally calcualate losses for a batch of images and take the mean or sum all the losses. But throughout this blog we input only a single image in the model so you can ignore this fuction and just assume that this funtion does nothing. for.eg reduce_loss(2)=2&#39;&#39;&#39; return loss.mean() if reduction==&#39;mean&#39; else loss.sum() if reduction==&#39;sum&#39; else loss class LabelSmoothing(nn.Module): def __init__(self, f:float=0.1, reduction = &#39;mean&#39;): super().__init__() self.f = f #factor for linear combination self.reduction = reduction #You can safely ignore this def forward(self,pred,targ): #this line of code implements Eq 1.1 ls = F.log_softmax(pred, dim = 1) #this line of code calculates the sum part of second term in Eq 2.2 l1 = reduce_loss(ls.sum(1), self.reduction) #this line of code calculates Eq 2.1 l2 = F.nll_loss(ls, targ,reduction= self.reduction) #finally this line implements Eq 2.2 return lin_comb(-l1/pred.shape[-1],l2,self.f) . . 3. How And Why Does It Work? . Label smoothing goes against the conventional practice of maximising the likelihood of ground truth label. Instead, it punishes the model if the logits which don&#39;t correspond to correct label get too low. This can be seen by the second term in equation of loss mentioned above i.e ${- frac{ varepsilon}{k} left( sum_{i=1}^{k} log hat y_{i} right)}$. We can see that if ${ hat y_{i} , for , i = {1,2,...,k}}$ go too close to 0 then the loss goes up (${ log}$ of something close to 0 is a large negative number). In contrast, maximising the likelihood of one-hot encoded ground-truth label encourages the logits that don&#39;t correspond to correct label to go as low as possible. With smooth labels ${y^{l}}$ our aim is to maximise ${P(y^{l} mid x, theta)}$. Let&#39;s see why maximising the likelihood of smooth labels instead of maximising the likelihood of one-hot encoded labels is benificial for our model. . Calculating Loss Without Label Smoothing . Let&#39;s imagine that we have a task to build a model for image classification task where each image can have one of three labels. This means our model will output a 3-dimensional vector containing our three logits. Assume that penultimate layer of the model has 4 activations. We put in an image in this model which has a target vector ${y^{h} = [0,1,0]^{T}}$. The penultimate layer&#39;s activations are ${X = [x_{1},x_{2},x_{3},x_{4}]^{T}}$, the last layer&#39;s outputs are ${Z = [z_{1},z_{2},z_{3}]^{T}}$ (A single vector is conventionally written as column vector, therefore, ${X}$, ${Z}$ and ${y^{h}}$ are written as transpose of row vectors). ${Z}$ is calculated from the penultimate layer&#39;s activation using the equation ${Z = W star X}$ (${ star}$ here denotes matrix multiplication). Bias is ignored for sake of brevity. ${W}$ is the weight matrix connecting penultimate layer and output layer. ${W = left[ begin{array}{ccc} w_{11} &amp; w_{12} &amp; w_{13} &amp; w_{14} w_{21} &amp; w_{22} &amp; w_{23} &amp; w_{24} w_{31} &amp; w_{32} &amp; w_{33} &amp; w_{34} end{array} right]}$. Shortly weight matrix can be written as ${W = [w_{1},w_{2},w_{3}]^{T}}$ where ${w_{i} = [w_{i1},w_{i2},w_{i3},w_{i4}]}$. The output vector ${Z}$ is calculated as ${W star X = left[ begin{array}{ccc} w_{11} times x_{1} &amp; w_{12} times x_{2} &amp; w_{13} times x_{3} &amp; w_{14} times x_{4} w_{21} times x_{1} &amp; w_{22} times x_{2} &amp; w_{23} times x_{3} &amp; w_{24} times x_{4} w_{31} times x_{1} &amp; w_{32} times x_{2} &amp; w_{33} times x_{3} &amp; w_{34} times x_{4} end{array} right]}$. In short this can be written as ${Z = left[ begin{array}{ccc} z_{1} z_{2} z_{3} end{array} right] = left[ begin{array}{ccc} w_{1}X^{T} w_{2}X^{T} w_{3}X^{T} end{array} right]}$ where ${w_{i}X^{T}}$ denotes inner product between ${w_{i}}$ and ${X^{T}}$. ${Z}$ is a vector of logits and is un-normalised. To get our prediction vector we would have to normalise this by passing ${Z}$ through a softmax layer. Our prediction vector would be ${ hat y = left[ begin{array}{ccc} frac {e^{w_{1}X^{T}}}{e^{w_{1}X^{T}}+e^{w_{2}X^{T}}+e^{w_{3}X^{T}}} frac {e^{w_{2}X^{T}}}{e^{w_{1}X^{T}}+e^{w_{2}X^{T}}+e^{w_{3}X^{T}}} frac {e^{w_{3}X^{T}}}{e^{w_{1}X^{T}}+e^{w_{2}X^{T}}+e^{w_{3}X^{T}}} end{array} right]}$. As given before, our target vector is ${y^{h} = [0,1,0]^{T}}$. So, our cross-entropy loss will be ${L left(y^{h}, hat y right) = - log left( frac {e^{w_{2}X^{T}}}{e^{w_{1}X^{T}}+e^{w_{2}X^{T}}+e^{w_{3}X^{T}}} right)}$. For preserving our sanity let&#39;s denote ${e^{w_{1}X^{T}}+e^{w_{2}X^{T}}+e^{w_{3}X^{T}}}$ by ${S}$. Then, ${L left(y^{h}, hat y right) = - log left( frac {e^{w_{2}X^{T}}}{S} right) = log {S}-{w_{2}X^{T}}}$ . Calculating Loss With Label Smoothing . Our prediction vector is same as before, but our target vector changes. Let&#39;s denote our label smoothed target vector as ${y^{l}}$. So, ${y^{l} = [ frac { varepsilon}{3}, 1- varepsilon + frac { varepsilon}{3}, frac { varepsilon}{3}]^{T}}$. Then, our new loss will be ${L left(y^{l}, hat y right) = - frac { varepsilon}{3} times log frac {e^{w_{1}X^{t}}}{S}- left(1- varepsilon+ frac{ varepsilon}{3} right) times log frac {e^{w_{2}X^{t}}}{S}- frac { varepsilon}{3} times log frac {e^{w_{3}X^{t}}}{S}}$. Grouping the varibles appropriately, . ${L left(y^{l}, hat y right)= - left(1- varepsilon right) times log frac {e^{w_{2}X^{t}}}{S}- frac { varepsilon}{3} times left( log frac {e^{w_{1}X^{t}}}{S}+ log frac {e^{w_{2}X^{t}}}{S}+ log frac {e^{w_{3}X^{t}}}{S} right)}$. Remember that ${ log a + log b = log ab}$. Utilising this rule, loss can be written as ${L left(y^{l}, hat y right)= left(1- varepsilon right) left( log S-w_{2}X^{T} right)- frac{ varepsilon}{3} times{ log left( frac{e^{w_{1}X^{T}+w_{2}X^{T}+w_{3}X^{T}}}{S^{3}} right)}}$. To further reduce this equation, we need to know two more rules: . ${ log frac{a}{b}= log a- log b}$ and | ${ log a^{b}=b log a}$. | Then, ${L left(y^{l}, hat y right)= left( log S-w_{2}X^{T} right)- varepsilon left( log S - w_{2}X^{T} right)- frac{ varepsilon}{3} left(w_{1}X^{T}+w_{2}X^{T}+w_{3}X^{T} right)+ frac{ varepsilon}{3} log left(S^{3} right)}$. . Expanding the second term in this expression and using rule 2 we get, ${L left(y^{l}, hat y right)= left( log S-w_{2}X^{T} right)- varepsilon log S+ varepsilon left(w_{2}X^{T} right)- frac{ varepsilon}{3} left(w_{1}X^{T}+w_{2}X^{T}+w_{3}X^{T} right)+{ varepsilon} log S}$. Notice, that first term of the last expression is our ${L left(y^{h}, hat y right)}$. Therefore, our loss with smooth labels can be finally written as ${L left(y^{l}, hat y right)=L left(y^{h}, hat y right)+ frac{ varepsilon}{3} left(2w_{2}X^{T}-w_{1}X^{T}-w_{3}X^{T} right)}$. . 4. Geometric Point Of View . (X -&gt; Penultimate layer&#39;s activation) . Our last layer&#39;s output for the image we input earlier is ${Z= left[ begin{array}{ccc} w_{1}X^{T} w_{2}X^{T} w_{3}X^{T} end{array} right]}$. Since this image belongs to class 2, minimising any of the loss functions calculated above increases ${w_{2}X^{T}}$ while ${w_{1}X^{T}}$ and ${w_{3}X^{T}}$ are decreased. More generally, if an image belongs to class ${k}$ then in minimising the loss, ${z_{k}=w_{k}X^{T}}$ is increased while every other logit is decreased. Also, notice a pattern that ${w_{i}}$ produces logits for class ${i}$ using the operation ${w_{i}X^{T}}$. Hence, ${w_{i}}$ can be thought of as a template for class ${i}$. So,from now on I&#39;ll sometimes refer to ${w_{i}}$ as template for class ${i}$. Let&#39;s try to view the process of minimising or maximising ${w_{i}X^{T}}$ geometrically. . Euclidean Norm . Euclidean norm of two vectors is simply the distance between the two vectors in their space. Euclidean Norm for two vectors ${a}$ and ${b}$ can be calculated as: ${ lVert a-b rVert= left(a^{T} star a-2a^{T} star{b}+b^{T} star b right)^{ frac{1}{2}}}$. ${ therefore lVert a-b rVert^{2}= a^{T} star a-2a^{T} star{b}+b^{T} star b}$. (Remeber that ${ star}$ denotes matrix multiplication.) . Loss Mimisation as Distance Minimisation/Maximisation . Now that we know how to calculate the euclidean norm, let&#39;s calculate it for ${w_{i}}$ and ${X}$. ${ lVert w_{i}-X rVert^{2}= w_{i}^{T} star w_{i}-2w_{i}^{T} star{X}+X^{T} star X= w_{i}^{T} star w_{i}-2w_{i}{X}^{T}+X^{T} star X}$. (For any two vectors ${a}$ and ${b}$, ${a star b=a.b^{T}}$ where ${ star}$ and ${.}$ denote matrix multiplication and inner product respectively). Geometrically, this quantity is square of the distance between template for class ${i}$ and penultimate layer&#39;s activation ${X}$. . Notice the second term inside the expression of ${ lVert w_{i}-X rVert^{2}}$ which is ${2w_{i}X^{T}}$. If this term increases, the distance between ${w_{i}}$ and ${X}$ decreases and whenever it decreases the mentioned distance increases. But notice that this second term is just the same as ${2 times z_{i}}$. This means whenever ${z_{i}}$ increases/decreases, distance between ${w_{i}}$ i.e tempelate for class ${i}$ and ${X}$ i.e penultimate layer&#39;s output vector decreases/increases. If an image belongs to class ${k}$, minimising the loss increases ${z_{k}}$ and decreases every other logit. This means that minimising the loss is same as minimising the distance between penultimate layer&#39;s output ${X}$ and template for correct class ${w_{k}}$ and maximising the distance between ${X}$ and template for every incorrect class i.e ${w_{i}}$ where ${i neq k}$. . Thus, we can infer that minimising ${L left(y^{h},Z right)}$ or ${L left(y^{l},Z right)}$ produces the same effect which is to bring ${w_{k}}$ close to ${X}$ when image belongs to class ${k}$ and taking ${w_{i}}$ where ${i neq k}$ far from ${X}$. The different performance of these two losses stem from the manner in which they go about doing this which is explained below. . 5. Derivatives Of Losses Tell The Difference. . Now let&#39;s, painstakingly write the two losses without any abridgement. . ${L left(y^{h}, hat y right)= log left(e^{w_{1}X^{T}}+e^{w_{2}X^{T}}+e^{w_{3}X^{T}} right)-w_{2}X^{T}}$ . ${L left(y^{l}, hat y right)= log left(e^{w_{1}X^{T}}+e^{w_{2}X^{T}}+e^{w_{3}X^{T}} right)-w_{2}X^{T}+ frac{ varepsilon}{3} left(2w_{2}X^{T}-w_{1}X^{T}-w_{3}X^{T} right)}$. . The reason we wrote the losses like this is because written this way, it&#39;ll be easy to take their derivatives w.r.t any term we want. We know that we minimise the loss using gradient descent. Imagine the loss surface as a convex surface (like a hollow ball cut in half and it&#39;s upper hemisphere removed). Our aim is to go to the lowest point in this convex region where the loss is lowest. We go to this point by continously changing our parameters using the gradient descent rules. Now, at this point we need to remember some rules from calculus. . Let&#39;s say there&#39;s a function ${f left(x right)}$. It&#39;s derivative w.r.t x, ${ normalsize frac{df}{dx}}$ can be denoted as ${f&#39;(x)}$. Notice that derivative is also a function of x. Suppose ${f left(x right)}$ is at it&#39;s minimum at point ${x^{ ast}}$. Then, ${f&#39;(x^{ ast})=0}$. . | ${ normalsize frac{d log x}{dx}= frac{1}{x}}$ . | Derivative of ${L left(y^{h}, hat y right)}$ . Let&#39;s imagine that we trained our model using the loss ${L left(y^{h}, hat y right)}$ and through meticulous training we have reached the minimum point on our loss surface i.e our loss is lowest it can be (Sadly, in practice this doesn&#39;t happen but we still assume this because by doing so we can infer how the parameters behave in order to reach the holy grail i.e global minima or a satisfactory local minima). The value of ${W}$ at minima is ${W^{ ast}= left[ begin{array}{ccc} w_{1}^{ ast} w_{2}^{ ast} w_{3}^{ ast} end{array} right]}$. . Now we take derivative of ${L left(y^{h}, hat y right)}$ w.r.t ${W}$. The derivative is written as ${L&#39;_{h} left(W right)= frac{ delta L_{h}}{ delta W}= left[ begin{array}{ccc} frac{ delta L_{h}}{ delta w_{1}} frac{ delta L_{h}}{ delta w_{2}} frac{ delta L_{h}}{ delta w_{3}} end{array} right]}$ (${L_{h}} , denotes , L left(y^{h}, hat y right) $). Since ${L_{h}}$ is composed of two variables ${X}$ and ${w_{i}}$, it&#39;s derivative w.r.t one of the variables is written with delta (${ delta}$) sign. This sign simply denotes that while taking derivative of a function w.r.t to a variable treat the other variable as constant. Since we are taking derivative w.r.t ${w_{i}}$ we will treat ${X}$ as constant. . ${ large frac{ delta L_{h}}{ delta w_{1}}= frac{e^{w_{1}X^{T}} X^{T}}{S}}$. | ${ large frac{ delta L_{h}}{ delta w_{2}}= frac{e^{w_{2}X^{T}} X^{T}}{S}- normalsize X^{T}}$. | ${ large frac{ delta L_{h}}{ delta w_{3}}= frac{e^{w_{3}X^{T}} X^{T}}{S}}$. | . Now from rules of calculus we know that ${ large frac{ delta L_{h}}{ delta w_{1}}= large frac{ delta L_{h}}{ delta w_{2}}= large frac{ delta L_{h}}{ delta w_{3}}= normalsize0}$ at ${W^{ ast}}$. . ${ therefore large frac{e^{w_{1}^{ ast}X^{T}} X^{T}}{S}=0 implies e^{w_{1}^{ ast}X^{T}}=0 implies normalsize w_{1}^{ ast}X^{T}=- infty ,(Eq.1)}$. Similarly, ${ normalsize w_{3}^{ ast}X^{T}=- infty ,(Eq.2)}$. The case is different with ${w_{2}}$ though because it is the tempelate corresponding to the correct class. ${ large frac{e^{w_{2}^{ ast}X^{T}} X^{T}}{S}- normalsize X^{T}=0 implies frac{e^{w_{2}^{ ast}X^{T}}}{e^{w_{1}^{ ast}X^{T}}+e^{w_{2}^{ ast}X^{T}}+e^{w_{3}^{ ast}X^{T}}}=1}$ ${(Eq ,3)}$. ${(Eq ,3)}$ implies that ${w_{1}^{ ast}}$ and ${w_{3}^{ ast}}$ are negligible when compared to ${w_{2}^{ ast}}$. ${Eq.1}$ and ${Eq.2}$ show that ${L_{h}}$ is minimum when the distance between tempelate of incorrect labels ($w_{1}^{ ast}$,$w_{2}^{ ast}$) and ${X}$ is ${ infty}$. From this we can infer the behaviour inflicted upon the weights connecting penultimate layer and Final Layer by reducing the loss ${L_{h}}$. Minimising this loss takes the weights corresponding to incorrect class away from penultimate layer&#39;s activations without any bounds. i.e ${X}$ and the templates of incorrect classes really begin to hate each other and go as far away from each other as possible. . Derivative of ${L left(y^{l}, hat y right)}$ . This time we train the model using the loss ${L left(y^{l}, hat y right)}$ and again reach the impractical situation where we are at the global minimum or a satisfactory local minimum of the loss surface. The ${W}$ at this point is ${W^{ star}}$. Note that this ${W^{ star}}$ is different from ${W^{ star}}$ of previous subsection because our loss surface is different. (Apologies if you get confused due to notation. ${W}$ is a variable while ${W^{ star}}$ is a fixed value of that variable which occurs at minima). . The derivative of loss w.r.t ${W}$ is given as ${L&#39;_{l} left(W right)= frac{ delta L_{l}}{ delta W}= left[ begin{array}{ccc} frac{ delta L_{l}}{ delta w_{1}} frac{ delta L_{l}}{ delta w_{2}} frac{ delta L_{l}}{ delta w_{3}} end{array} right]}$ (${L_{l}} , denotes , L left(y^{l}, hat y right)$). . ${ large frac{ delta L_{l}}{ delta w_{1}}= frac{e^{w_{1}X^{T}} X^{T}}{S}- frac{ varepsilon}{3} normalsize X^{T}}$. | ${ large frac{ delta L_{l}}{ delta w_{2}}= frac{e^{w_{2}X^{T}} X^{T}}{S}- normalsize X^{T}+ large frac {2 varepsilon X^{T}}{3}}$. | ${ large frac{ delta L_{l}}{ delta w_{3}}= frac{e^{w_{3}X^{T}} X^{T}}{S}- frac{ varepsilon}{3} normalsize X^{T}}$. | . We know that ${ large frac{ delta L_{l}}{ delta w_{1}}= large frac{ delta L_{l}}{ delta w_{2}}= large frac{ delta L_{l}}{ delta w_{3}}=0}$ at ${W^{ star}}$. . ${ therefore large frac{e^{w_{1}^{ ast}X^{T}}X^{T}}{S}- frac{ varepsilon X^{T}}{3}=0 implies e^{w_{1}^{ ast}X^{T}}= frac{S large varepsilon}{3} implies normalsize w_{1}^{ ast}X^{T}= log frac{ normalsize S varepsilon}{3}}$ ${(Eq.3)}$. Similarly, ${ normalsize w_{3}^{ ast}X^{T}}= large log frac{S varepsilon}{3}$ ${(Eq.4)}$. In case of ${ large frac{ delta L_{l}}{ delta w_{2}}}$, ${ large frac {e^{w_{2}^{ ast}X^{T}}X^{T}}{S}- normalsize X^{T}+ large frac {2 varepsilon X^{T}}{3} =0 implies large e^{w_{2}^{ ast}X^{T}}= normalsize S(1- frac{2 varepsilon}{3})}$ ${(Eq.5)}$. . To interpret ${Eq.5}$ let&#39;s put in the value of ${ varepsilon}$. Generally, ${ varepsilon}$ is taken as 0.1. Putting that in ${Eq.5}$, ${ large e^{w_{2}^{ ast}X^{T}}= normalsize S( frac{2.8}{3})= normalsize0.93(e^{w_{1}^{ ast}X^{T}}+e^{w_{2}^{ ast}X^{T}}+e^{w_{3}^{ ast}X^{T}}) implies 0.07(e^{w_{2}^{ ast}X^{T}})=e^{w_{1}^{ ast}X^{T}}+e^{w_{3}^{ ast}X^{T}}}$. This shows that ${w_{2}^{ ast}X^{T}}$ is still large compared to ${w_{1}^{ ast}X^{T}}$ and ${w_{3}^{ ast}X^{T}}$. But there&#39;s one thing different, ${Eq.3}$ and ${Eq.4}$ show that at optimal point ${w_{1}^{ ast}X^{T}}$ and ${w_{3}^{ ast}X^{T}}$ are not ${- infty}$ but a finite quantity i.e ${ normalsize log frac{S varepsilon}{3}}$. This shows that minimising ${L_{l}}$ doesn&#39;t decrease ${w_{1}^{ ast}X^{T}}$ and ${w_{3}^{ ast}X^{T}}$ without any bounds, but decreases them upto a certain point which is same for ${w_{1}^{ ast}X^{T}}$ and ${w_{3}^{ ast}X^{T}}$. We can see that ${X}$ is equidistant from both ${w_{1}^{ ast}}$ and ${w_{3}^{ ast}}$. . Geometrically, we can say that minimising ${L_{l}}$ decreases the distance between tempelate of correct class and penultimate layer&#39;s activation (${X}$), and also encourages ${X}$ to go far from tempelates of incorrect classes but also remain equidistant from them. In this case ${X}$ hates the templates of incorrect classes but not as much as the ${X}$ of the previous subsection. Also, it hates all the incorrect class templates equally and tries to remain equidistant from them. The ${X}$ and template of correct class in this section love each other but not as strongly as those of previous section. (Apologies for the cheesy interpretation) . So this is where ${L_{l}}$ is different from ${L_{h}}$. . I hope now you can make sense of the main statement of the paper by Rafael Müller et.al which I quote verbatim: &quot;label smoothing encourages the activations of the penultimate layer to be close to the template of the correct class and equally distant to the templates of the incorrect classes.&quot; . 6. Okay, So How Does It Help My Model? . (In this section ${Xi}$ will denote the penultimate layer&#39;s activation when an image belonging to class ${i}$ is input in the model.) . ${L_{h}}$ = Loss calculated with one-hot encoded target vectors | ${L_{l}}$ = Loss calculated with target vectors with smooth labels | . Let&#39;s go with the above scenario that we have a task of image classifiaction where a given image can belong to 3 classes. Suppose that class1 and class2 among these are symantically very similar (for.eg toy poodle and miniature poodle class of ImageNet). This means that if you input an image belonging to class1 and another image belonging to class2, their penultimate layer&#39;s activation can be very similar. Now, you prepare your dataset but you unfortunately forget to shuffle it randomly and so, all the images belonging to class1 are placed before all the images of class2 in the dataset. You begin training on this dataset by using a suitable batch size and using the loss ${L_{h}}$. By the time a batch of images belonging to class2 goes inside your model, the model has already been partially tuned by class1 images. Since you are training with ${L_{h}}$ loss, the ${X1}$ which derive from images of class1 have been dragged extremely far away from template of class2 and class3. Now, a batch of images belonging to class2 goes inside the model. Since class2 is symantically similar to class1, images belonging to this class have their penultimate layer&#39;s activation ${X2}$ very similar to ${X1}$. Because of this, these images will show a very strong affinity for class1 and despise being predicted that they belong to class2 because ${w_{1}(X2)^{T}}$ will be high for these images and at same time ${w_{2}(X2)^{T}}$ will be low. This will incur large value of loss which is bad for model. To remedy this, the model will need to take large steps and will take longer time to reduce the huge loss value. Instead, if we had trained on ${L_{l}}$, our model wouldn&#39;t have to work as hard to adapt since model itself is not entirely confident that if penultimate layer&#39;s activation are similar to ${X1}$ then the label is class1. Since ${w_{2}}$ is not dragged too far away from ${X1}$ and thus consequently ${X2}$ , the loss wouldn&#39;t be as high as in the previous case and the model will have the ability to adapt quickly. (Maybe this example also shows the importance of randomly shuffling your data). . Another advantage is incurred in classification. Suppose that this time learning from our previous mistake we shuffled the data randomly but trained the model with loss ${L_{h}}$. Since images belonging to class1 and class2 are very similar, both ${X1}$ and ${X2}$ are close to both ${w_{1}}$ and ${w_{2}}$. Also both (${X1}$) and (${X2}$) are far away from ${w_{3}}$. Although this model will accurately differentiate between class1 and class3 or class2 and class3,it may also sometimes misclassify images, if the image belonging to class1 or class2 is fed into the model. Because their ${X&#39;s}$ are so similar, model may assign class2 to an image belonging to class1 or vice versa. But instead if we train with ${L_{l}}$ loss, ${X1}$ will be equidistant from ${w_{2}}$ and ${w_{3}}$. Similarly, ${X2}$ will be equidistant from ${w_{1}}$ and ${w_{3}}$. Then, if we put an image belonging to class2 and it produces ${X2}$ from it&#39;s penultimate layer, model will correctly assign it class2 and will not confuse it with class1. (If distance of ${X2}$ from ${w_{1}}$ and ${w_{3}}$ is sufficiently large). . Sometimes, during our data labelling process, some images may get incorrect labels due to human error or other factors. In that case you don&#39;t want the penultimate layer activations of your images to cling too tightly to template of incorrectly labelled class, which would inevitably happen if you use ${L_{h}}$ loss. To make the modul robust against these incorrect labels, Label smoothing can come in handy because it decreases the model&#39;s confidence in it&#39;s incorrect ground-truth labels and doesn&#39;t let the ${X&#39;s}$ of images get too close to their incorrect label templates. Even though ${X}$ will get close to the tempelate of it&#39;s incorrect labels, it would be easier to modify if model is trained with ${L_{l}}$ instead of ${L_{h}}$. . 7. Conclusion . We may conclude that if our dataset has symantically different classes and is correctly labelled (for.eg Imagenette dataset by fastai), then our normal loss function may perform well. But if it has symantically similar classes (for.eg Imagewoof dataset by fastai) or has incorrect labels, then you may want to use Label Smoothing.(Also, don&#39;t forget to randomly shuffle your data ;). . If you notice a mistake in this blog post please mention them in the comment section or email them to me at iamabhimanyu08@gmail.com, I&#39;ll make sure to correct them right away. .",
            "url": "https://abhimanyu08.github.io/blog/deep-learning/2020/05/17/final.html",
            "relUrl": "/deep-learning/2020/05/17/final.html",
            "date": " • May 17, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Bayes' Theorem",
            "content": "1. Notations . P(X) = Probality of event X. | P(X|Y) = Probability of X given that Y has occured. | . 2. Pre-requisites . You need to have knowledge of basic probability theory. If you are comfortable in calculating probabilities of discrete events and comfortable with the sum rule and product rule then you&#39;re good to go (If you&#39;re not, don&#39;t worry, I&#39;ve tried to give a terse explaination using an example below). Try out this question. | . Que - Bag A has 5 red and 3 blue balls, Bag B has 6 red and 4 blue balls. The probability that a person chooses Bag A is 0.3 and he&#39;ll choose Bag B with probability 0.7. What is the probability of a person selecting a blue ball from bag B? What is the total probability of him coming out with a red ball? . Ans - . Probability of choosing blue ball from B = (He chooses B) and (and then he selected blue ball from B) i.e ${P(B) times P(blue mid B)}$. If your answer is 0.28, then you&#39;re comfortable with the product rule. . | Probability of him coming out with a red ball = (He chooses red ball from A) or (he chooses red ball from B) i.e ${P(A) times P(red mid A)+ P(B) times P(red mid B)}$. If your answer comes out to be 0.6075, then you&#39;re comfortable with the sum rule. . | . Intuitively, sum rule comes in play when there is a choice between mutually exclusive events (these events are generally seperated by a or between them) for eg. The event of (either choosing A and then a red ball) or (B and then a red ball) (as shown in second bullet point above). Product rule comes into play when two events occur simultaneously for eg. choosing bag B and then selecting a blue ball from it (as shown in first bullet point above). . 3. Formula and basic jargon . Let me state the formula for Bayes&#39; theorem quickly. Subsequently, I&#39;ll explain every term of the formula in detail using an example and introduce some basic jargon along the way. . ${ displaystyle large P(A mid B)={ frac {P(B mid A)P(A)}{P(B)}}}$ (3.0) . Let&#39;s try to understand this formula by investigating a theft incidence. . There are two secret vaults A and B. Vault A contains 6 gold bars and 2 diamonds. Vault B contains 2 gold bars and 8 diamonds. Vault A is easier to break open and therefore if a thief encounters both vaults he&#39;s more likely to break into A than in B. Let&#39;s say a thief will break in A with probability 0.7 and in B with probability 0.3. Unfortunately, despite all the security, a theft occurs. Due to security alarms, thief had to rush and could only take whichever piece of ornament he could get his hands on (In other words, he didn&#39;t have the time to think that diamond is more precious so I should pick diamond instead of gold). After the heroic efforts of the police, the thief is caught and he&#39;s found having a diamond. You are a detective but like Sherlock Holmes, unauthorised. You have a personal history with the thief and want to get some more lead into this case by knowing from which vault the theft occured. Sadly, being unauthorised, you&#39;re not allowed to enter the crime scene. Thus, you would have to guess the vault from which the thief has stolen the diamond? Let&#39;s say you hypothesise that he must have stolen from A because it was easier to break into. Subsequently, you go home and then sit down with a pen and paper to test the validity of your hypothesis using the Bayes&#39; theorem . Let us define some symbols first, . ${P(A)}$ = probability of breaking into A which is given as 0.7. | ${P(B)}$ = probability of breaking into B which is given as 0.3. | ${P(G)}$ = probability of stealing the gold bar. | ${P(D)}$ = probability of stealing the diamond. | . You have to compute probability of thief having broken into A given that diamond was found in his hand i.e ${P(A mid D)}$. Expanding this term using the formula 3.0 gives us: ${ displaystyle P(A mid D)={ frac {P(D mid A)P(A)}{P(D)}}}$ (3.1) . The sentence &quot;The thief was found having a diamond&quot; is called Evidence. Probability of finding the Evidence is written mathematically as ${P(D)}$ which is the denominator of our Eq.3.1. To solve the question, we begin by listing all the ways through which Evidence could have occured. There are two cases in which diamond could&#39;ve been stolen:- . He broke into A and then picked up a diamond i.e ${P(A) times P(D mid A)}$ or 2. He broke into B and then picked up a diamond i.e ${P(B) times P(D mid B)}$. | All these listed ways make up your ${P(D)}$ in Eq.3.1. Thus ${P(D)}$ written mathematically will be equal to -&gt; ${P(A) times P(D mid A) + P(B) times P(D mid B)}$ (3.2) . Now, You will write down your Prior belief in the correctness of your hypothesis if you had not seen the evidence. What are the odds that thief broke into A? The odds are simply ${P(A)}$ or 0.7. Thus, ${P(A)}$ is called Prior, because it reflects your prior belief about the correctness of your hypothesis before seeing the Evidence. Now, what are the chances that he stole the diamond given that he infiltrated A? In other words,what are the chances of seeing the Evidence given that your hypothesis is true? Mathematically speaking, ${P(D mid A)}$. This entity is called Likelihood. Likelihood can be described as answer to the question &quot;How likely is that evidence occurs if I assume my hypothesis is true?&quot; . Likelihood and Prior make up the numerator in (3.1). Thus your numerator is ${P(A) times P(D mid A)}$. (3.3) . Why is the entity (3.3) our numerator and why not anything else? If you notice carefully (3.3) is probability of our case no 1 (He broke into A and then picked up a diamond). Intuitively, we can think of it as following:- Out of all the cases that make up your evidence ${P(D)}$ you are only interested in the ones in which your hypothesis holds true. Thus only the case no 1 from above list of cases interests you and you put that in your numerator. And if you remember, that is basic probability; we calculate probability using the formula -&gt; cases that interest us/total no of cases, for eg. What are the odds of selecting a red card from a deck of cards -&gt; 26/52 or 0.5. . Thus,finally after putting all the pieces together we can calculate . ${ displaystyle P(A mid D) = { frac {P(A) times P(D mid A)}{P(D)}} = { displaystyle frac {Equation 3.3}{Equation 3.2}} = { frac {P(A) times P(D mid A)}{P(A) times P(D mid A) + P(B) times P(D mid B)}} = 0.42}$ . The chances that your hypothesis is true is 42%. Put in other words, the chances of your hypothesis being wrong are 58%. Hence,it is more likely that he broke into B and not A! Now, reflecting on the details of the theft, you suddenly realise that &quot;Ah! That makes sense. There are more diamonds in B than in A and the thief hurriedly picked up whatever ornament he could get his hands on and got diamond. Thus,if he broke into B he had more chances of picking a diamond blindly than doing so after breaking into A. So he must have stolen from B&quot; The calculated probability ${P(A mid D)}$ is called the Posterior. This probability is an updated version of our Prior based on new evidence. Now that we found the evidence that thief had a diamond, we believe that he broke into A with probability 0.42 (posterior) instead of 0.7 (prior). In other words your belief in your hypothesis went down in the light of new evidence. This is the main motive of Bayes&#39; theorem, It helps us update our Prior beliefs continuously by collecting new Evidence. . Quick summary and technique to solve problems involving Bayes&#39; Theorem : . Find out what is given in the problem. The given part serves as Evidence which aids us in assessing our Hypothesis. . | List out all the ways in which Evidence could have occured, calculate the probabilities of those ways using product rule and sum rule and write them as denominator. . | Pick out the way amongst the list of ways in which your Hypothesis holds true and put the probability of that in the numerator. . | . 4. Interesting Study Demonstrating The Counter-Intuitiveness Of The Bayes&#39; Theorem . (This part of blog is inspired from a great video by 3 Blue 1 Brown). . Let me ask you an interesting question. Steve is very shy and withdrawn, invariably helpful but with little interest in people or in the world of reality. A meek and tidy soul, he has a need for order and structure, and a passion for detail.&quot; Having read this sentence what do you think is the profession of Steve, a librarian or a farmer ? . (This quesion was asked by Nobel Laureate Daniel Kahneman and Amos Tversky in the studies which they conducted that showed that humans are intuitively bad staticians (even those who had PhDs in the field of statistics) and sometimes overestimate the correctness of their prior beliefs. Daniel Kahneman has written about these studies in his book &quot;Thinking ,fast and slow&quot;.) . Most people would guess that Steve is a librarian because he fits in the stereotypical image of one. Let&#39;s look at this problem with a Bayesian perspective. Let&#39;s say that the sentence written in bold above is our evidence. Now we hypothesise that Steve is a librarian. Let&#39;s calculate the validity of our hypothesis. . Steve is a random person taken from a representative sample. . Let&#39;s say the probability of observing the above traits in a random person are ${P(E)}$. | Let the probability of a random person being a farmer be ${P(F)}$. | Let the probability of a random person being a librarian be ${P(L)}$. | . We would have to consider following questions to calculate the probability of our hypothesis given the evidence : . Out of 100 librarians how many do you think fit the description given above in bold typeface? We are allowed to incorporate our stereotypes in estimating the answer to this question. Let&#39;s say 85 out of 100 librarians fit the evidence. Mathematically speaking, given that a person is a librarian, the probability of him fiiting the above evidence (he is shy and a &quot;meek and tidy soul&quot;) is ${P(E mid L)}$ = 0.85 . | Out of 100 farmers how many do you think fit the description given above in bold typeface? Let&#39;s say 30 farmers fit the evidence (beacuse we all stereotypically think that farmers are less likely to be shy or a &quot;meek and tidy soul&quot;). Mathematically speaking, given that a person is a farmer, the probability of him fiting the above evidence is ${P(E mid F)}$ = 0.3 . | We also need to take into account some statistical facts to decide our prior beliefs. At the time of conduction of this study, there were 20 farmers for every 1 librarian in america. Thus, out of 210, 10 people were librarian and 200 are farmers.Therefore, probability of a random person being a farmer i.e ${P(F)}$ = 0.95 and probability of a random person being a librarian i.e ${P(L)}$ is 0.05 (assuming our representative sample has only farmers and librarians). . Listing all the ways in which the evidence can occur: . The person selected at random is a librarian and he is a &quot;meek and tidy soul&quot; (${P(L) times P(E mid L)}$) or 2. The person selected at random is a farmer and he is a &quot;meek and tidy soul&quot; (${P(F) times P(E mid F)}$). | Writing this mathematically -&gt; ${P(E) = P(L) times P(E mid L) + P(F) times P(E mid F)}$ . The case which interests us is case 1. Thus, ${ displaystyle P(L mid E) = frac{P(L) times P(E mid L)}{P(L) times P(E mid L) + P(F) times P(E mid F)}}$ . After doing the above calculation we find out that probability of Steve being a librarian is a mere 13 %. In other words, if you assemble 100 meek and tidy souls like Steve only 13 of them would turn out to be librarians. This seems surprising and counter-intuitive because we incorporated our stereotypes in our calculations (by saying that 85 out of 100 librarians fit the evidence), yet the final calculations conclude that our hypothesis (which complied with our stereotypes) was wrong. . An intutive way of thinking about this is as following: . There are way more farmers in general population than librarians, therefore there are way more &quot;meek and tidy souls&quot; ploughing the fields (77 out of 100 as per our calculations) than those who are meticulously keeping the books in the library. Take a sample of 210 people for example out of which 10 are librarians and 200 are farmers. According to our stereotypical estimates 85% of 10 librarians or ~9 librarians are shy, while only 30% of 200 lirarians or ~60 farmers are shy.Hence,out of 210 people 69 people are shy and tidy souls, majority of which are farmers. Thus, if we randomly picked a guy named Steve and he comes out as shy, he probably belongs to the group of 60 farmers. . 5. Bayes&#39; Theorem As A Way Of Updating Our Priors And Belief Systems. . (This part of blog is inspired from this great video by Veritasium) . Suppose, you go to a doctor and he tells you that results of your test for a disease are unfortunately positive. It is known that 0.1% of the population might have the disease. You know that the tests you took give correct results 99% of the time. Thus, you may be disheartened because such an accurate test has declared you of being sick from a rare disease. Intuitively, you would think that there is a 99% chance of you having this disease. But, let&#39;s look at this from a bayesian perspective. . Evidence -&gt; The test shows positive. ${P(E)}$ = 0.99 | Hypothesis -&gt; You have the disease given the evidence. ${P(D mid E)}$ | Prior belief before seeing the evidence -&gt; Probaility of you having the disease before you went for tests. ${P(D)}$ = 0.001 (because 0.1% of the population has it and you&#39;re part of the population) | . Ways In Which Evidence Can Be Observed (Test result Can Come Out As Positive): . You have the disease and test comes as positive (${P(D) times P(E mid D)}$). or 2. You don&#39;t have the disease and test shows positive (incorrectly) (${P( neg D).P(E mid neg D)}$). | Mathematically -&gt; ${P(D) = P(D) times P(E mid D) + P( neg D).P(E mid neg D)}$ . We are interested in case 1. . Thus, probability of you having the disease given positive test results ${P(D mid E)}$ = ${ displaystyle frac {P(D) times P(E mid D)}{P(D) times P(E mid D) + P( neg D) times P(E mid neg D)}}$. . After calculations, the probability of you having the disease comes out to be a mere 9%, which again seems counter-intuitive. Even after being declared positive by a pretty accurate test you are probably healthy and test is False! . This counter-intuitive result stems from the fact that probability of our hypothesis given the evidence is directly proportional to our prior i.e probability of our hypothesis being correct without the evidence (${P(D)}$ in above calculation). In this particular example, the probability of us having the disease without having the test results in our hand was so low (0.001) that even the new strong evidence couldn&#39;t vote in favour of our hypothesis that we have the disease. . Think of just 1000 people which also includes you. According to given data, 1 out of these 1000 is sick from the disease. Let&#39;s say that he goes for the test and is correctly identified as positive. The other 999 also go for tests. The test will falsely identify 1% of 999 healthy people, i.e 10 healthy people are shown positive. So now, there are 11 people in entire population with positive test results and you are one of them. Out of these 11 positive test results only 1 is correct. That&#39;s why having a positive result in first trial is not as bad as you might think! . But What If You Took A Second Test And It Comes As Positive . Suppose just to be sure, you go through tests from a different lab and the result again comes out as positive (assuming that this lab also gives correct results 99% of the times). Now, what are the chances that you have the disease. Let&#39;s agai hypothesise that you have the disease and test the validity of out hypothesis. Everything remains the same in terms of data except the prior. The basic definition of the prior is &quot;Probability that your hypothesis is true before collecting the evidence&quot;. Thus, in this case the prior is probability of you having the disease without having seen the results from second test. Therefore, prior should be 9% or 0.09 for the second case (Posterior from the first test). Even though the earlier test was likely to be false, it served us by updating our prior from 0.001 to 0.09 by providing us with a strong evidence. . The probability of having the disease given that second test result is also positive = ${ displaystyle frac {0.99 times 0.09}{0.99 times 0.09 + 0.01 times 0.91}}$ = ${91 %}$. . Thus, now you have 91% chances of being sick and intuitively this makes sense because the chances of two such accurate tests showing false results are pretty low. . You had a hypothesis that you are sick with 0.1 % odds. Then, you collected a evidence by going through a test and that evidence updated your belief in your hypothesis to 9%. Subsequently, you went out to collect another evidence by going through another test. That test further updated your belief in hypothesis to 91%. . This case shows that Bayes&#39; theorem serves us by updating our priors with help of new evidences. The posteriors serve as priors for the next time any evidence is collected. This process iteratively helps in scientifically solidifying or falsifying our hypotheses by regularly collecting new evidences and updating our Priors subsequently. . If you notice a mistake in this blog post please mention them in the comment section or email them to me at iamabhimanyu08@gmail.com, I&#39;ll make sure to correct them right away. .",
            "url": "https://abhimanyu08.github.io/blog/probability-theory/2020/03/23/final.html",
            "relUrl": "/probability-theory/2020/03/23/final.html",
            "date": " • Mar 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "",
          "content": ". layout: page title: About Me permalink: /about/ – . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://abhimanyu08.github.io/blog/_pages/about.html",
          "relUrl": "/_pages/about.html",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}