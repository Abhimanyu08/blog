{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors\n",
    "> Discussion of methods to calculate word vectors \n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [deep-learning, mathematics]\n",
    "- image: images/saddle.JPG "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's Imagine that we are the first research scientists ever who are attempting to design some algorithms which will make our computer able to make some sense of human language (We'll select English as the human language for sake of this blog). We start by first finding tasks which our computer should perform with certain level of competence if it finally gets the abilities we want to bless it with. Trying to solve the problem of analogies seems like a reasonable task for this purpose. If we ask our computer king is to queen as man is to ___ ? Hopefully, our computer should reply \"woman\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the task at hand closely, we realize that we can't just type in our computer \"King is to queen as man is to what?\" and expect it to answer. Our computer isn't a human which can understand English. The written language is a symbolic representation of our thoughts. We encode our thoughts in symbols so that we can convey those to other human. People who are unable to speak encode their thoughts in form of sign languages. There is one problem with process of conveying thoughts to other person by encoding them in symbols. The other person should know how to decode those symbols to understand them. You are able to decode the symbolic representation of English which you are so effortlessly doing right now while reading this blog. But suppose this blog was written in a language you don't know. Then this whole blog will just be a collection of meaningless symbols to you. Therefore, we have to encode our thoughts or messages by the rules which the receiver knows how to decode. In our case, the receiver is a computer, therefore, we have to convey our query to the computer in form which it can understand. We can design a function `find_analogy` which takes in three arguments `arg1`, `arg2` and `arg3`. It does some mathematical computations on these three arguments and returns a word such that the word is answer to question `arg1` is to `arg2` as `arg3` is to what. If we want answer to our question \"King is to queen as man is to what?\" we need to run the line of code `find_analogy(king, queen, man)`. But our computer doesn't know what the word \"king\",\"queen\" and \"man\" mean. We have to encode the words in a form the computer understands. Well, we know that a computer can make sense of mathematical entities like scalars and vectors. Why don't we encode every word in dictionary so that each is represented by a unique scalar/vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simplest solution and represent our words with scalars. Let each word be represented by it's position in the standard English dictionary. So the word 'aardvark' is represented by scalar 1 and word \"zebra\" is represented by scalar 273000 (approximate number of words in Oxford dictionary). We can immediately sense something wrong about this representation. What would adding or subtracting representations of two words even mean? The analogy task by which we intend to challenge understanding of our computer requires having a sense of meaning. It should be able to reason that king-queen relationship is same as man-woman since we remove abstract concept of masculinity from king and man and add another abstract concept of femininity to arrive at queen and woman respectively. If we trick our computer by asking \"king is to man as queen is to what?\" then it should be able to do some computation which resembles subtracting abstract concept of royalty from king and queen to arrive at man and woman respectively. And there's no way a computer can capture such abstract concept if we just encode each word by it's position in the dictionary. Thus, with a little bit of thought we come to conclusion that representing words with vectors makes more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, digressing a little bit, let us ponder on the question that given two vectors how do we know that if those two vectors are close to each other. We define closeness in terms of smaller angle between the two vectors. If those two vectors point in opposite directions then they are as far apart from each other as they can be. We can quantify this notion by calculating the cosine of acute angle between those two vector. Let ${a}$ and ${b}$ be two vectors and ${\\theta}$ be the acute angle between them. Then, ${\\cos\\theta = \\large\\frac{a.b}{\\Vert a\\Vert \\Vert b \\Vert}}$. The closer the ${\\cos\\theta}$ is to 1 the closer the vectors are to each other in terms of direction. If it's 0 then this means vectors are perpendicular to each other and if it's closer to -1 then this means vectors point in opposite directions. How does this relate to our problem in hand?  Well, we can say that we want that the notion of simalirity between two words to be captured by an operation as simple as dot-product of the vectors of these two words. In other words, we want vectors of similar words to point in somewhat same direction. We will design vectors for every word such that they are unit vectors which makes cosine similarity the same as computing the dot product. This way we can design a function called 'is_similar' which takes in vectors for two words and will compute their dot product. If the dot product is close to 1 then the function will return True , otherwse it'll return False. So, we hope that we can design word vectors in such a way that synonymous words will have almost the same vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so, we arrive at our next problem. **How do we find good vector representations of words?** Let's answer this questions by trying out different possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let |V| be the length of our vocabulary/dictionary. Let's represent each word by a |V|- dimensional vector which has 1 at index which is equal to position of word in dictionary and 0 everywhere else. Therefore, representation for word 'aardvark' is ${\\left[\\begin{array}{ccc}1\\\\0\\\\0\\\\.\\\\.\\\\.\\\\0\\end{array}\\right]}$ and 'Zebra' will be represented by ${\\left[\\begin{array}{ccc}0\\\\0\\\\0\\\\.\\\\.\\\\.\\\\1\\end{array}\\right]}$. We'll call this encoding the 'one-hot' representation of our words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, digressing a little bit, let us ponder on the question that given two vectors how do we know that if those two vectors are point in somewhat same directions? We can quantify this notion by calculating the cosine of angle between those two vectors. Let ${a}$ and ${b}$ be two vectors and ${\\theta}$ be the acute angle between them. Then, ${\\cos\\theta = \\large\\frac{a.b}{\\Vert a\\Vert \\Vert b \\Vert}}$. If ${\\cos\\theta = 1}$ then vectors point in the same direction. If it's $0$ then the vectors are perpendicular to each other and if it's $-1$ then the vectors point in opposite directions. \n",
    "\n",
    ">Note: We'll restrict the word vectors to be unit vectors. Then, calculating cosine similarity is same as taking dot-product.\n",
    "\n",
    "How does this relate to our problem in hand? Well, we can say that we want that the notion of similarity between two words to be captured by an operation as simple as dot-product of the vectors of these two words. In other words, we want vectors of similar meaning to point in somewhat same direction. This way we can design a function called is_similar which takes in vectors for two words and will compute their dot product. If the dot product is close to 1 then the function will say yes these two words are similar. So, we hope that we can design word vectors in such a way that vectors of synonymous words point in somewhat same direction (and antonyms in the opposite). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, knowing the English language we know that words 'motel' and 'hotel' are very similar which means we want the vectors of these two words to give a dot-product close to 1, according to concept we just discussed. But, if we take the dot product of vectors from our 'one-hot' representation of these words, it comes out as 0. In-fact dot-product of any two word vectors from our one-hot representation is 0. This fact is disheartening since this means that one-hot representation are not capable of capturing similarity between two words. Each word-vector is orthogonal to every other word-vector in this representation which is same as saying no two words are similar to each other nor are they oppoisite which isn't the case in real world. So, we part our ways with the one-hot representation and search for other alternatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-based Methods:\n",
    "\n",
    "We are looking for some concept about words which we can latch onto to form good vector representations of words. Through a fortunate stroke of serendipity we come across a quote by English linguist John Rupert Firth that is \"A word is characterzied by the company it keeps\". This quote implies that a word's meaning is decided by the context in which it appears. Take the word \"bank\" for example which is used in the following two sentences. \n",
    "1. \"I deposited the cheque in the bank.\"\n",
    "2. \"I was sitting alongside a river bank.\"\n",
    "\n",
    "In the two sentences above we can see that word \"bank\" takes on two completely different meaning based on the context in which it appears. Therefore, meaning of word bank depends on the words it is surrounded by (context words). Moreover, consider the following examples:\n",
    "\n",
    "1. The service at hotel we stayed in was very good\n",
    "2. The motel we are going to stay at is famous for the excellent service it provides.\n",
    "\n",
    "Since, motel and hotel are pretty much synonymous, they tend to be surrounded by similar words like 'stay', 'service' etc. This means that we can say that two words are similar if they have very similar context words.\n",
    "\n",
    "To make use of this concept, we first collect a large corpus of text. We then design a two dimensional matrix ${M}$ (let's call it count matrix) where ${M_{ij}}$ = Number of times the j'th word occured in the context of i'th word in the corpus of text. Each row of this matrix will represent a word in vocabulary. The first row will represent the word 'aardvark' and first cell of this row will be the number of times word 'aardvark' occured in context of word 'aardvark'. Similarly, the last cell of this row will represent the number of times word 'zebra' occured in context of word 'aardvark'. This means each row contains ${|V|}$ cells and there ${|V|}$ such rows. Therefore, ${M}$ is ${|V|\\times|V|}$ matrix. We need to specify one more detail to design this matrix which is what does it mean by occuring in context of something? To formally specify this we design a hyperparameter called context size and denote it by letter c. c is the number of words either side of a center word that classify as context words for.eg in the sentence \"I hope this year is better than the previous one.\" if we choose c = 2 and our center word is 'year' then words 'hope', 'this', 'is', 'better' are it's context words.\n",
    "\n",
    "To effeciently fill out the cells in the matrix we first initialise a ${|V|\\times|V|}$ matrix and initialise it with all 0's. Then, we take the first word in the large corpus we collected and collect it's context words. Then we increment the count in respective cells. We do this step for each word in the corpus.\n",
    "\n",
    "We can see intutively that synonymous words will have very similar rows because in a large corpus of text they will have similar context words. We can take the row for any word, normalise it (to turn it into a unit vector) and declare that row as the vector representation of that word. Then, taking the dot-product of vectors of similar words will result in a number close to 1.\n",
    "\n",
    "This marks our first breakthrough. We have finally devised a representation of words which our computer can understand and then tell us which words are similar by doing an operation as simple as a dot-product. But it is only when we put this algorithm to practice in real-world do we notice that it has numerous shortcomings some of which are:\n",
    "\n",
    "1. Every word is represented by a ${|V|}$ dimensional vector. ${|V|}$ can be of the order of millions because it is the lenght of our vocabulary. Storing vector for every word takes a toll on memory of our computer.\n",
    "\n",
    "2. ${M}$ is a huge ${|V|\\times |V|}$ matrix which is sparse. This means that most of the entries of our matrix are 0. This is because most of the words don't occur in context of some particular words for eg words 'heat' and 'snow' probably never occur in context of each other.\n",
    "\n",
    "3. Some of the words occur numerous times in context of other words which isn't very informative for.eg words like 'the' and 'is' occur in contexts of almost every word many times. This leads to drastic imbalance in word-frequency.\n",
    "\n",
    "4. New words are constantly being added to vocabulary. Words like 'instagram' and 'google' may not be in vocabulary of ancient texts but they are so prevalent now that they need their own definitions in a standard dictionary. \n",
    "\n",
    "Let's try to tackle these shortcomings ony by one.\n",
    "\n",
    "We have the concept of **Singular Value Decompostition (SVD)** to our rescue for tackling the first shortcoming. Take a number, for instance 68. We can write 68 as 2*2*17. Writing down it's factors reveals a lot of facts about the number 68 like it is divisible by 2,4 and 17, it is multiple of a prime number etc. Similarly, factoring a matrix can reveal a lot of facts about it. SVD says that any matrix A can be factorised in two three simple matrices ${U, \\Sigma}$ and ${V}$  as ${U\\Sigma V^{T}}$ where ${U}$ and ${V}$ are orthogonal matrices and ${\\Sigma}$ is a diagonal matrix.  \n",
    "\n",
    "Let's do this with our count matrix ${M}$. ${M = U\\Sigma V^{T}}$. Here, each of ${U, \\Sigma}$ and ${V}$ are ${|V|\\times |V|}$ matrix. Then we take only the first k columns of ${U}$ which results in ${|V|\\times k}$ matrix. We declare the rows of this matrix as vector representations of our words. This now solves our first problem because each word is now a ${k}$ dimensional vector where ${k}$ can be chosen by us based on some threshold.\n",
    "\n",
    "But the method of SVD which we used to circumvent one of our problems has some shortcomings in itself. SVD is not a trivial operation to perform. It does not scale well for large matrices because the amount of time it takes to perform SVD increases in quadratic fashion with respect to size of matrix. The matrix on which we want to perform SVD is already a huge matrix which will only get larger as new words are added in the vocabulary. These shortcomings are severe enough to compel us to look for other ways to find good vector representation of our words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration-based methods:\n",
    "\n",
    "Although our previous method didn't work out in the end, we found that guiding principle of \"A word is characterzied by the company it keeps\" is pretty good. We would want to continue on this train of thought to devise other methods of finding word vectors. We can notice one fact that context words are pretty good predictors of a center word. That's what we have been doing since childhood when we are solving 'Fill in the blanks\" type questions in our exams. Try to complete the follwing sentence: \"A ___ was seen flying in the sky after taking off from airport.\" If you just read till the word \"sky\", you must have seen the word \"flying\" and there would be many possibilities in your mind for the word that fills in the blank for eg 'bird', 'airplane', 'birds', 'helicopter' etc. but since you have seen the word \"A\" before the blank you immediately rule out the possibility of \"birds\" being the appropriate answer. Then, if you read the whole sentence and see the word \"airport\" you immediately lock in \"airplane\" as the most likely answer. \n",
    "\n",
    "This shows that context words are very good **predictors** of center words. Given a sentence with one word missing we can reasonably predict that word by looking at the words that surround the blank. We can use this concept to devise new word vectors. Take the large corpus of text we collected previously. Take the first word in the corpus and treat it as a blank. Try to **predict** this word by it's context words. If context words are able to **predict** the blank word correctly then everything is fine, but if they are not then there must be something wrong with the vectors of those words. Try to **modify** those vectors so that next time they don't commit the same mistake and correctly fill in the blank. Do this for every word in the corpus and hopefully we have corrected all the bad word vectors. If not, we can repeat this procedure multiple times till we are satisfied with the quality of our word vectors.\n",
    "\n",
    "I've highlighted the words **predict** and **modify** because it is not clear how do we these operations. How do we predict the blank word from vectors of context word ?\n",
    "\n",
    "To answer this let's change our notion of dot-product. Earlier we considered dot-product of vectors of two words to be a measure of similarity of those two words. Now, let's see the dot product of two word vectors as likelihood of them existing in the vicinity of each other. In other words, if dot-product of two word vectors is close to 1 then that means they are very likely to occur nearby in the corpus. For instance, take word vector of \"deep\" and take it's dot product with vector of every word in the vocabulary and collect the results in a list. This will give us a list of length ${|V|}$. We can see this list as a \"score\" for compability of each word in vocabulary with word \"deep\". We hope that score for word \"learning\" is close to 1 (because if we see the word \"deep\" there's high chances that word \"learning\" is nearby in a corpus) and score for word like \"shallow\" is close to -1. Wouldn't it be great if we could turn this list of scores in a list of probabilies depeding on the score. This list of probabilities would then tell the probability of each word in vocabulary occuring in context of word \"deep\". Since all the scores are between -1 and 1 let's exponentiate them to turn them into positive numbers (because ${e^{x}}$ is always positive and if ${x \\lt y \\implies e^{x} \\lt e^{y}}$ which will make sure large score turn into large positive numbers). Now the list is a collection of positive numbers with no upper bound. To squeeze these numbers between 0 and 1 let's divide each number by the sum of all the numbers in the list. This way all the numbers sum to 1 which makes them a valid probability distribution. The sequence of operations we did to convert scored to probabilities is called **Softmax** and takes a single line of code to implement. \n",
    "\n",
    "Let's write down what we did in mathematical notations.\n",
    "\n",
    "Let ${u}$ denote the word vector of word \"deep\" and ${v_{1}, v_{2}, ..., v_{|V|}}$ be the word vectors of every word in vocabulary.\n",
    "\n",
    "1. Take the dot product of each ${v_{i}}$ with ${u}$ and collect in a list called ${scores_{deep}}$. \n",
    "\n",
    "   ${scores_{deep}}$ = ${\\left[u.v_{1},\\space u.v_{2}, ..., \\space u.v_{|V|}\\right]}.$\n",
    "   \n",
    "\n",
    "2. Exponentiate each of the entry in ${scores_{deep}}$\n",
    "\n",
    "   ${scores_{deep}}$ = ${\\left[exp(u.v_{1}),\\space exp(u.v_{2}), ..., \\space exp(u.v_{|V|})\\right]}.$\n",
    "\n",
    "   Store sum of all the entries of ${scores_{deep}}$ in a variable 'Sum'\n",
    "\n",
    "   Sum = ${exp(u.v_{1}) + exp(u.v_{2}) + ... + exp(u.v_{|V|}) = \\large\\Sigma_{i=1}^{|V|} exp(u.v_{i})}$\n",
    "\n",
    "\n",
    "3. Divide each entry by Sum and rename the list to ${probabilities_{deep}}$.\n",
    "\n",
    "    ${probabilities_{deep}}$ = ${\\left[\\large\\frac{exp(u.v_{1})}{Sum},\\space \\frac{exp(u.v_{2})}{Sum}, ..., \\space \\frac{exp(u.v_{1})}{Sum}\\right]}.$\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sentence with one word missing we can reasonably predict that word by looking at the words that surround the blank. We can use this concept to devise new word vectors. Take the large corpus of text we collected previously. Take the first word in the corpus and treat it as a blank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
