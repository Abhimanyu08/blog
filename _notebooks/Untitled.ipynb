{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors\n",
    "> Discussion of methods to calculate word vectors \n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [deep-learning, mathematics]\n",
    "- image: images/saddle.JPG "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's say we are the first research scientists ever who have sat down to solve the problem of Natural Language Processing. Although the problem looks daunting at first, we've got to start somewhere. Let's say we first start by trying to solve the problem of analogies i.e If we ask our computer king is to queen as man is to ___ ? Hopefully, our computer should reply \"woman\"? This task may seem like daunting at first but we would we able to perform it by the end of this blog. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the task at hand closely, we realise that we can't just stand in front of our computer and ask it \"King is to queen as man is to what?\". Our computer isn't a human which can understand the natural language. The language which we speak is a symbolic representation of our thoughts. We encode our thoughts in a symbolic representation (let's say english) so that we can convey those thoughts to other human. That human then decodes that symbolic representation and knows about our thoughts. Same thing happens when we convey our message in written form. \n",
    "\n",
    "Suppose we saw a flock of birds in the sky and excitedly remark \"Hey! Look at those birds in the sky\". These words will fall on our friend's ears and he will immediately look up in the sky. But what if we spoke these words to a stranger who doesn't know English but only Hindi ?  These words will fall on his ears and he wouldn't be able to understand it because his brain simply can't decode those particular vibrations in air that our speaking induced. If that stranger has got a translator then that translator will first decode our words, which will lead to an image of flock of birds in his brain, then he will encode this image by rules of Hindi and convey the message to stranger. The stranger's brain knows the rules to decode the symbolic representations conveyed in Hindi\n",
    "\n",
    "So, from this instance we know that we have to encode our thoughts or messages by the rules which the receiver knows how to decode. Therefore we have to convey our query to the computer in form which it can understand. Suppose the computer has a function find_analogy which takes in three arguments arg1, arg2 and arg3. It does some **mathematical computations** on these three arguments and returns a word such that the word is answer to question arg1 is to arg2 as arg3 is to what. Therefore, if we want answer to our question \"King is to queen as man is to what?\" we need to run the line of code find_analogy(king, queen, man). But how do we represent words 'king', 'queen' and 'man' ? Thus, we arrive at our first fundamental problem. **How do we represent words?** Do we pass them as strings. We can't, because as highlighted earlier the function performs some mathematical computations on it's arguments and there's no way we can do mathematical operations on string.\n",
    "\n",
    "So now we arrive at our first conclusion that in order to let our computer understand words we need to represent them by mathematical entities like scalars or vectors. Let's start with simplest solution and represent our words with scalars. Let each word be represented by it's position in the standard english dictionary. So the word 'aardvark' is represented by scalar 1 and word \"zebra\" is represented by sacalar 273000 (approximate number of words in Oxford dictionary). We can immediately sense something wrong about this representation. What would adding or subtracting representations of two words even mean? In the analogy task by which we intend to challenge understanding of our computer requires having a sense of meaning. It should be able to reason that king-queen relationship is same as man-woman since we remove abstract concept of masculinity from king and man and add another abstract concept of femininity to arrive at queen and woman respectively. If we trick our computer by asking \"king is to man as queen is to what?\" then it should be able to do some computation which resemble subtrating abstract concept of royalty from king and queen to arrive at man and woman respectively. And there's no way a computer can capture such abstarct concept if we just encode each word by it's position in the dictionary. Thus, with a little bit of thought we come to conclusion that represeting words with vectors makes more sense. \n",
    "\n",
    "And so, we arrive at our next problem. **How do we find good vector representations of words?** Another problem is **what should be the dimensionality of space in which those word vectors lie?** A single dimensional space would be same as representing each word with a scalar and we have already parted our ways from that train of thought. Would a 2-dimensional space suffice? Let's answer these questions by trying out different possibilities.\n",
    "\n",
    "Let |V| be the length of our vocabulary/dictionary. Let's represent each word by a |V|- dimensional vector which has 1 at index which is equal to position of word in dictionary and 0 everywhere else. Therefore, representation for word 'aardvark' is ${\\left[\n",
    "         \\begin{array}{ccc}\n",
    "         1\\\\\n",
    "         0\\\\\n",
    "         .\\\\\n",
    "         .\\\\\n",
    "         .\\\\\n",
    "         0\n",
    "         \\end{array}\n",
    "    \\right]}$ and 'Zebra' will be represented by ${\\left[\n",
    "         \\begin{array}{ccc}\n",
    "         0\\\\\n",
    "         0\\\\\n",
    "         .\\\\\n",
    "         .\\\\\n",
    "         .\\\\\n",
    "         1\n",
    "         \\end{array}\n",
    "    \\right]}$. We'll call this encoding the 'one-hot' representaion of our words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, digressing a little bit, let us ponder on the question that given two vectors how do we know that if those two vectors are close to each other. We define closeness in terms of smaller angle between the two vectors. If those two vectors point in opposite directions then they are as far apart from each other as they can be. We can quantify this notion by calculating the cosine of acute angle between those two vector. Let ${a}$ and ${b}$ be two vectors and ${\\theta}$ be the acute angle between them. Then, ${\\cos\\theta = \\large\\frac{a.b}{\\Vert a\\Vert \\Vert b \\Vert}}$. The closer the ${\\cos\\theta}$ is to 1 the closer the vectors are to each other in terms of direction. If it's 0 then this means vectors are perpendicular to each other and if it's closer to -1 then this means vectors point in opposite directions. How does this relate to our problem in hand?  Well, we can say that we want that the notion of simalirity between two words to be captured by an operation as simple as dot-product of the vectors of these two words. In other words, we want vectors of similar words to point in somewhat same direction. We will design vectors for every word such that they are unit vectors which makes cosine similarity the same as computing the dot product. This way we can design a function called 'is_similar' which takes in vectors for two words and will compute their dot product. If the dot product is close to 1 then the function will return True , otherwse it'll return False. So, we hope that we can design word vectors in such a way that synonymous words will have almost the same vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, knowing the English language we know that words 'motel' and 'hotel' are very similar which means we want the vectors of these two words to give a large dot-product. But if we take the dot product of vectors from our 'one-hot' representation of these words, it comes out as 0. In-fact dot-product of any two word vectors from our one-hot representation is 0. This fact is disheartening since this means that one-hot representation are not capable of capturing similarity between two words. Each word-vector is orthogonal to every other word-vector in this representation which is same as saying no two words are similar to each other nor are they oppoisite which just isn't the case in real world. So, we part our ways with the one-hot representation and search for other alternatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-based Methods:\n",
    "\n",
    "We are looking at some concept about words which we can latch onto to form good vector representations of words. Through a fortunate stroke of serendipity we come across a quote by English linguist John Rupert Firth that is \"A word is characterzied by the company it keeps\". This quote implies that a word's meaning is decided by the context in which it appears. Take the word \"bank\" for example which is used in the following two sentences. \n",
    "1. \"I deposited the cheque in the bank.\"\n",
    "2. \"I was sitting alongside a river bank.\"\n",
    "\n",
    "In the two sentences above we can see that word \"bank\" takes on two completely different meaning based on the context in which it appears. Therefore, meaning of word bank depends on the words it is surrounded by (context words). Moreover, consider the following examples:\n",
    "\n",
    "1. The service at hotel we stayed in was very good\n",
    "2. The motel we are going to stay at is famous for the excellent service it provides.\n",
    "\n",
    "Since, motel and hotel are pretty much synonymous, they tend to be surrounded by similar words like 'stay', 'service' etc. This means that we can say that two words are similar if they have very similar context words.\n",
    "\n",
    "To make use of this concept, we first collect a large corpus of text. We then design a two dimensional matrix ${M}$ (let's call it count matrix) where ${M_{ij}}$ = Number of times the j'th word occured in the context of i'th word in the corpus of text. Each row of this matrix will represent a word in vocabulary. The first row will represent the word 'aardvark' and first cell of this row will be the number of times word 'aardvark' occured in context of word 'aardvark'. Similarly, the last cell of this row will represent the number of times word 'zebra' occured in context of word 'aardvark'. This means each row contains ${|V|}$ cells and there ${|V|}$ such rows. Therefore, ${M}$ is ${|V|\\times|V|}$ matrix. We need to specify one more detail to design this matrix which is what does it mean by occuring in context of something? To formally specify this we design a hyperparameter called context size and denote it by letter c. c is the number of words either side of a center word that classify as context words for.eg in the sentence \"I hope this year is better than the previous one.\" if we choose c = 2 and our center word is 'year' then words 'hope', 'this', 'is', 'better' are it's context words.\n",
    "\n",
    "To effeciently fill out the cells in the matrix we first initialise a ${|V|\\times|V|}$ matrix and initialise it with all 0's. Then, we take the first word in the large corpus we collected and collect it's context words. Then we increment the count in respective cells. We do this step for each word in the corpus.\n",
    "\n",
    "We can see intutively that synonymous words will have very similar rows because in a large corpus of text they will have similar context words. We can take the row for any word, normalise it (to turn it into a unit vector) and declare that row as the vector representation of that word. Then, taking the dot-product of vectors of similar words will result in a number close to 1.\n",
    "\n",
    "This marks our first breakthrough. We have finally devised a representation of words which our computer can understand and then tell us which words are similar by doing an operation as simple as a dot-product. But it is only when we put this algorithm to practice in real-world do we notice that it has numerous shortcomings some of which are:\n",
    "\n",
    "1. Every word is represented by a ${|V|}$ dimensional vector. ${|V|}$ can be of the order of millions because it is the lenght of our vocabulary. Storing vector for every word takes a toll on memory of our computer.\n",
    "\n",
    "2. ${M}$ is a huge ${|V|\\times |V|}$ matrix which is sparse. This means that most of the entries of our matrix are 0. This is because most of the words don't occur in context of some particular words for eg words 'heat' and 'snow' probably never occur in context of each other.\n",
    "\n",
    "3. Some of the words occur numerous times in context of other words which isn't very informative for.eg words like 'the' and 'is' occur in contexts of almost every word many times. This leads to drastic imbalance in word-frequency.\n",
    "\n",
    "4. New words are constantly being added to vocabulary. Words like 'instagram' and 'google' may not be in vocabulary of ancient texts but they are so prevalent now that they need their own definitions in a standard dictionary. \n",
    "\n",
    "Let's try to tackle these shortcomings ony by one.\n",
    "\n",
    "We have the concept of **Singular Value Decompostition (SVD)** to our rescue for tackling the first shortcoming. Take a number, for instance 68. We can write 68 as 2*2*17. Writing down it's factors reveals a lot of facts about the number 68 like it is divisible by 2,4 and 17, it is multiple of a prime number etc. Similarly, factoring a matrix can reveal a lot of facts about it. SVD says that any matrix A can be factorised in two three simple matrices ${U, \\Sigma}$ and ${V}$  as ${U\\Sigma V^{T}}$ where ${U}$ and ${V}$ are orthogonal matrices and ${\\Sigma}$ is a diagonal matrix.  \n",
    "\n",
    "Let's do this with our count matrix ${M}$. ${M = U\\Sigma V^{T}}$. Here, each of ${U, \\Sigma}$ and ${V}$ are ${|V|\\times |V|}$ matrix. Then we take only the first k columns of ${U}$ which results in ${|V|\\times k}$ matrix. We declare the rows of this matrix as vector representations of our words. This now solves our first problem because each word is now a ${k}$ dimensional vector where ${k}$ can be chosen by us based on some threshold.\n",
    "\n",
    "But the method of SVD which we used to circumvent one of our problems has some shortcomings in itself. SVD is not a trivial operation to perform. It does not scale well for large matrices because the amount of time it takes to perform SVD increases in quadratic fashion with respect to size of matrix. The matrix on which we want to perform SVD is already a huge matrix which will only get larger as new words are added in the vocabulary. These shortcomings are severe enough to compel us to look for other ways to find good vector representation of our words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration-based methods:\n",
    "\n",
    "Although our previous method didn't work out in the end, we found that guiding principle of \"A word is characterzied by the company it keeps\" is pretty good. We would want to continue on this train of thought to devise other methods of finding word vectors. We can notice one fact that context words are pretty good predictors of a center word. That's what we have been doing since childhood when we are solving 'Fill in the blanks\" type questions in our exams. Try to complete the follwing sentence: \"A ___ was seen flying in the sky after taking off from airport.\" If you just read till the word \"sky\", you must have seen the word \"flying\" and there would be many possibilities in your mind for the word that fills in the blank for eg 'bird', 'airplane', 'birds', 'helicopter' etc. but since you have seen the word \"A\" before the blank you immediately rule out the possibility of \"birds\" being the appropriate answer. Then, if you read the whole sentence and see the word \"airport\" you immediately lock in \"airplane\" as the most likely answer. \n",
    "\n",
    "This shows that context words are very good **predictors** of center words. Given a sentence with one word missing we can reasonably predict that word by looking at the words that surround the blank. We can use this concept to devise new word vectors. Take the large corpus of text we collected previously. Take the first word in the corpus and treat it as a blank. Try to **predict** this word by it's context words. If context words are able to **predict** the blank word correctly then everything is fine, but if they are not then there must be something wrong with the vectors of those words. Try to **modify** those vectors so that next time they don't commit the same mistake and correctly fill in the blank. Do this for every word in the corpus and hopefully we have corrected all the bad word vectors. If not, we can repeat this procedure multiple times till we are satisfied with the quality of our word vectors.\n",
    "\n",
    "I've highlighted the words **predict** and **modify** because it is not clear how do we these operations. How do we predict the blank word from vectors of context word ?\n",
    "\n",
    "To answer this let's change our notion of dot-product. Earlier we considered dot-product of vectors of two words to be a measure of similarity of those two words. Now, let's see the dot product of two word vectors as likelihood of them existing in the vicinity of each other. In other words, if dot-product of two word vectors is close to 1 then that means they are very likely to occur nearby in the corpus. For instance, take word vector of \"deep\" and take it's dot product with vector of every word in the vocabulary and collect the results in a list. This will give us a list of length ${|V|}$. We can see this list as a \"score\" for compability of each word in vocabulary with word \"deep\". We hope that score for word \"learning\" is close to 1 (because if we see the word \"deep\" there's high chances that word \"learning\" is nearby in a corpus) and score for word like \"shallow\" is close to -1. Wouldn't it be great if we could turn this list of scores in a list of probabilies depeding on the score. This list of probabilities would then tell the probability of each word in vocabulary occuring in context of word \"deep\". Since all the scores are between -1 and 1 let's exponentiate them to turn them into positive numbers (because ${e^{x}}$ is always positive and if ${x \\lt y \\implies e^{x} \\lt e^{y}}$ which will make sure large score turn into large positive numbers). Now the list is a collection of positive numbers with no upper bound. To squeeze these numbers between 0 and 1 let's divide each number by the sum of all the numbers in the list. This way all the numbers sum to 1 which makes them a valid probability distribution. The sequence of operations we did to convert scored to probabilities is called **Softmax** and takes a single line of code to implement. \n",
    "\n",
    "Let's write down what we did in mathematical notations.\n",
    "\n",
    "Let ${u}$ denote the word vector of word \"deep\" and ${v_{1}, v_{2}, ..., v_{|V|}}$ be the word vectors of every word in vocabulary.\n",
    "\n",
    "1. Take the dot product of each ${v_{i}}$ with ${u}$ and collect in a list called ${scores_{deep}}$. \n",
    "\n",
    "   ${scores_{deep}}$ = ${\\left[u.v_{1},\\space u.v_{2}, ..., \\space u.v_{|V|}\\right]}.$\n",
    "   \n",
    "\n",
    "2. Exponentiate each of the entry in ${scores_{deep}}$\n",
    "\n",
    "   ${scores_{deep}}$ = ${\\left[exp(u.v_{1}),\\space exp(u.v_{2}), ..., \\space exp(u.v_{|V|})\\right]}.$\n",
    "\n",
    "   Store sum of all the entries of ${scores_{deep}}$ in a variable 'Sum'\n",
    "\n",
    "   Sum = ${exp(u.v_{1}) + exp(u.v_{2}) + ... + exp(u.v_{|V|}) = \\large\\Sigma_{i=1}^{|V|} exp(u.v_{i})}$\n",
    "\n",
    "\n",
    "3. Divide each entry by Sum and rename the list to ${probabilities_{deep}}$.\n",
    "\n",
    "    ${probabilities_{deep}}$ = ${\\left[\\large\\frac{exp(u.v_{1})}{Sum},\\space \\frac{exp(u.v_{2})}{Sum}, ..., \\space \\frac{exp(u.v_{1})}{Sum}\\right]}.$\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
