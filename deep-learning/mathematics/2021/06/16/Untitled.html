<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Why Squared Error Minimization = Maximum Likelihood Estimation | Abhimanyu</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Why Squared Error Minimization = Maximum Likelihood Estimation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An investigation into why minimizing squared error is the same as maximum likelihood estimation" />
<meta property="og:description" content="An investigation into why minimizing squared error is the same as maximum likelihood estimation" />
<link rel="canonical" href="https://abhimanyu08.github.io/blog/deep-learning/mathematics/2021/06/16/Untitled.html" />
<meta property="og:url" content="https://abhimanyu08.github.io/blog/deep-learning/mathematics/2021/06/16/Untitled.html" />
<meta property="og:site_name" content="Abhimanyu" />
<meta property="og:image" content="https://abhimanyu08.github.io/blog/images/normal-dist.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-16T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2021-06-16T00:00:00-05:00","dateModified":"2021-06-16T00:00:00-05:00","image":"https://abhimanyu08.github.io/blog/images/normal-dist.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://abhimanyu08.github.io/blog/deep-learning/mathematics/2021/06/16/Untitled.html"},"description":"An investigation into why minimizing squared error is the same as maximum likelihood estimation","@type":"BlogPosting","url":"https://abhimanyu08.github.io/blog/deep-learning/mathematics/2021/06/16/Untitled.html","headline":"Why Squared Error Minimization = Maximum Likelihood Estimation","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://abhimanyu08.github.io/blog/feed.xml" title="Abhimanyu" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Why Squared Error Minimization = Maximum Likelihood Estimation | Abhimanyu</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Why Squared Error Minimization = Maximum Likelihood Estimation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An investigation into why minimizing squared error is the same as maximum likelihood estimation" />
<meta property="og:description" content="An investigation into why minimizing squared error is the same as maximum likelihood estimation" />
<link rel="canonical" href="https://abhimanyu08.github.io/blog/deep-learning/mathematics/2021/06/16/Untitled.html" />
<meta property="og:url" content="https://abhimanyu08.github.io/blog/deep-learning/mathematics/2021/06/16/Untitled.html" />
<meta property="og:site_name" content="Abhimanyu" />
<meta property="og:image" content="https://abhimanyu08.github.io/blog/images/normal-dist.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-16T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2021-06-16T00:00:00-05:00","dateModified":"2021-06-16T00:00:00-05:00","image":"https://abhimanyu08.github.io/blog/images/normal-dist.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://abhimanyu08.github.io/blog/deep-learning/mathematics/2021/06/16/Untitled.html"},"description":"An investigation into why minimizing squared error is the same as maximum likelihood estimation","@type":"BlogPosting","url":"https://abhimanyu08.github.io/blog/deep-learning/mathematics/2021/06/16/Untitled.html","headline":"Why Squared Error Minimization = Maximum Likelihood Estimation","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://abhimanyu08.github.io/blog/feed.xml" title="Abhimanyu" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Abhimanyu</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Why Squared Error Minimization = Maximum Likelihood Estimation</h1><p class="page-description">An investigation into why minimizing squared error is the same as maximum likelihood estimation</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-06-16T00:00:00-05:00" itemprop="datePublished">
        Jun 16, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#deep-learning">deep-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#mathematics">mathematics</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Pre-requisites">Pre-requisites </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Interchange-between-vector-notation-and-algebraic-notation">Interchange between vector notation and algebraic notation </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Minimizing-the-squared-error-loss">Minimizing the squared error loss </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Maximum-Likelihood-Estimation">Maximum Likelihood Estimation </a></li>
<li class="toc-entry toc-h3"><a href="#Minimising-Squared-Error-${\rightarrow}$-Maximising-Likelihood">Minimising Squared Error ${\rightarrow}$ Maximising Likelihood </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/Untitled.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Pre-requisites">
<a class="anchor" href="#Pre-requisites" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pre-requisites<a class="anchor-link" href="#Pre-requisites"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Interchange-between-vector-notation-and-algebraic-notation">
<a class="anchor" href="#Interchange-between-vector-notation-and-algebraic-notation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Interchange between vector notation and algebraic notation<a class="anchor-link" href="#Interchange-between-vector-notation-and-algebraic-notation"> </a>
</h3>
<p>Before starting out, I would just like to familiarise the readers with some notational somersaults we might come across in the blog.</p>
<p>Let ${S = x_{1}^{2} + x_{2}^{2} +x_{3}^{2}+...+ x_{m}^{2} }$. Another way of writing this equation can be</p>
<p>${S = \Sigma_{i=1}^{m}\space x_{i}^{2}}$.</p>
<p>If we collect ${x_{1}, x_{2},...,{x_m}}$ in a single vector ${\small X}$, such that ${\small X = \left[
         \begin{array}{ccc}
         x_{1}\\
         x_{2}\\
         . \\
         . \\
         x_{m}
         \end{array}\right]}$, then ${\small S = X^{T}X}$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Minimizing-the-squared-error-loss">
<a class="anchor" href="#Minimizing-the-squared-error-loss" aria-hidden="true"><span class="octicon octicon-link"></span></a>Minimizing the squared error loss<a class="anchor-link" href="#Minimizing-the-squared-error-loss"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Minimizing the Squared Error is the technique any machine learning practitioner uses while tackling a Regression problem in which the target is a continuous variable. Let's say we have collected all our independent variables in a matrix ${X}$ of shape (m,n) where m is the number of training examples and each training example is represented by a n dimensional vector. All the dependent variables are represented by a vector ${Y}$ of length m (each training example has a real number as it's target). Our task is to find a vector ${\hat\beta}$ of length n such that ${X\hat\beta = Y}$. Now this task can simply be solved by saying ${\hat\beta = X^{-1}Y}$ but this is the correct answer only in the case when ${X}$ is a squared matrix and ${Y}$ lies in the column space of ${X}$ i.e m =n and Y is simply a weighted sum of columns of X. If that's not the case ${X\hat\beta}$ can never be equal to ${Y}$. All we can hope to do is minimize the distance between the two vectors ${X\hat\beta}$ and ${Y}$. The distance we choose to minimize is ${(Y-X\hat\beta)^{2}}$ which can also be written as ${(Y-X\hat\beta)^{T}(Y-X\hat\beta)}$. Let's denote this error by ${\epsilon}$. Now, if you are like me, you may wonder why don't we simply minimize ${Y - X\beta}$ or some other power of ${Y - X\beta}$. This is perfectly valid question to ask and we are going to explore this in this blog. For now, just remember that ${\hat\beta}$ is the value of ${\beta}$ that minimises the squared error; written mathematically as : ${\hat\beta = argmin_{\beta} \space(Y-X\beta)^{T}(Y-X\beta)}$. Solving this we get ${\hat\beta = (X^{T}X)^{-1}X^{T}Y}$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Maximum-Likelihood-Estimation">
<a class="anchor" href="#Maximum-Likelihood-Estimation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Maximum Likelihood Estimation<a class="anchor-link" href="#Maximum-Likelihood-Estimation"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another way of looking at the regression task is that we have m observations of ${(x,y)}$ where ${x}$ is a n dimensional vector and ${y}$ is it's corresponding target value. These m data points come from a true but unknown data distribution ${P(X,Y\space;\space\theta)}$ where ${\theta_{true}}$ is the parameter of the distribution. Let's say we want to predict how many matches will each of the player win out of his next 10 matches by just looking at the player's age and ATP ranking. Therefore, our independent variable is ${X = [\text Age, \text Ranking]}$ and dependent variable is ${Y =}$ number of matches won out of next 10 for eg. Nadal will be represented by vector ${(35,3)}$ and Federer by ${(39,8)}$. What do you think ${P([18,123], 10)}$ will be? In other words what is the probability that a player aged 18 and ranked 123 wins all 10 of his next matches? We can safely say that this a highly unlikely event since the player is inexperienced. In the same way, we would expect ${P([30,1], 8)}$ to be high since the player is ranked highest in the world. Now instead of guessing, we want to construct a probability distribution over random variables ${X}$ and ${Y}$. Once we have such a distribution we can simply plug in age and ranking of any player and calculate ${P([\text age,\text ranking], Y\space ; \space \theta_{true})}$ for each ${Y}$ in ${(0,10)}$ and report the one which gave the highest probability. Unfortunately, we can't know true distribution since for that we would have to collect data from every single active player which can be a lot. But, we can estimate the true distribution. Any distribution is entirely characterised by it's parameters. for eg a Gaussian distribution is charachterised by it's mean and variance and is denoted as ${N (x; \mu, \sigma^{2})}$. If we can estimate the parameters of a distribution, then can construct a good estimate of the distribution as a whole. Let's denote the estimate of parameters ${\theta_{true}}$ as ${\hat\theta}$.</p>
<p>To construct such estimate of a distribution we collect the age and ranking of m players and observe them for the next 10 matches and note down how many of those matches they won. Let each of these observations be represented by ${x^{i}, y^{i}}$ where each ${x^{i}}$ is itself a vector of length two containing the age and ranking of ${i^{th}}$ player and ${y^{i}}$ is the number of matches won by that player out of 10. Let's assume every observation is independent of any other observation. Given each ${x^{i}}$ our distribution should predict ${y^{i}}$ and this can only happen when ${P(x^{i}, y^{i}\space ;\space \hat\theta)}$ is maximum of among all other ${P(x^{i}, y\space ; \space \hat\theta)}$ where ${y \neq y_{i}}$. The quantity ${P(x^{i}, y^{i})}$ is known as likelihood of the observation  ${(x^{i}, y^{i})}$. Since we want likelihood of all the observations to be maximum, we might as well say that we we want the quantity ${P(x^{1}, y^{1})\cdot P(x^{2}, y^{2})\cdot P(x^{3}, y^{3}) ... P(x^{m}, y^{m})}$ to be maximum which in short can be written as ${\Pi_{i=1}^{m}P(x^{i}, y^{i}\space ; \space \theta)}$. Then ${\hat\theta}$ will be whatever value of ${\theta}$ which can achieve this feat.</p>
<p>${\therefore \hat\theta = argmax_{\theta} \space\Pi_{i=1}^{m} P(x^{i}, y^{i}\space ; \space \theta)}$.</p>
<p>This method of estimating ${\theta_{true}}$ is called maximum likelihood estimation and obtained ${\hat\theta}$ is called maximum likelihood estimate of ${\theta_{true}}$. Maximum likelihood estimates of true parameters have many desirable properties, most important of which is: As we increase the number of training examples, the probability of estimates being close to true parameter increases. Well, this is obvious for any type of estimate.  What's special about the maximum likelihood estimates is that they do so the fastest of any other type of estimate. In other words, if we are using maximum likelihood estimates to estimate our paramters and someone else is using other methods, then our performance on prediction will improve faster on collecting more training data.</p>
<p>On a side note we can si</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Minimising-Squared-Error-${\rightarrow}$-Maximising-Likelihood">
<a class="anchor" href="#Minimising-Squared-Error-%24%7B%5Crightarrow%7D%24-Maximising-Likelihood" aria-hidden="true"><span class="octicon octicon-link"></span></a>Minimising Squared Error ${\rightarrow}$ Maximising Likelihood<a class="anchor-link" href="#Minimising-Squared-Error-%24%7B%5Crightarrow%7D%24-Maximising-Likelihood"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's get back to minimising sqaured error and see through a sequence of logical steps how it is maximum likelihood estimation in disguise.</p>
<p>${\hat\beta = argmin_{\beta} \space(Y-X\beta)^{T}(Y-X\beta) = argmin_{\beta} \space(Y-X\beta)^{T}\space I\space(Y-X\beta)}$ where ${I}$ is the identity matrix.</p>
<p>Mutltiplying a function by a negative number turns it's minimas to maximas and vice versa. So let's multiply our function by ${\frac{-1}{2}}$</p>
<p>${\therefore \hat\beta = argmax_{\beta} \space\frac{-1}{2}(Y-X\beta)^{T}\space I\space(Y-X\beta)}$</p>
<p>Exponentiating the function and then taking it's log returns the function itself, because log and exp are inverse functions of each other.</p>
<p>${\therefore \hat\beta = argmax_{\beta} \space log\space exp\frac{-1}{2}(Y-X\beta)^{T}\space I\space(Y-X\beta)}$.</p>
<p>Subtracting a constant from a function doesn't change the location of it's maximas and minimas. So, let's subtract ${log \sqrt{2\pi^{k}}}$</p>
<p>${\therefore \hat\beta = argmax_{\beta} \space log\space exp\frac{-1}{2}(Y-X\beta)^{T}\space I\space(Y-X\beta) - log \sqrt{2\pi^{k}}}$.</p>
<p>Since ${log a - log b = log \frac{a}{b}}$</p>
<p>${\therefore \hat\beta = argmax_{\beta} \space log\space \frac{\normalsize exp\frac{-1}{2}(Y-X\beta)^{T}\space I\space(Y-X\beta)}{\normalsize\sqrt{2\pi^{k}}}}$.</p>
<p>Just to jog your memory, let me write the expression of probability density of a random variable ${W}$ which has a multivariate Normal distribution whose mean vector is ${\mu}$ and covariance matrix is ${I}$</p>
<p>${P_{normal}(W\space ; \mu, I) = \frac{\normalsize exp \frac{-1}{2}(W-\mu)^{T}\space I\space(W-\mu)}{\normalsize\sqrt{2\pi^{k}}}}$. This equation looks strikingly familiar to the equation of our morphed square minimization objective. That is indeed the case, but we have to be careful. We may be tempted to interpret ${ \normalsize\frac{ exp\frac{-1}{2}(Y-X\beta)^{T}\space I\space(Y-X\beta)}{\sqrt{2\pi^{k}}}}$ as ${P_{normal}(Y\space ; X\beta ,I)}$ but remember that the mean of a normal distribution is a constant quantity and in our interpretation it's a variable ${X\beta}$. But, the quantity ${X\beta}$ is constant for a given ${X}$. Therefore, our interpretation should be ${ \normalsize\frac{ exp\frac{-1}{2}(Y-X\beta)^{T}\space I\space(Y-X\beta)}{\sqrt{2\pi^{k}}} = \small P_{normal}(Y|X\space ; X\beta ,I)}$, which in words is read as probability of ${Y}$ given ${X}$. Now we can plug this in the equation for ${\hat\beta}$</p>
<p>${\therefore \hat\beta = argmax_{\beta}\space log P_{normal}(Y|X\space ; X\beta, I)}$.</p>
<p>This means that by minimising squared error we were indeed maximising the log-likelihood of variable ${Y|X}$. Also, we can see that by minimising squared error we were implicitly assuming that given a particular ${X}$, ${Y}$ is a multivariate gaussian random variable with Identity matrix as it's covariance matrix. Now you may be confused that given a particular ${X}$ why is ${Y}$ a vector in our representation? Shouldn't it be a fixed number? This is because in a large dataset there may be instances with same input value ${x}$ but different labels due to error in data collection or just by the virtue of randomness in the true data distribution.</p>
<p>Now we have turned the task of finding the argument that minimises squared error into the task of finding the argument that maximises the likelihood of multivariate gaussian distribution which is very easy if you remember what a Gaussian curve looks like. If not, given below is the picture of a graph of probablity density of a random variable with gaussian distribution which has scalar mean ${\mu}$ and scalar variance ${\sigma}$ i.e ${P_{normal}(X\space ; \space \mu, \sigma)}$</p>
<p><img src="/blog/images/copied_from_nb/my_icons/normal-dist.jpg" alt=""></p>
<p>In this picture, we can see that maximum of the distribution occurs at the mean of random variable also called expectation of random variable and denoted as ${E[X]}$. This means that ${P_{normal}(Y|X\space ; X\beta, I)}$ is maximised when ${X\beta}$ is mean of variable ${Y|X}$.</p>
<p>${\therefore X\hat\beta = E[Y|X]}$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's understand this by a small example. Let our training dataset be ${X = \left[
         \begin{array}{ccc}
         1 &amp; 2\\
         1 &amp; 3\\
         1 &amp; 2
         \end{array}
\right]}$ and coresponding targets be ${Y = \left[
         \begin{array}{ccc}
         10\\
         11\\
         12
         \end{array}
\right]}$. As you can see same input ${[1,2]}$ has two different labels 10 and 12. We have to find a ${\hat\beta}$ which minimises ${(Y-X\hat\beta)^{T}(Y-X\hat\beta)}$ and then our predictions on training dataset will simply by ${X\hat\beta}$.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="Abhimanyu08/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/deep-learning/mathematics/2021/06/16/Untitled.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog where I write about things I learn on my self-taught journey in the field of Artificial Intelligence</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/Abhimanyu08" title="Abhimanyu08"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/A_Bhimany_u" title="A_Bhimany_u"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
